{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgUHfiVbwTfQ"
      },
      "source": [
        "# GENERAR DATASET FORMA AUTOMATICA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUqzE0k2U1lb"
      },
      "source": [
        "> **Para ejecutar el siguiente codigo no hace falta estar conectados a una GPU**\n",
        "\n",
        "\n",
        "El siguiente codigo nos permite generar preguntas para nuestro dataset usando OpenRouter, en esta web podemos usar casi todos los modelos del mercado pero para generar el dataset vamos a limitarnos a las versiones gratuitas de DeepSeek-R1 y DeepSeek-V3\n",
        "\n",
        "La API de OpenRouter tiene limitaciones cuando se usan modelos gratuitos:\n",
        "\n",
        "\n",
        "> Free limit: If you are using a free model variant (with an ID ending in :free), then you will be limited to 20 requests per minute and 200 requests per day.\n",
        "\n",
        "\n",
        "\n",
        "El siguiente codigo tiene dos modos:\n",
        "\n",
        "*   Modo de único Prompt: Le metemos solo un prompt del excel ([Prompts para generar el dataset usando distintos modelos](https://docs.google.com/spreadsheets/d/1MQF8Z5_HqVSOzDXD8Ya7zEljJ13rD1Kw9FsUd6xhEJo/edit?pli=1&gid=0#gid=0)) y nos va a generar las preguntas y las respuestas siguiendo las instrucciones del prompt\n",
        "\n",
        "*   Modo Múltiples Prompts: Copiamos distintas preguntas creadas por nosotros o por otros modelos LLM a un archivo de texto o csv, este modo además permite darle un prompt inicial de contexto antes de que empiece a generar las respuestas a nuestras preguntas, hay un ejemplo de prompt de contexto en el ([excel de prompts](https://docs.google.com/spreadsheets/d/1MQF8Z5_HqVSOzDXD8Ya7zEljJ13rD1Kw9FsUd6xhEJo/edit?pli=1&gid=0#gid=0))\n",
        "\n",
        "Una vez genera el contenido, nos descarga el csv con las preguntas, las respuestas, y el modelo que ha utilizado\n",
        "\n",
        "Se pueden generar varios modelos para que ambos generen preguntas y se vayan alternando.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBo74hgrStBc"
      },
      "source": [
        "**Paquita dice que antes de dar a play lo pienses dos veces a la hora de agregar cuantas consultas quieres hacer, ya que si no tienes un conteo de las restantes en la API el codigo mostrará que no se pueden generar más consultas y el csv no se generará con las consultas antes del error**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw-k3y8dY9c5"
      },
      "source": [
        "## Codigo para generar el contenido del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZh0d5RstAcX"
      },
      "outputs": [],
      "source": [
        "!pip install requests tqdm python-dotenv\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import time\n",
        "import requests\n",
        "import json\n",
        "import re\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "def extract_question_answer(response_text: str) -> Tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Extrae la pregunta y respuesta de un texto generado por el modelo usando varios métodos.\n",
        "\n",
        "    Args:\n",
        "        response_text: Texto generado por el modelo.\n",
        "\n",
        "    Returns:\n",
        "        Tupla con (pregunta, respuesta).\n",
        "    \"\"\"\n",
        "    # Normalizar el texto para lidiar con diferentes formatos\n",
        "    text = response_text.strip()\n",
        "\n",
        "    # Método 1: Buscar el delimitador exacto\n",
        "    if \"RESPUESTAMODELO\" in text:\n",
        "        parts = text.split(\"RESPUESTAMODELO\", 1)\n",
        "        return parts[0].strip(), parts[1].strip()\n",
        "\n",
        "    # Método 2: Buscar variaciones del delimitador\n",
        "    for delimiter in [\"RESPUESTA MODELO\", \"Respuesta Modelo\", \"Respuesta:\", \"Respuesta del modelo:\"]:\n",
        "        if delimiter in text:\n",
        "            parts = text.split(delimiter, 1)\n",
        "            return parts[0].strip(), parts[1].strip()\n",
        "\n",
        "    # Método 3: Buscar un patrón de pregunta y respuesta\n",
        "    # Asumiendo que la pregunta termina con un signo de interrogación\n",
        "    # y la respuesta comienza en la siguiente línea\n",
        "    if \"?\" in text:\n",
        "        # Encontrar la última pregunta en el texto\n",
        "        question_parts = text.split(\"?\")\n",
        "        # La pregunta es todo hasta el último signo de interrogación\n",
        "        all_but_last = \"?\".join(question_parts[:-1]) + \"?\"\n",
        "        last_part = question_parts[-1]\n",
        "\n",
        "        # Si hay más de una línea después del signo de interrogación,\n",
        "        # la primera línea podría ser parte de la pregunta\n",
        "        lines_after = last_part.strip().split(\"\\n\")\n",
        "\n",
        "        if len(lines_after) > 1 and not lines_after[0]:\n",
        "            # La respuesta comienza después de una línea en blanco\n",
        "            question = all_but_last.strip()\n",
        "            answer = \"\\n\".join(lines_after[1:]).strip()\n",
        "        else:\n",
        "            # La respuesta comienza inmediatamente después del signo de interrogación\n",
        "            question = all_but_last.strip()\n",
        "            answer = last_part.strip()\n",
        "\n",
        "        return question, answer\n",
        "\n",
        "    # Si nada funciona, intenta una división por la mitad (última opción)\n",
        "    lines = text.strip().split(\"\\n\")\n",
        "    if len(lines) >= 2:\n",
        "        # Asumimos que la mitad es pregunta y la otra mitad respuesta\n",
        "        midpoint = len(lines) // 2\n",
        "        return \"\\n\".join(lines[:midpoint]).strip(), \"\\n\".join(lines[midpoint:]).strip()\n",
        "\n",
        "    # Si todo falla, devuelve un error\n",
        "    print(f\"ERROR: No se pudo extraer la pregunta y respuesta. Texto completo:\\n{text}\")\n",
        "    return \"ERROR: No se pudo extraer la pregunta\", \"ERROR: No se pudo extraer la respuesta\"\n",
        "\n",
        "def validate_qa_pair(question: str, answer: str) -> bool:\n",
        "    \"\"\"\n",
        "    Valida que el par pregunta-respuesta sea coherente.\n",
        "\n",
        "    Args:\n",
        "        question: La pregunta extraída.\n",
        "        answer: La respuesta extraída.\n",
        "\n",
        "    Returns:\n",
        "        True si parece un par válido, False en caso contrario.\n",
        "    \"\"\"\n",
        "    # La pregunta debería terminar con signo de interrogación\n",
        "    has_question_mark = \"?\" in question\n",
        "\n",
        "    # Verificar longitudes mínimas\n",
        "    question_length_ok = len(question.split()) >= 3\n",
        "    answer_length_ok = len(answer.split()) >= 5\n",
        "\n",
        "    # La respuesta no debe contener la palabra \"pregunta\" o \"question\"\n",
        "    no_question_in_answer = \"pregunta\" not in answer.lower() and \"question\" not in answer.lower()\n",
        "\n",
        "    # Verificar que no contenga instrucciones del formato\n",
        "    no_instructions = \"formato\" not in question.lower() and \"instrucciones\" not in question.lower()\n",
        "\n",
        "    # Verificar que no haya mensajes de error\n",
        "    no_errors = \"ERROR:\" not in question and \"ERROR:\" not in answer\n",
        "\n",
        "    return has_question_mark and question_length_ok and answer_length_ok and no_question_in_answer and no_instructions and no_errors\n",
        "\n",
        "def is_similar_to_existing(question: str, existing_questions: List[str], threshold: float = 0.7) -> bool:\n",
        "    \"\"\"\n",
        "    Comprueba si una pregunta es similar a las existentes usando una comparación simple.\n",
        "\n",
        "    Args:\n",
        "        question: Pregunta a comprobar\n",
        "        existing_questions: Lista de preguntas existentes\n",
        "        threshold: Umbral de similitud (0-1)\n",
        "\n",
        "    Returns:\n",
        "        True si es similar, False si no\n",
        "    \"\"\"\n",
        "    # Normalizar la pregunta (minúsculas, sin puntuación)\n",
        "    normalized_question = re.sub(r'[^\\w\\s]', '', question.lower())\n",
        "    words = set(normalized_question.split())\n",
        "\n",
        "    for existing in existing_questions:\n",
        "        normalized_existing = re.sub(r'[^\\w\\s]', '', existing.lower())\n",
        "        existing_words = set(normalized_existing.split())\n",
        "\n",
        "        # Calcular similitud Jaccard (proporción de palabras en común)\n",
        "        intersection = len(words.intersection(existing_words))\n",
        "        union = len(words.union(existing_words))\n",
        "\n",
        "        if union > 0 and intersection / union > threshold:\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def call_openrouter_api(\n",
        "    prompt: str,\n",
        "    model: str,\n",
        "    api_key: str,\n",
        "    system_message: Optional[str] = None,\n",
        "    max_retries: int = 3\n",
        ") -> Optional[Dict[Any, Any]]:\n",
        "    \"\"\"\n",
        "    Realiza una llamada a la API de OpenRouter.\n",
        "\n",
        "    Args:\n",
        "        prompt: Prompt a enviar.\n",
        "        model: Nombre del modelo a utilizar.\n",
        "        api_key: Clave de API de OpenRouter.\n",
        "        system_message: Mensaje de sistema opcional.\n",
        "        max_retries: Número máximo de reintentos en caso de fallo.\n",
        "\n",
        "    Returns:\n",
        "        Respuesta de la API o None si hubo un error.\n",
        "    \"\"\"\n",
        "    url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": \"https://colab.research.google.com/\"\n",
        "    }\n",
        "\n",
        "    messages = []\n",
        "    if system_message:\n",
        "        messages.append({\"role\": \"system\", \"content\": system_message})\n",
        "\n",
        "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    data = {\n",
        "        \"model\": model,\n",
        "        \"messages\": messages\n",
        "    }\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.post(url, headers=headers, json=data)\n",
        "            response.raise_for_status()\n",
        "            return response.json()\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error en la llamada a la API (intento {attempt+1}/{max_retries}): {e}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                # Esperar un tiempo exponencial antes de reintentar\n",
        "                wait_time = 2 ** attempt\n",
        "                print(f\"Reintentando en {wait_time} segundos...\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                print(\"Se alcanzó el número máximo de reintentos.\")\n",
        "                return None\n",
        "\n",
        "def extract_response(api_response: Dict[Any, Any]) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Extrae la respuesta del modelo desde la respuesta de la API.\n",
        "\n",
        "    Args:\n",
        "        api_response: Respuesta de la API.\n",
        "\n",
        "    Returns:\n",
        "        Texto de la respuesta o None si no se puede extraer.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return api_response['choices'][0]['message']['content']\n",
        "    except (KeyError, IndexError, TypeError) as e:\n",
        "        print(f\"Error al extraer la respuesta: {e}\")\n",
        "        print(f\"Respuesta completa de la API: {json.dumps(api_response, indent=2)}\")\n",
        "        return None\n",
        "\n",
        "def save_to_csv(data: List[Dict[str, str]], output_file: str) -> None:\n",
        "    \"\"\"\n",
        "    Guarda los datos en un archivo CSV y permite descargarlo desde Colab.\n",
        "\n",
        "    Args:\n",
        "        data: Lista de diccionarios con los datos a guardar.\n",
        "        output_file: Ruta al archivo de salida.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(output_file, 'w', encoding='utf-8', newline='') as file:\n",
        "            fieldnames = ['instruction', 'output', 'model']\n",
        "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "\n",
        "            writer.writeheader()\n",
        "            for item in data:\n",
        "                writer.writerow(item)\n",
        "\n",
        "        print(f\"Dataset guardado exitosamente en {output_file}\")\n",
        "\n",
        "        # Permitir la descarga del archivo desde Colab\n",
        "        files.download(output_file)\n",
        "    except Exception as e:\n",
        "        print(f\"Error al guardar el dataset: {e}\")\n",
        "\n",
        "def generate_dataset_single_prompt(\n",
        "    prompt: str,\n",
        "    output_file: str,\n",
        "    models: List[str],\n",
        "    api_key: str,\n",
        "    num_iterations: int = 10\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Genera un dataset usando un único prompt general múltiples veces.\n",
        "\n",
        "    Args:\n",
        "        prompt: Prompt general para generar preguntas y respuestas.\n",
        "        output_file: Ruta al archivo de salida.\n",
        "        models: Lista de modelos a utilizar.\n",
        "        api_key: Clave de API de OpenRouter.\n",
        "        num_iterations: Número de veces que se utilizará el prompt.\n",
        "    \"\"\"\n",
        "    print(f\"Modo: Prompt único - Generando {num_iterations} ejemplos por modelo\")\n",
        "\n",
        "    # Definir temas para rotar\n",
        "    topics = [\n",
        "        \"cookies y navegación web\",\n",
        "        \"derechos ARCO\",\n",
        "        \"redes sociales\",\n",
        "        \"videovigilancia\",\n",
        "        \"menores y consentimiento\",\n",
        "        \"geolocalización\",\n",
        "        \"datos biométricos\",\n",
        "        \"marketing directo\",\n",
        "        \"filtraciones de datos\",\n",
        "        \"transferencias internacionales\",\n",
        "        \"derecho al olvido\",\n",
        "        \"uso de IA con datos personales\",\n",
        "        \"aplicaciones móviles\",\n",
        "        \"datos en el ámbito laboral\",\n",
        "        \"información de salud\",\n",
        "        \"datos bancarios y financieros\"\n",
        "    ]\n",
        "\n",
        "    # Crear archivo para ejemplos fallidos\n",
        "    fallidos_file = f\"ejemplos_fallidos_{int(time.time())}.txt\"\n",
        "\n",
        "    # Preparar el dataset\n",
        "    dataset = []\n",
        "    existing_questions = []\n",
        "    fallidos = 0\n",
        "\n",
        "    # Procesar cada modelo y generar múltiples ejemplos\n",
        "    total_iterations = len(models) * num_iterations\n",
        "    with tqdm(total=total_iterations, desc=\"Generando dataset\") as pbar:\n",
        "        for model in models:\n",
        "            print(f\"\\nProcesando modelo: {model}\")\n",
        "\n",
        "            for i in range(num_iterations):\n",
        "                # Seleccionar tema para esta iteración\n",
        "                current_topic = topics[i % len(topics)]\n",
        "\n",
        "                # Crear un prompt específico para esta iteración\n",
        "                iteration_prompt = f\"\"\"\n",
        "{prompt}\n",
        "\n",
        "⚠️ INSTRUCCIÓN CRUCIAL: Genera UN ÚNICO par de pregunta-respuesta sobre protección de datos.\n",
        "\n",
        "TEMA ESPECÍFICO: Genera una pregunta relacionada con \"{current_topic}\".\n",
        "Asegúrate de que sea una pregunta concreta y relevante para usuarios españoles.\n",
        "\n",
        "FORMATO EXACTO A SEGUIR:\n",
        "[Escribe aquí UNA ÚNICA pregunta sobre {current_topic}]\n",
        "RESPUESTAMODELO\n",
        "[Escribe aquí la respuesta a esa única pregunta]\n",
        "\n",
        "RECUERDA: Solo UN par pregunta-respuesta. Termina tu respuesta después de contestar la pregunta.\n",
        "\"\"\"\n",
        "\n",
        "                # Intentar hasta 3 veces si obtenemos duplicados\n",
        "                max_attempts = 3\n",
        "                success = False\n",
        "\n",
        "                for attempt in range(max_attempts):\n",
        "                    # Llamar a la API\n",
        "                    response_data = call_openrouter_api(iteration_prompt, model, api_key)\n",
        "\n",
        "                    if response_data:\n",
        "                        # Extraer la respuesta\n",
        "                        response_text = extract_response(response_data)\n",
        "\n",
        "                        if response_text:\n",
        "                            # Extraer pregunta y respuesta del texto generado\n",
        "                            question, answer = extract_question_answer(response_text)\n",
        "\n",
        "                            # Validar el par pregunta-respuesta\n",
        "                            if validate_qa_pair(question, answer):\n",
        "                                # Verificar si es similar a preguntas existentes\n",
        "                                if not is_similar_to_existing(question, existing_questions):\n",
        "                                    # Añadir al dataset\n",
        "                                    dataset.append({\n",
        "                                        'instruction': question,\n",
        "                                        'output': answer,\n",
        "                                        'model': model\n",
        "                                    })\n",
        "                                    # Guardar para futuras comparaciones\n",
        "                                    existing_questions.append(question)\n",
        "\n",
        "                                    print(f\"\\nIteración {i+1}/{num_iterations}:\")\n",
        "                                    print(f\"Tema: {current_topic}\")\n",
        "                                    print(f\"Pregunta: {question[:100]}...\")\n",
        "                                    print(f\"Respuesta: {answer[:100]}...\")\n",
        "\n",
        "                                    success = True\n",
        "                                    break\n",
        "                                else:\n",
        "                                    print(f\"\\n⚠️ Intento {attempt+1}: Pregunta similar ya existe, reintentando...\")\n",
        "                                    # Añadir más variación al prompt\n",
        "                                    iteration_prompt += f\"\\n\\nIMPORTANTE: Asegúrate de que la pregunta sea ORIGINAL y DIFERENTE a esta: \\\"{question}\\\"\"\n",
        "                            else:\n",
        "                                print(f\"\\n⚠️ Intento {attempt+1}: Par pregunta-respuesta no válido, reintentando...\")\n",
        "                                print(f\"\\nRespuesta completa del modelo:\")\n",
        "                                print(\"-\" * 50)\n",
        "                                print(response_text)\n",
        "                                print(\"-\" * 50)\n",
        "\n",
        "                if not success:\n",
        "                    fallidos += 1\n",
        "                    print(f\"\\n❌ No se pudo generar un par único en {max_attempts} intentos para el tema: {current_topic}\")\n",
        "\n",
        "                    # Guardar ejemplo fallido\n",
        "                    with open(fallidos_file, \"a\", encoding=\"utf-8\") as f:\n",
        "                        f.write(f\"--- EJEMPLO FALLIDO {fallidos} ---\\n\")\n",
        "                        f.write(f\"MODELO: {model}\\n\")\n",
        "                        f.write(f\"TEMA: {current_topic}\\n\")\n",
        "                        if 'response_text' in locals():\n",
        "                            f.write(f\"ÚLTIMO INTENTO:\\n{response_text}\\n\\n\")\n",
        "                        f.write(\"-\" * 50 + \"\\n\\n\")\n",
        "\n",
        "                pbar.update(1)\n",
        "\n",
        "                # Pequeña pausa para evitar sobrecargar la API\n",
        "                time.sleep(0.5)\n",
        "\n",
        "    # Guardar el dataset\n",
        "    if dataset:\n",
        "        save_to_csv(dataset, output_file)\n",
        "        print(f\"\\n✅ Se han generado {len(dataset)} ejemplos únicos para el dataset.\")\n",
        "        print(f\"❌ Se han detectado {fallidos} ejemplos con formato incorrecto o duplicados.\")\n",
        "        if fallidos > 0:\n",
        "            print(f\"📝 Los ejemplos fallidos se han guardado en '{fallidos_file}'.\")\n",
        "    else:\n",
        "        print(\"❌ No se pudieron generar ejemplos válidos para el dataset.\")\n",
        "\n",
        "# Ejecutar en Colab (interfaz interactiva)\n",
        "def main_colab():\n",
        "    from IPython.display import display, HTML\n",
        "\n",
        "    display(HTML(\"<h3>Generador de Dataset para Fine-Tuning</h3>\"))\n",
        "\n",
        "    # Solicitar los parámetros\n",
        "    api_key = input(\"Introduce tu clave API de OpenRouter: \")\n",
        "\n",
        "\n",
        "    # Entrada directa del prompt\n",
        "    print(\"\\nIntroduce el prompt general (escribe o pega el texto y presiona Enter dos veces para finalizar):\")\n",
        "    print(\"Para terminar la entrada, escribe una línea que solo contenga '***FIN***'\")\n",
        "\n",
        "    lines = []\n",
        "    while True:\n",
        "        line = input()\n",
        "        if line == \"***FIN***\":\n",
        "            break\n",
        "        lines.append(line)\n",
        "\n",
        "    general_prompt = \"\\n\".join(lines)\n",
        "\n",
        "    if not general_prompt.strip():\n",
        "        print(\"El prompt no puede estar vacío.\")\n",
        "        return\n",
        "\n",
        "\n",
        "    # Solicitar el número de iteraciones\n",
        "    num_iterations = input(\"Introduce el número de ejemplos a generar por modelo [10]: \")\n",
        "    num_iterations = int(num_iterations) if num_iterations.strip() else 10\n",
        "\n",
        "    # Solicitar los modelos\n",
        "    models_input = input(\"Introduce los modelos a utilizar (separados por comas) [deepseek/deepseek-chat:free]: \")\n",
        "    models = [m.strip() for m in models_input.split(\",\")] if models_input.strip() else [\"deepseek/deepseek-chat:free\"]\n",
        "\n",
        "    # Solicitar el nombre del archivo de salida\n",
        "    output_file = input(\"Introduce el nombre del archivo de salida [dataset.csv]: \")\n",
        "    output_file = output_file if output_file.strip() else \"dataset.csv\"\n",
        "\n",
        "    print(\"\\nResumen de la configuración:\")\n",
        "    print(f\"- Prompt general: '{general_prompt[:100]}...'\")\n",
        "    print(f\"- Número de iteraciones por modelo: {num_iterations}\")\n",
        "    print(f\"- Modelos: {', '.join(models)}\")\n",
        "    print(f\"- Archivo de salida: {output_file}\")\n",
        "\n",
        "    confirm = input(\"\\n¿Confirmar y comenzar la generación? (s/n): \")\n",
        "    if confirm.lower() in [\"s\", \"si\", \"sí\", \"y\", \"yes\"]:\n",
        "        generate_dataset_single_prompt(\n",
        "            prompt=general_prompt,\n",
        "            output_file=output_file,\n",
        "            models=models,\n",
        "            api_key=api_key,\n",
        "            num_iterations=num_iterations\n",
        "        )\n",
        "    else:\n",
        "        print(\"Operación cancelada.\")\n",
        "\n",
        "\n",
        "\n",
        "# Código para ejecutar directamente en una celda de Colab\n",
        "main_colab()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn823uUfWxYC"
      },
      "source": [
        "Prompt único para generar el dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0t6fjKVrwSWJ"
      },
      "outputs": [],
      "source": [
        "💡 Objetivo: Generar UN ÚNICO par de pregunta-respuesta sobre privacidad y protección de datos en España para entrenar un modelo con RAG.\n",
        "\n",
        "⚠️ INSTRUCCIÓN CRUCIAL: Genera SÓLO UN par pregunta-respuesta en cada ejecución. No incluyas múltiples ejemplos ni numeración.\n",
        "\n",
        "📊 Temas a cubrir (diversidad temática):\n",
        "🔹 Derechos digitales: acceso, rectificación, supresión, oposición, portabilidad, limitación, olvido.\n",
        "🔹 Consentimiento: validez, revocación, menores, excepciones.\n",
        "🔹 Cookies y tracking: tipos, banners, rechazos, perfilado.\n",
        "🔹 Datos sensibles: salud, biométricos, ideología, orientación sexual.\n",
        "🔹 Contextos específicos: laboral, educativo, sanitario, financiero, comercial.\n",
        "🔹 Tecnologías emergentes: IA, reconocimiento facial, IoT, blockchain.\n",
        "🔹 Seguridad: brechas, notificaciones, medidas técnicas.\n",
        "🔹 Transferencias: internacionales, entre empresas, cesiones.\n",
        "🔹 Videovigilancia: ámbito privado, público, laboral.\n",
        "🔹 Responsabilidades: empresas, DPO, encargados, autoridades.\n",
        "🔹 Sanciones: sin mencionar cantidades específicas.\n",
        "\n",
        "🔹 Características de las preguntas:\n",
        "✔️ Pregunta concreta (10-30 palabras) enfocada en un único tema\n",
        "✔️ Evita preguntas genéricas; usa casos prácticos realistas\n",
        "✔️ Incluye contexto específico (quién, dónde, situación concreta)\n",
        "✔️ Varía la formulación: usa \"¿Es legal...?\", \"¿Qué derechos tengo si...?\", \"¿Qué ocurre cuando...?\"\n",
        "✔️ Asegúrate de que sea una pregunta ORIGINAL y diferente a los ejemplos\n",
        "\n",
        "🔹 Características de las respuestas:\n",
        "✅ 2-4 frases concisas pero completas\n",
        "✅ Explica condiciones y matices (no solo \"sí/no\")\n",
        "✅ Lenguaje claro sin jerga legal\n",
        "✅ No menciones artículos o leyes específicas\n",
        "✅ Tono profesional pero accesible\n",
        "✅ Información actualizada según RGPD y LOPDGDD\n",
        "\n",
        "🔹 Precisión de las respuestas:\n",
        "✅ Asegúrate de que la respuesta refleje con precisión el marco legal español actual (RGPD y LOPDGDD)\n",
        "✅ Si existe ambigüedad legal, menciónalo explícitamente\n",
        "✅ Evita respuestas que puedan resultar engañosas por simplificar en exceso\n",
        "✅ No incluyas opiniones personales o interpretaciones controvertidas\n",
        "\n",
        "\n",
        "🔹 Tono de la respuesta:\n",
        "✅ Profesional pero accesible\n",
        "✅ Objetivo y no alarmista\n",
        "✅ Informativo sin ser condescendiente\n",
        "✅ Directo sin ser brusco\n",
        "\n",
        "🔹 Escenarios específicos a considerar (Estos son solo ejemplos puedes generar tus propios escenarios, se original):\n",
        "- Una persona intentando ejercer sus derechos frente a una gran empresa\n",
        "- Un empleado con preocupaciones sobre la privacidad en su trabajo\n",
        "- Un padre/madre preocupado por los datos de sus hijos menores\n",
        "- Un usuario de aplicaciones móviles o servicios online\n",
        "- Un propietario de pequeño negocio que necesita cumplir con la normativa\n",
        "- Situaciones en comunidades de vecinos o espacios compartidos\n",
        "- Interacciones con administraciones públicas\n",
        "- Espionaje entre compañeros de trabajo\n",
        "- Empresas comerciando con datos privador\n",
        "\n",
        "⚠️ IMPORTANTE: Asegúrate de que la pregunta sea ORIGINAL y no se parezca demasiado a los ejemplos proporcionados. Busca ángulos o situaciones novedosas dentro del tema elegido.\n",
        "\n",
        "\n",
        "⚠️ FORMATO EXACTO A SEGUIR:\n",
        "\n",
        "\n",
        "Pregunta sobre los temas nombrados anteriormente\n",
        "RESPUESTAMODELO\n",
        "Respuesta clara, razonada, y convincente\n",
        "\n",
        "📝 Ejemplos (para referencia):\n",
        "\n",
        "\n",
        "\"Un comercio online me instaló cookies de seguimiento sin avisarme. ¿Esto es legal?\"\n",
        "RESPUESTAMODELO\n",
        "\"No, en España un sitio web solo puede instalar cookies de seguimiento si obtiene tu consentimiento previo. Deben informarte de manera clara sobre su uso y permitirte aceptarlas o rechazarlas fácilmente. Solo las cookies estrictamente necesarias pueden activarse sin tu permiso.\"\n",
        "\n",
        "\"Una empresa de crédito ha rechazado mi solicitud basándose en un algoritmo. ¿Puedo exigir una explicación?\"\n",
        "RESPUESTAMODELO\n",
        "\"Sí, cuando una decisión automatizada afecta significativamente tus derechos, puedes solicitar una explicación clara sobre los criterios utilizados. También puedes pedir una revisión manual si crees que la decisión ha sido injusta o errónea.\"\n",
        "\n",
        "\"Trabajo desde casa y mi empresa usa software de monitoreo en mi ordenador. ¿Hasta qué punto es legal?\"\n",
        "RESPUESTAMODELO\n",
        "\"El monitoreo es legal si está justificado y comunicado de manera transparente. Tu empresa debe informarte sobre qué datos recopila, con qué finalidad y durante cuánto tiempo. Además, el control debe ser proporcional y no invadir tu privacidad más allá de lo necesario para evaluar tu rendimiento laboral.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHSZV3dZ9X0Y"
      },
      "source": [
        "Para detectar preguntas duplicadas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NId9n1E59ZmX"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv('/content/FinalData - Hoja 1 (4).csv')\n",
        "\n",
        "# Calcular similitud entre preguntas\n",
        "vectorizer = TfidfVectorizer(stop_words='english')  # Puedes ajustar a español si necesitas\n",
        "vectors = vectorizer.fit_transform(df['instruction'])\n",
        "similarity = cosine_similarity(vectors)\n",
        "\n",
        "# Umbral de similaridad (ajustar según necesidad)\n",
        "threshold = 0.8\n",
        "\n",
        "# Identificar grupos de preguntas similares\n",
        "grupos_similares = []\n",
        "ya_procesadas = set()\n",
        "\n",
        "for i in range(len(df)):\n",
        "    if i in ya_procesadas:\n",
        "        continue\n",
        "\n",
        "    similar_indices = []\n",
        "    for j in range(len(df)):\n",
        "        if i != j and similarity[i, j] > threshold:\n",
        "            similar_indices.append(j)\n",
        "\n",
        "    if similar_indices:\n",
        "        grupo = [i] + similar_indices\n",
        "        grupos_similares.append(grupo)\n",
        "        ya_procesadas.update(similar_indices)\n",
        "\n",
        "# Mostrar los grupos de preguntas similares\n",
        "print(f\"Se encontraron {len(grupos_similares)} grupos de preguntas similares:\\n\")\n",
        "\n",
        "for idx, grupo in enumerate(grupos_similares):\n",
        "    print(f\"Grupo {idx+1} (Similaridad > {threshold}):\")\n",
        "    for i, ind in enumerate(grupo):\n",
        "        print(f\"  {i+1}. Índice {ind}: {df['instruction'].iloc[ind][:100]}...\")\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "# Opcional: Exportar los resultados a un CSV para revisar más fácilmente\n",
        "output_rows = []\n",
        "\n",
        "for group_idx, grupo in enumerate(grupos_similares):\n",
        "    for idx in grupo:\n",
        "        output_rows.append({\n",
        "            'grupo': group_idx + 1,\n",
        "            'indice_original': idx,\n",
        "            'pregunta': df['instruction'].iloc[idx],\n",
        "            'respuesta': df['output'].iloc[idx],\n",
        "            'modelo': df['model'].iloc[idx] if 'model' in df.columns else 'Unknown'\n",
        "        })\n",
        "\n",
        "grupos_df = pd.DataFrame(output_rows)\n",
        "output_file = 'grupos_similares.csv'\n",
        "grupos_df.to_csv(output_file, index=False)\n",
        "print(f\"Resultados exportados a {output_file}\")\n",
        "\n",
        "# Descargar el archivo\n",
        "files.download(output_file)\n",
        "\n",
        "# Contar cuántas preguntas están en los grupos (posibles duplicados)\n",
        "num_duplicados = sum(len(grupo) for grupo in grupos_similares) - len(grupos_similares)\n",
        "print(f\"\\nNúmero total de posibles preguntas duplicadas: {num_duplicados}\")\n",
        "print(f\"Número de filas en el dataset original: {len(df)}\")\n",
        "print(f\"Número estimado de filas después de eliminar duplicados: {len(df) - num_duplicados}\")\n",
        "\n",
        "grupos_similares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ7qPeJLTl28"
      },
      "source": [
        "# Finetunning Llama 3\n",
        "---Resumen\n",
        "Aqui poner que es unsloth que usamos y que modelos se pueden usar para hacer finetunning\n",
        "\n",
        "La biblioteca Unsloth permite completar el proceso de entrenamiento y entrenamiento fino 2x más rápido y requiere mucha menos VRAM gracias a derivaciones matemáticas complejas y kernels de GPUs optimizados manualmente\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSX6xhJ5TbIZ"
      },
      "outputs": [],
      "source": [
        "#%%Capture para evitar que genere salida en collab el comando de install\n",
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install unsloth\n",
        "# Get latest Unsloth\n",
        "!pip install --upgrade --no-deps \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS0fAdKLiWlX"
      },
      "source": [
        "Debido a que tenemos recursos limitados vamos a usar modelos cuantizados en 4 bits para reducir el consumo de memoria y el espacio en disco que utilizan.\n",
        "\n",
        "¿Que significa cuantizar modelos?\n",
        "\n",
        "Normalmente los modelos almacenan sus parámetros en Punto Flotante de 16 Bits, al cuantizar los modelos de 16 Bits a 4 Bits conseguimos:\n",
        "\n",
        "✅ Menos uso de VRAM/RAM → Nos permite utilizar modelos más grandes que debido a limitaciones de google collab no podríamos ejecutar en este entorno.\n",
        "\n",
        "✅ Inferencia más rápida → Al reducir los datos aumenta la velocidad de cálculo.\n",
        "\n",
        "✅ Descarga más rápida → Al reducir el tamaño en disco que ocupan conseguimos ahorrarnos tiempo en descargar/subir los distintos modelos además de ahorrar espacio en google collab, el cual esta muy limitado.\n",
        "\n",
        "¿Que desventajas tiene?\n",
        "\n",
        "\n",
        "🔴 Pérdida de precisión → Reducir los bits disminuye la calidad de las respuestas, afectando tareas complejas.\n",
        "\n",
        "🔴 Problemas en cálculos precisos → Modelos cuantizados pueden fallar en matemáticas avanzadas o generación de código detallado.\n",
        "\n",
        "🟠  Fine-tuning más difícil → La cuantización a 4 bits reduce la precisión de los pesos. Unsloth utiliza técnicas optimizadas para reducir el efecto negativo que esto produce en el finetunning, pero sigue habiendo ligeras restricciones comparado con modelos en FP16 o FP32."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4aEbvOzUK-d"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Se puede poner la longitud que se quiera, Unsloth utiliza RoPE Scaling\n",
        "dtype = None # None para detección automática de la GPU. Float16 para Tesla T4, V100, Bfloat16 para Ampere+\n",
        "load_in_4bit = True # Usamos 4bit para reducir el uso de memoria. Can be False.\n",
        "\n",
        "\n",
        "#Modelos 4bit cuantizados por unsloth\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
        "    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGPMaCTDqUyj"
      },
      "source": [
        "Ahora vamos a utilizar LoRA (Low-Rank Adapter), esto nos permite re-entrenar un modelo sin modificar toda su estructura. En lugar de ajustar todos los parámetros del modelo base, LoRA introduce dos nuevas matrices que se encargan de aprender y almacenar las actualizaciones específicas necesarias durante el fine-tuning.\n",
        "\n",
        "Una matriz se especializa en capturar las modificaciones necesarias durante el entrenamiento y la otra ayuda a combinarlas con los parámetros originales del modelo.\n",
        "\n",
        "Gracias a que solo se actualizan estas nuevas matrices en lugar de todos los parámetros del modelo original, podemos realizar fine-tunning de modelos más grandes con un hardware limitado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNWXr-KltxGq"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEZyUecvuras"
      },
      "source": [
        "## Preparación del dataset\n",
        "\n",
        "Para que Ollama y llama.cpp funcionen como un chatbot personalizado como por ejemplo ChatGPT, solo debemos tener 2 columnas: una de instrucciones y una de salida, por lo que nuestro dataset solo consta de esta dos columnas.\n",
        "\n",
        "Vamos a utilizar la librería de load_dataset del paquete datasets de hugging face, la cual nos facilita el uso de datasets para fine-tunning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmWNeFp-vHGZ"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\n",
        "    \"csv\",\n",
        "    data_files = \"/content/input.csv\",\n",
        "    split = \"train\",\n",
        ")\n",
        "print(dataset.column_names)\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-tD_iyYxTR4"
      },
      "source": [
        "Una vez ya tenemos cargado el dataset le vamos a dar un formato adecuado para fine-tunning.\n",
        "\n",
        "Vamos a utilizar la libreria to_sharegpt de Unsloth, para generar conversaciones largas combinando los inputs y outputs del dataset.\n",
        "\n",
        "Gracias a esto en vez de hacer el finetunning con preguntas sueltas, realizamos el entrenamiento con conversaciones simuladas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8H4EsAxxS1U"
      },
      "outputs": [],
      "source": [
        "from unsloth import to_sharegpt\n",
        "dataset = to_sharegpt(\n",
        "    dataset,\n",
        "    merged_prompt = \\\n",
        "        \"[[La pregunta es: {instruction}.]]\"\n",
        "        ,\n",
        "    conversation_extension = 5, # Este parámetro agrupa aleatoriamente hasta x preguntas y respuestas en una sola conversación. Esto es útil para simular conversaciones más largas.\n",
        "    output_column_name = \"output\",\n",
        ")\n",
        "#Para imprimir como se ve ahora el dataset\n",
        "from pprint import pprint\n",
        "pprint(dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXKKSvVgzYB-"
      },
      "source": [
        "El dataset ahora utiliza etiquetas como \"human\" y \"gpt\":\n",
        "\n",
        "```\n",
        "{'conversations': [{'from': 'human',\n",
        "                    'value': \"La pregunta es: ('Antonio instala cámaras en su \"\n",
        "                             'tienda sin avisar a empleados y clientes. ¿Qué '\n",
        "                             \"consecuencias puede tener?',).\"},\n",
        "                   {'from': 'gpt',\n",
        "                    'value': 'Según el Artículo 22 de la Ley Orgánica 3/2018 '\n",
        "                             'de Protección de Datos Personales y Garantía de '\n",
        "                             'los Derechos Digitales, debe informar a los '\n",
        "                             'afectados sobre la videovigilancia. Si no lo '\n",
        "                             'hace, puede enfrentar sanciones de la AEPD.'},\n",
        "```\n",
        "Sin embargo para un modelo de OpenAI o Hugging Face, se requieren etiquetas estándar como **user** para el usuario y **assistant** para el modelo.\n",
        "\n",
        "Para arreglar esto vamos a utilizar la libreria standardize_sharegpt de Unsloth que cambia todas las etiquetas como human, gpt, system, etc... a **user** y **assistant**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HR7_HxXysv7"
      },
      "outputs": [],
      "source": [
        "from unsloth import standardize_sharegpt\n",
        "dataset = standardize_sharegpt(dataset)\n",
        "\n",
        "#Para imprimir como se ve ahora el dataset\n",
        "from pprint import pprint\n",
        "pprint(dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo0Yv5Mt00NF"
      },
      "source": [
        "## Plantilla de conversación con el modelo\n",
        "\n",
        "Una plantilla de chat es útil para el fine-tuning porque proporciona una estructura coherente que enseña al modelo cómo interactuar de una mejor manera en las conversaciones, y por lo tanto mejorar la calidad de sus respuestas.\n",
        "\n",
        "Este es el formato de  un Prompt de Llama-3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2SRFD38096V"
      },
      "outputs": [],
      "source": [
        "#chat_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "#{SYSTEM}<|end_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "#{INPUT}<|end_of_text|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "#{OUTPUT}<|end_of_text|>\"\"\"\n",
        "\n",
        "chat_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "{SYSTEM}<|eot_id|>\n",
        "<|start_header_id|>user<|end_header_id|>\n",
        "{INPUT}<|eot_id|>\n",
        "<|start_header_id|>assistant<|end_header_id|>\n",
        "{OUTPUT}<|eot_id|>\"\"\"\n",
        "\n",
        "from unsloth import apply_chat_template\n",
        "dataset = apply_chat_template(\n",
        "    dataset,\n",
        "    tokenizer = tokenizer,\n",
        "    chat_template = chat_template,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htc-yqdK6Ny_"
      },
      "source": [
        "## Entrenamiento del modelo\n",
        "\n",
        "Vamos a utilizar el Transformers Reinforcement Learning (TRL) de Hugginface **SFTTrainer** (Supervised Fine-Tuning) el cual permite entrenar modelos preexistentes con datos etiquetados para mejorar su desempeño en tareas específicas.\n",
        "\n",
        "Tambien se puede utilizar DPOTrainer: Utiliza el aprendizaje por refuerzo directo (Direct Preference Optimization) para mejorar las respuestas generadas por el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6R0i0OqB597z"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = 2048,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 40,\n",
        "        #max_steps=None,\n",
        "        num_train_epochs=4,\n",
        "        learning_rate =  5e-5,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 10,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "#Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdUVpTcH7ThB"
      },
      "source": [
        "Es necesario tener una cuenta en https://wandb.ai/authorize y obtener la clave API para poder hacer el entrenamiento\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3SWM2RF7Vu7"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8iM7gau7ZYo"
      },
      "source": [
        "### Memoria Final y estadisticas de tiempo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvKJml8M7eBY"
      },
      "outputs": [],
      "source": [
        "\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRBXK1wG7u4U"
      },
      "source": [
        "\n",
        "### Inferencia\n",
        "Vamos a ejecutar el modelo, Unsloth hace la inferencia de manera nativa 2 veces más rápida. Hay que usar promtp similares al finetunning para obtener buenos resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-i_hr1J73N0"
      },
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "messages = [                    # Change below!\n",
        "    #{\"role\": \"user\", \"content\": '¿Cuál es el objeto de la Ley Orgánica 3/2018 según el Artículo 1? \\n'},\n",
        "    {\"role\": \"user\", \"content\": '\"¿Puede el colegio hacer fotos a los alumnos y publicarlas en la web del colegio?\"\\n'},\n",
        "    #{\"role\": \"user\", \"content\": '\"¿Cuál es el deber de confidencialidad según el Artículo 5 de la Ley Orgánica 3/2018?\"\\n'},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGohnNGT8cLm"
      },
      "source": [
        "## Guardar y Cargar el modelo finetuneado\n",
        "\n",
        "Para guardar el modelo final como adaptadores LoRA, utiliza `push_to_hub` de Huggingface para guardarlo en línea o `save_pretrained` para guardarlo localmente.\n",
        "\n",
        "[NOTA] Esto SOLO guarda los adaptadores LoRA, no el modelo completo. ¡Para guardarlo en 16bit o GGUF, baja más abajo!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXD4le6y94uc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"lora_model\") # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29AH6HEI98bC"
      },
      "source": [
        "Ahora, si deseas cargar los adaptadores LoRA que acabamos de guardar para inferencia, cambia False a True:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJma3s7D97yP"
      },
      "outputs": [],
      "source": [
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "pass\n",
        "\n",
        "messages = [                    # Change below!\n",
        "    {\"role\": \"user\", \"content\": '¿Puede un trabajador de un supermercado pedirme el DNI?\\n'\\\n",
        "                                '\\n'\\\n",
        "                                ''},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XnT-Voc_6Hr"
      },
      "source": [
        "A continuación, guardaremos el modelo en GGUF / llama.cpp.\n",
        "\n",
        "Clonamos llama.cpp y por defecto lo guardamos en q8_0. Permitimos todos los métodos como q4_k_m. Utiliza `save_pretrained_gguf` para guardarlo localmente y `push_to_hub_gguf` para subirlo a Hugging Face.\n",
        "\n",
        "Algunos métodos de cuantificación compatibles (lista completa en nuestra página de Wiki):\n",
        "\n",
        "- q8_0: Conversión rápida. Uso de recursos alto, pero generalmente aceptable.\n",
        "- q4_k_m: Recomendado. Utiliza Q6_K para la mitad de los tensores `attention.wv` y `feed_forward.w2`, el resto usa Q4_K.\n",
        "- q5_k_m: Recomendado. Utiliza Q6_K para la mitad de los tensores `attention.wv` y `feed_forward.w2`, el resto usa Q5_K.\n",
        "\n",
        "¡También soportamos guardar en múltiples opciones de GGUF en formato de lista! Esto puede acelerar el proceso en 10 minutos o más si deseas varios formatos de exportación.\n",
        "\n",
        "El siguiente codigo es para guardar el modelo en la carpeta /model, es necesario tener una api key en hugginface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0TIoxYH5__Au",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if True: model.push_to_hub_gguf(\"serdom02/Leyeneitor_8bitQ8_0\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"serdom02/model_16bitGGUF\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"serdom02/model_q4_k_mGGUF\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"serdom02/model\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTjohupyAgKP"
      },
      "source": [
        "## Si no subimos el modelo a Hugging Face tenemos que crear el modelo de Ollama\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cXYtIa2EMH3"
      },
      "source": [
        "Ollama necesita un archivo de modelo (Modelfile), que especifica el formato del prompt del modelo. Vamos a imprimir el generado automáticamente por Unsloth:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmgNLwsbERBS"
      },
      "outputs": [],
      "source": [
        "print(tokenizer._ollama_modelfile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5P8Ddl2E5wL"
      },
      "source": [
        "Ahora crearemos un modelo de Ollama llamado `unsloth_model` utilizando el archivo de modelo (Modelfile) que generamos automáticamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBlAipXXE6C3"
      },
      "outputs": [],
      "source": [
        "!ollama create unsloth_model -f ./model/Modelfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGlR5y7pBkQ0"
      },
      "source": [
        "## Descargar el modelo a nuestro ordenador\n",
        "\n",
        "Con este codigo creamos un zip con el modelo para poder descargarlo todo junto a nuestro ordenador local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GhWuu_nBnQ3"
      },
      "outputs": [],
      "source": [
        "!zip -r /content/file.zip /content/model\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"/content/file.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I0aeb-eD26h"
      },
      "source": [
        "Si se ejecuta en el ordenador personal en vez de en colab hay que cambiar la ruta dentro del archivo Modelfile: ![Sin título.png](data:image/png;base64,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)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJdUc4ji-Q8c"
      },
      "source": [
        "# Ollama\n",
        "\n",
        "Unsloth ahora permite ajustar el modelo automáticamente y crear un archivo de modelo, exportando a Ollama.\n",
        "\n",
        "Primero vamos a installar Ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O15au36B-Vyp"
      },
      "outputs": [],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsSmwZzNCQeU"
      },
      "source": [
        "### Usar ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPbHMS-6CmS4"
      },
      "source": [
        "Usamos `subprocess` para iniciar Ollama de forma no bloqueante. En tu propio escritorio, simplemente puedes abrir una nueva terminal y escribir `ollama serve`, pero en Colab, necesitamos usar este truco."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wloHHZfCKKz"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3) # Esperar a que Ollama se cargue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuCqqUq4AMLO"
      },
      "source": [
        "### Usar Modelo subido a Hugginface\n",
        "Si hemos subido el modelo a hugging face podemos descargarlo con el siguiente comando (si el modelo se llama model como este caso, si no habria que sustituir el nombre del modelo por el que sea)\n",
        "Una vez termine de descargar el modelo hay que parar la ejecucion de la celda para usar el resto del cuaderno"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqEM3Lbl_x2t"
      },
      "outputs": [],
      "source": [
        "#!ollama run hf.co/serdom02/model_8bitQ8_0\n",
        "#!ollama run hf.co/serdom02/model_16bitGGUF\n",
        "#!ollama run hf.co/serdom02/model_q4_k_mGGUF\n",
        "!ollama run llama3:8b\n",
        "#!ollama run hf.co/serdom02/Leyeneitor_8bitQ8_0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zUqVHKrBhC7"
      },
      "source": [
        "Hablar con el modelo de Hugging Face (Solo cambiamos el nombre del modelo)\n",
        "\n",
        "Hay que detener la celda anterior y volver a iniciar Ollama para poder hablar con el modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN-zen9pXhXg"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3) # Esperar a que Ollama se cargue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X3VD1EaAtnL"
      },
      "source": [
        "### Hablar con el modelo usando ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICFGEuz2GpH3"
      },
      "source": [
        "Una vez ya tenemos ollama instalado y el modelo descargado podemos interactuar con el modelo usando el siguiente codigo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSjE5z7fGpyH"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import re\n",
        "\n",
        "url = \"http://localhost:11434/api/chat\"\n",
        "data = {\n",
        "    \"model\": \"hf.co/serdom02/Leyeneitor_8bitQ8_0\", #Solo cambiamos esta parte\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"El vecino del bloque de enfrente me ha grabado desde su casa y me ha pillado desnudo, creo que lo ha publicado en las redes, puedo denunciar?\"}] #Aqui ponemos el mensaje\n",
        "}\n",
        "\n",
        "response = requests.post(url, json=data, stream=True)\n",
        "\n",
        "# Concatenar la respuesta completa y limpiar espacios\n",
        "full_response = \"\"\n",
        "for line in response.iter_lines():\n",
        "    if line:\n",
        "        decoded_line = json.loads(line)\n",
        "        if \"message\" in decoded_line and \"content\" in decoded_line[\"message\"]:\n",
        "            # Limpiar espacios dobles y unir las partes\n",
        "            full_response += decoded_line[\"message\"][\"content\"]\n",
        "\n",
        "# Eliminar saltos de línea y limpiar los espacios extras\n",
        "full_response = re.sub(r'\\s+', ' ', full_response).strip()\n",
        "\n",
        "print(full_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s20qCTdICnt2"
      },
      "source": [
        "Si ollama se queda congelado podemos usar kill para matar el proceso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upycRraGCsvP"
      },
      "outputs": [],
      "source": [
        "#Para resetear ollama\n",
        "!kill -9 $(ps aux | grep '[o]llama' | awk '{print $2}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQVgDT5zH57k"
      },
      "source": [
        "# Rag con Llama 3\n",
        "\n",
        "Fuente: https://medium.com/@danushidk507/rag-with-llama-using-ollama-a-deep-dive-into-retrieval-augmented-generation-c58b9a1cfcd3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiPLtIAUIFBw"
      },
      "outputs": [],
      "source": [
        "!pip install -U langchain langchain-huggingface transformers\n",
        "!pip install -U langchain-ollama\n",
        "!pip install sentence-transformers\n",
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Geh84e61IG8K"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain-community faiss-cpu\n",
        "from langchain.vectorstores import FAISS\n",
        "#print(faiss.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0txzLu3IvgQ"
      },
      "source": [
        "## Ingesta de Datos\n",
        "Comenzamos cargando y dividiendo los documentos. Utilizamos PyPDFLoader para cargar un archivo PDF y dividirlo en fragmentos más pequeños y superpuestos, lo que mejora la precisión de la recuperación de información."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kpz4Ar6cT7-"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "#from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Lista de rutas de los PDFs\n",
        "pdf_paths = [\n",
        "    \"BOE-A-1978-31229-consolidado.pdf\",\n",
        "    \"BOE-A-1995-25444-consolidado_CodigoPenal.pdf\",\n",
        "    \"Protección de Datos Personales y garantia de los derechos digitales.pdf\",\n",
        "    \"RGPD_boe.pdf\"\n",
        "]\n",
        "\n",
        "# Cargar documentos de todos los PDFs\n",
        "documents = []\n",
        "for path in pdf_paths:\n",
        "    try:\n",
        "        loader = PyPDFLoader(path)\n",
        "        documents.extend(loader.load())\n",
        "    except Exception as e:\n",
        "        print(f\"Error cargando {path}: {e}\")\n",
        "\n",
        "for doc in documents:\n",
        "    doc.page_content = doc.page_content.replace('\\r\\n', '\\n').replace('\\n\\n', '\\n').strip()\n",
        "\n",
        "\n",
        "# Dividir los documentos en fragmentos\n",
        "#text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=30, separator=\"\\n\")\n",
        "#docs = text_splitter.split_documents(documents=documents)\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=800,\n",
        "    chunk_overlap=200,\n",
        "    separators=[\"\\n\\n\\n\", \"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
        "    tokenizer_name=\"cl100k_base\"  # Compatible con modelos más modernos\n",
        ")\n",
        "\n",
        "\n",
        "docs = text_splitter.split_documents(documents=documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wmAhvLleAo3"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksK6pmIeJait"
      },
      "source": [
        "## Embeddings de Datos y Almacenamiento con FAISS\n",
        "\n",
        "FAISS (Facebook AI Similarity Search) es una biblioteca versátil y eficiente para la búsqueda de similitudes en vectores. Permite la recuperación escalable y rápida de embeddings.\n",
        "Elección del Modelo de Embedding\n",
        "\n",
        "Utilizamos sentence-transformers/all-mpnet-base-v2, conocido por su rendimiento robusto en diversas tareas de procesamiento de texto. Alternativas como BGE o MiniLM pueden ser utilizadas para equilibrar la velocidad y la precisión según el caso específico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNUGuqwXJn6u"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import faiss\n",
        "\n",
        "# Load embedding model\n",
        "#embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "embedding_model_name = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
        "#model_kwargs = {\"device\": \"cuda\"}\n",
        "model_kwargs = {\"device\": \"cpu\"}\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=embedding_model_name,\n",
        "    model_kwargs=model_kwargs\n",
        ")\n",
        "\n",
        "# Create FAISS vector store\n",
        "vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "# Save and reload the vector store\n",
        "vectorstore.save_local(\"faiss_index_\")\n",
        "persisted_vectorstore = FAISS.load_local(\"faiss_index_\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "# Create a retriever\n",
        "retriever = persisted_vectorstore.as_retriever()\n",
        "\n",
        "print(f\"Se indexaron {len(docs)} fragmentos.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ3LCsD0JplS"
      },
      "source": [
        "## Seleccionamos el modelo\n",
        "\n",
        "Aqui podemos probar como responde el modelo sin RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cI3DdntQT9EB"
      },
      "outputs": [],
      "source": [
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# Initialize the LLaMA model\n",
        "llm = OllamaLLM(model=\"hf.co/serdom02/Leyeneitor_8bitQ8_0\", base_url=\"http://127.0.0.1:11434\") #aqui ponemos el modelo finetuneado\n",
        "\n",
        "# Test with a sample prompt\n",
        "response = llm.invoke(\"Un cuento sobre proteccion de datos\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taF4AoOWMhqh"
      },
      "source": [
        "## Ahora podemos interactuar con el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYF2u4QOa8T-"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3) # Esperar a que Ollama se cargue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuv0msS5i3ww",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Importación correcta para versiones recientes de Langchain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "import torch\n",
        "\n",
        "# --- 1. Separación de roles con ChatPromptTemplate ---\n",
        "system_prompt = \"\"\"\n",
        "Eres un experto legal en protección de datos en España.\n",
        "Tu tarea es analizar la legalidad de las situaciones planteadas por el usuario.\n",
        "La respuesta debe ser clara, rigurosa y formal, como si fuera escrita por un profesional del derecho.\n",
        "No hagas suposiciones. No generalices. No repitas ideas.\n",
        "\"\"\"\n",
        "\n",
        "user_prompt = \"\"\"\n",
        "Fragmentos recuperados:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "A continuación, presenta el análisis legal y la conclusión en tres partes:\n",
        "1. **Introducción breve** del problema legal.\n",
        "2. **Análisis jurídico** apoyado en los fragmentos recuperados, citando tus fuentes.\n",
        "3. **Conclusión clara**, con un juicio legal concreto.\n",
        "\"\"\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
        "    HumanMessagePromptTemplate.from_template(user_prompt)\n",
        "])\n",
        "\n",
        "# --- 2. Crear la cadena usando load_qa_chain con el prompt de chat ---\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=chat_prompt)\n",
        "\n",
        "print(\"-\"*50)\n",
        "print(\"Asistente legal de protección de datos listo (usando load_qa_chain con prompt separado). Escribe 'Exit' para salir.\")\n",
        "print(\"-\"*50)\n",
        "\n",
        "# --- 3. Bucle de Consulta Interactivo ---\n",
        "while True:\n",
        "    query = input(\"\\nPregunta: \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "    if not query.strip():\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        retriever.search_kwargs = {'k': 5}\n",
        "        docs_retrieved = retriever.invoke(query)\n",
        "\n",
        "        input_data = {\n",
        "            \"input_documents\": docs_retrieved,\n",
        "            \"question\": query\n",
        "        }\n",
        "\n",
        "        result = chain.invoke(input_data)\n",
        "\n",
        "        if isinstance(result, dict) and 'output_text' in result:\n",
        "            print(\"\\nRespuesta:\")\n",
        "            print(result['output_text'])\n",
        "        elif isinstance(result, str):\n",
        "            print(\"\\nRespuesta:\")\n",
        "            print(result)\n",
        "        else:\n",
        "            print(\"\\nRespuesta recibida (formato inesperado):\")\n",
        "            print(result)\n",
        "\n",
        "        print(\"\\nFuentes Recuperadas:\")\n",
        "        for i, doc in enumerate(docs_retrieved):\n",
        "            source = doc.metadata.get('source', 'N/A')\n",
        "            page = doc.metadata.get('page', 'N/A')\n",
        "            print(f\"  [{i+1}] Fuente: {source}, Página: {page}\")\n",
        "            print(f\"     Fragmento: {doc.page_content[:150]}...\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError durante la ejecución de la cadena: {e}\")\n",
        "\n",
        "print(\"\\nSaliendo del asistente.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M6vxs-EuSUW"
      },
      "source": [
        "Plantillas para Propmt Template:\n",
        "\n",
        "**Plantilla1 Muy restrictiva, se centra en usar solo datos sacados del RAG:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqmjISNbvMYZ"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"\n",
        "Eres un asistente legal experto en protección de datos en España.\n",
        "Tu tarea es analizar la legalidad de la situación descrita en la 'Pregunta' basándote ÚNICA y EXCLUSIVAMENTE en los siguientes 'Textos Legales Recuperados'.\n",
        "NO uses ningún conocimiento externo. Si la información en los textos no es suficiente para dar una respuesta fundada, indícalo claramente.\n",
        "Justifica tu respuesta paso a paso, haciendo referencia explícita a los artículos o secciones relevantes de los textos proporcionados si es posible.\n",
        "\n",
        "Textos Legales Recuperados:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "Análisis Legal y Conclusión (Basado SÓLO en los textos recuperados):\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFL_ls9VvI8j"
      },
      "source": [
        "**Plantilla 2 Menos restrictiva, permite al modelo usar sus conocimientos adquiridos del finetunning y razonar un poco fuera de los contenidos del RAG**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPlb7geDvH70"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"\n",
        "Eres un asistente legal experto en protección de datos en España.\n",
        "Tu tarea es analizar la legalidad de la situación descrita en la 'Pregunta' basándote **principalmente** en los siguientes 'Textos Legales Recuperados'.\n",
        "Si los textos recuperados contienen suficiente información, responde únicamente basándote en ellos.\n",
        "Si los textos no contienen una respuesta clara pero la pregunta se refiere a conceptos básicos del RGPD, usa lo aprendido en el entrenamiento para dar una respuesta general, especificando que no proviene de los textos recuperados.\n",
        "Si la pregunta requiere información específica que no está en los textos recuperados ni en tu entrenamiento, indícalo claramente.\n",
        "\n",
        "Textos Legales Recuperados:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "Análisis Legal y Conclusión:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GEM9djZltvB"
      },
      "source": [
        "## Clasificación de consultas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWFAIrWxjGWa"
      },
      "source": [
        "El Problema de la función anterior es que si el usuario intenta interactuar con el modelo para temas no legales, por ejemplo, saludar, al incluir los resultados del RAG en el propmt el modelo siempre va a intentar responder usando el contexto legal que recibe del RAG, por ejemplo:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgdnHyrdjjlJ"
      },
      "source": [
        "\n",
        "\n",
        "> Pregunta: Hola me llamo sergio\n",
        "\n",
        "> Respuesta:\n",
        "\"La pregunta plantea si el usuario puede solicitar a una empresa que elimine sus datos personales,\n",
        "aunque la normativa oblige a conservarlos. Como experto en protección de datos,\n",
        "debes explicar que solo se pueden conservar los datos estrictamente necesarios para cumplir con obligaciones legales,\n",
        "y que el usuario debe ser informado sobre el plazo de conservación.\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwWDkkm6jtVJ"
      },
      "source": [
        "Para solucionar esto hay que analizar la entrada del usuario para verificar si es una consulta legal que necesite de RAG o simplemente esta interactuando con el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4-fTIcUsDe6"
      },
      "outputs": [],
      "source": [
        "llm_evaluador = OllamaLLM(model=\"llama3:8b\",base_url=\"http://127.0.0.1:11434\") #aqui ponemos el modelo finetuneado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqpL5-k9km_e",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain  # Necesario para la clasificación\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "import torch\n",
        "\n",
        "# --- 1. Función de Clasificación de Intención (Modelo de Clasificación Separado) ---\n",
        "def classify_intent_with_classifier(query, llm_evaluador):\n",
        "    \"\"\"Clasifica si se necesita usar RAG para generar la respuesta.\"\"\"\n",
        "    classification_prompt_template = \"\"\"\n",
        "    Analiza la siguiente pregunta del usuario. Determina si la pregunta requiere un análisis legal sobre protección de datos en España (como RGPD, LOPDGDD) o si se puede responder sin el uso de RAG, es decir, si el modelo tiene suficiente conocimiento para dar una respuesta sin fragmentos adicionales.\n",
        "\n",
        "    Responde únicamente con la palabra \"USAR_RAG\" si la pregunta requiere fragmentos recuperados para una respuesta precisa.\n",
        "    Responde únicamente con la palabra \"NO_RAG\" si la pregunta no requiere fragmentos adicionales y puede ser respondida directamente con el conocimiento general del modelo.\n",
        "\n",
        "    Pregunta del usuario: \"{user_query}\"\n",
        "\n",
        "    Clasificación (USAR_RAG o NO_RAG):\"\"\"\n",
        "\n",
        "    prompt = PromptTemplate(template=classification_prompt_template, input_variables=[\"user_query\"])\n",
        "\n",
        "    classification_runnable = prompt | llm_evaluador\n",
        "\n",
        "    try:\n",
        "        result = classification_runnable.invoke({\"user_query\": query})\n",
        "\n",
        "        # Extraer texto. Ajusta la clave ('text') si tu LLM devuelve otra cosa.\n",
        "        if isinstance(result, dict):\n",
        "            answer_text = result.get('text', '').strip().upper()\n",
        "        elif isinstance(result, str):\n",
        "            answer_text = result.strip().upper()\n",
        "        else:\n",
        "            answer_text = str(result).strip().upper()\n",
        "\n",
        "        # Debug de la clasificación\n",
        "        print(f\"--- DEBUG: Clasificación LLM respondió: '{answer_text}' ---\")\n",
        "\n",
        "        if \"USAR_RAG\" in answer_text:\n",
        "            return \"USAR_RAG\"\n",
        "        elif \"NO_RAG\" in answer_text:\n",
        "            return \"NO_RAG\"\n",
        "        else:\n",
        "            print(\"--- WARN: Respuesta de clasificación no clara, asumiendo NO_RAG por seguridad ---\")\n",
        "            return \"NO_RAG\"  # Default seguro\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error en la clasificación de intención con LLM: {e}\")\n",
        "        return \"USAR_RAG\"  # Default seguro en caso de error\n",
        "\n",
        "# --- 2. Definiciones para el prompt de RAG ---\n",
        "system_prompt_rag = \"\"\"\n",
        "Eres un experto legal en protección de datos en España.\n",
        "Tu tarea es analizar la legalidad de las situaciones planteadas por el usuario utilizando fragmentos recuperados.\n",
        "La respuesta debe ser clara, rigurosa y formal, como si fuera escrita por un profesional del derecho.\n",
        "No hagas suposiciones. No generalices. No repitas ideas.\n",
        "\"\"\"\n",
        "\n",
        "user_prompt_rag = \"\"\"\n",
        "Fragmentos recuperados:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "A continuación, presenta el análisis legal y la conclusión en tres partes:\n",
        "1. **Introducción breve** del problema legal.\n",
        "2. **Análisis jurídico** apoyado en los fragmentos recuperados, citando tus fuentes.\n",
        "3. **Conclusión clara**, con un juicio legal concreto.\n",
        "\"\"\"\n",
        "\n",
        "chat_prompt_rag = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_prompt_rag),\n",
        "    HumanMessagePromptTemplate.from_template(user_prompt_rag)\n",
        "])\n",
        "\n",
        "# --- 3. Definiciones para la respuesta conversacional ---\n",
        "conversational_prompt_template = \"\"\"\n",
        "Ya no eres un asistente legal, ahora eres un asistente de IA amigable y conversador. Responde directamente al usuario de forma natural.\n",
        "Usuario: {user_input}\n",
        "Asistente:\"\"\"\n",
        "PROMPT_CONVERSATIONAL = PromptTemplate(template=conversational_prompt_template, input_variables=[\"user_input\"])\n",
        "\n",
        "# --- 4. Crear la cadena usando load_qa_chain para el modelo de RAG ---\n",
        "chain_rag = load_qa_chain(llm, chain_type=\"stuff\", prompt=chat_prompt_rag)\n",
        "\n",
        "# --- 5. Bucle de Consulta Interactivo ---\n",
        "\n",
        "print(\"-\"*50)\n",
        "print(\"Asistente listo. Escribe 'Exit' para salir.\")\n",
        "print(\"-\"*50)\n",
        "\n",
        "while True:\n",
        "    query = input(\"\\nPregunta: \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "    if not query.strip():\n",
        "        continue\n",
        "\n",
        "    # --- Paso 1: Clasificar la intención del usuario ---\n",
        "    intent = classify_intent_with_classifier(query, llm_evaluador)\n",
        "    print(f\"--- Intención Detectada: {intent} ---\")\n",
        "\n",
        "    # --- Ejecutar flujo según la intención ---\n",
        "    if intent == \"USAR_RAG\":\n",
        "        print(\"--- Ejecutando RAG para consulta legal ---\")\n",
        "        try:\n",
        "            # 1. Recuperar documentos (RAG)\n",
        "            retriever.search_kwargs = {'k': 3}  # Ajusta 'k' si es necesario\n",
        "            docs_retrieved = retriever.invoke(query)\n",
        "\n",
        "            # 2. Preparar input para la cadena RAG\n",
        "            input_data = {\n",
        "                \"input_documents\": docs_retrieved,\n",
        "                \"question\": query\n",
        "            }\n",
        "\n",
        "            # 3. Ejecutar la cadena RAG\n",
        "            result = chain_rag.invoke(input_data)\n",
        "\n",
        "            # 4. Imprimir resultado RAG\n",
        "            if isinstance(result, dict) and 'output_text' in result:\n",
        "                print(\"\\nRespuesta (Legal):\")\n",
        "                print(result['output_text'])\n",
        "            elif isinstance(result, str):\n",
        "                print(\"\\nRespuesta (Legal):\")\n",
        "                print(result)\n",
        "            else:\n",
        "                print(\"\\nRespuesta RAG recibida (formato inesperado):\")\n",
        "                print(result)\n",
        "\n",
        "            # 5. Imprimir fuentes (Opcional)\n",
        "            print(\"\\nFuentes Recuperadas:\")\n",
        "            for i, doc in enumerate(docs_retrieved):\n",
        "                source = doc.metadata.get('source', 'N/A')\n",
        "                page = doc.metadata.get('page', 'N/A')\n",
        "                print(f\"  [{i+1}] Fuente: {source}, Página: {page}\")\n",
        "                print(f\"     Fragmento: {doc.page_content[:150]}...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError durante la ejecución de la cadena RAG: {e}\")\n",
        "\n",
        "    elif intent == \"NO_RAG\":\n",
        "        print(\"--- Generando respuesta sin usar RAG ---\")\n",
        "        try:\n",
        "            # 1. Formatear prompt conversacional simple\n",
        "            simple_prompt = PROMPT_CONVERSATIONAL.format(user_input=query)\n",
        "\n",
        "            # 2. Llamar directamente al LLM (sin RAG)\n",
        "            response = llm.invoke(simple_prompt)\n",
        "\n",
        "            # 3. Extraer y limpiar la respuesta\n",
        "            if isinstance(response, dict):\n",
        "                 # Adapta la clave si 'invoke' devuelve un dict con otra clave para el texto\n",
        "                 response_text = response.get('text', str(response))\n",
        "            elif isinstance(response, str):\n",
        "                 response_text = response\n",
        "            else:\n",
        "                 response_text = str(response)\n",
        "\n",
        "            # Limpieza: quitar el prompt si el LLM lo repite\n",
        "            print(\"\\nRespuesta (Conversacional):\")\n",
        "            print(response_text)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError durante la llamada conversacional al LLM: {e}\")\n",
        "    else:\n",
        "        print(f\"--- ERROR: Intención desconocida '{intent}' recibida de la clasificación ---\")\n",
        "\n",
        "print(\"\\nSaliendo del asistente.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHWFiDgzo35z"
      },
      "source": [
        "Vemos que ahora si le decimos algo que no es claramente algo relacionado con temas legales, como \"Hola me llamo Sergio\" se detecta que es una conversación normal y no se activa el RAG:\n",
        "\n",
        "\n",
        "> Pregunta: Hola me llamo sergio\n",
        "\n",
        "\n",
        "> Respuesta (Conversacional):\n",
        "Hola Sergio, ¿en qué puedo ayudarte hoy?\n",
        "\n",
        "¿Pero que pasa si hacemos una pregunta legal y luego seguimos intentando consultar al modelo sobre la pregunta inicial?\n",
        "\n",
        "\n",
        "\n",
        "> Pregunta:  Tengo una consulta sobre protección de datos, mi gimnasio me pide mi huella dactilar para poder entrar al gimnasio, ¿pueden exirgirmelo?\n",
        "\n",
        "\n",
        "> Respuesta (Legal):\n",
        "**Introducción**: Un gimnasio quiere usar datos biométricos (como la huella dactilar) para controlar el acceso a sus instalaciones. Este tratamiento de datos personales requiere una base legal clara y garantías de seguridad adecuadas.\n",
        "**Análisis**: El uso de datos biométricos para fines como el control de acceso debe basarse en un principio de consentimiento explícito, salvo que exista una justificación legal específica (como la seguridad pública). En este caso, se deben aplicar medidas adicionales para proteger estos datos sensibles, ya que permiten identificar de forma única a cada persona. Además, el gimnasio debe informar claramente sobre el propósito del tratamiento y las garantías de seguridad adoptadas.\n",
        "**Conclusión**: Para que este tratamiento sea lícito, debe existir una base legal clara (como el consentimiento informado o una obligación legal) y se deben aplicar medidas adecuadas para proteger estos datos sensibles.\n",
        "\n",
        "\n",
        "\n",
        "> Pregunta: ¿Puedo negarme?\n",
        "\n",
        "\n",
        "> Respuesta (Legal):\n",
        "Una empresa farmacéutica me ha solicitado información personal para estudiar la eficacia de un medicamento, pero prefiero no participar. ¿Tengo derecho a negarme?\n",
        "Análisis legal: Por ser un estudio clínico, se requiere tu consentimiento explícito. Si no das tu autorización, no pueden procesar tus datos. Sin embargo, si la investigación tiene relevancia para el bienestar público o hay una obligación legal, podrían excepcionalmente utilizar los datos anónimos o con medidas de protección específicas.\n",
        "Conclusión: Tienes derecho a negarte si el estudio clínico no cumple con tus derechos y garantías de privacidad.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Podemos observar que el modelo al activarse el RAG pierde el contexto de la pregunta anterior y responde algo totalmente diferente, seguramente debido a que RAG ha recuperado textos donde se incluya \"¿puedo negarme?\" que no estan relacionados con el contexto anterior, por lo que se inyecta contexto erroneo en el propmt.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8LOwn4wq58V"
      },
      "source": [
        "## Memoria Conversacional con RAG\n",
        "\n",
        "Para solucionar el problema de continuidad del contexto en la conversación vamos a añadir memoria al RAG, para ello, LangChain tiene una variedad de herramientas que permite implementarlo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mERu0HnsDe6",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain  # Necesario para la clasificación\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "import torch\n",
        "\n",
        "# Añadimos una clase de memoria conversacional\n",
        "class ConversationMemory:\n",
        "    def __init__(self, max_turns=10):\n",
        "        self.max_turns = max_turns\n",
        "        self.history = []\n",
        "\n",
        "    def add_turn(self, user_input, assistant_response):\n",
        "        self.history.append((user_input, assistant_response))\n",
        "        if len(self.history) > self.max_turns:\n",
        "            self.history.pop(0)\n",
        "\n",
        "    def get_memory_as_text(self):\n",
        "        return \"\\n\".join([f\"Usuario: {u}\\nAsistente: {a}\" for u, a in self.history])\n",
        "\n",
        "# --- 1. Función de Clasificación de Intención (Modelo de Clasificación Separado) ---\n",
        "def classify_intent_with_classifier(query, llm_evaluador):\n",
        "    \"\"\"Clasifica si se necesita usar RAG para generar la respuesta.\"\"\"\n",
        "    classification_prompt_template = \"\"\"\n",
        "        Analiza la siguiente consulta del usuario. Decide si es necesario usar RAG (fragmentos legales recuperados) o si el modelo puede responder directamente.\n",
        "\n",
        "        Solo debes responder con:\n",
        "        - \"USAR_RAG\" → si se requiere un análisis legal riguroso, citas precisas o fundamentación jurídica específica.\n",
        "        - \"NO_RAG\" → si el modelo puede dar una respuesta general, introductoria o basada en conocimiento común sin necesidad de fragmentos legales.\n",
        "\n",
        "        Ejemplos:\n",
        "        - Pregunta: \"¿Puede una empresa ceder mis datos sin consentimiento?\" → NO_RAG\n",
        "        - Pregunta: \"¿Qué dice el artículo 6 del RGPD sobre el consentimiento?\" → USAR_RAG\n",
        "        - Pregunta: \"¿Puedes citar el artículo 13 del RGPD?\" → USAR_RAG\n",
        "        - Pregunta: \"¿En qué artículo se regula el consentimiento explícito?\" → USAR_RAG\n",
        "        - Pregunta: \"Puedes decirme en qué artículos te basas?\" → USAR_RAG\n",
        "        - Pregunta: \"Cita textualmente en qué artículos te basas\" → USAR_RAG\n",
        "        - Pregunta: \"¿Qué derechos tengo como interesado?\" → NO_RAG\n",
        "        - Pregunta: \"¿Cómo elimino mis datos personales?\" → NO_RAG\n",
        "\n",
        "\n",
        "        Pregunta del usuario: \"{user_query}\"\n",
        "\n",
        "        Clasificación (USAR_RAG o NO_RAG):\"\"\"\n",
        "\n",
        "    prompt = PromptTemplate(template=classification_prompt_template, input_variables=[\"user_query\"])\n",
        "\n",
        "    classification_runnable = prompt | llm_evaluador\n",
        "\n",
        "    try:\n",
        "        result = classification_runnable.invoke({\"user_query\": query})\n",
        "\n",
        "        # Extraer texto. Ajusta la clave ('text') si tu LLM devuelve otra cosa.\n",
        "        if isinstance(result, dict):\n",
        "            answer_text = result.get('text', '').strip().upper()\n",
        "        elif isinstance(result, str):\n",
        "            answer_text = result.strip().upper()\n",
        "        else:\n",
        "            answer_text = str(result).strip().upper()\n",
        "\n",
        "        # Debug de la clasificación\n",
        "        print(f\"--- DEBUG: Clasificación LLM respondió: '{answer_text}' ---\")\n",
        "\n",
        "        if \"USAR_RAG\" in answer_text.upper():\n",
        "            return \"USAR_RAG\"\n",
        "        elif \"NO_RAG\" in answer_text:\n",
        "            return \"NO_RAG\"\n",
        "        else:\n",
        "            print(\"--- WARN: Respuesta de clasificación no clara, asumiendo NO_RAG por seguridad ---\")\n",
        "            return \"NO_RAG\"  # Default seguro\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error en la clasificación de intención con LLM: {e}\")\n",
        "        return \"USAR_RAG\"  # Default seguro en caso de error\n",
        "\n",
        "# --- 2. Definiciones para el prompt de RAG ---\n",
        "system_prompt_rag = \"\"\"\n",
        "Eres un experto legal en protección de datos en España.\n",
        "Tu tarea es analizar la legalidad de las situaciones planteadas por el usuario utilizando fragmentos recuperados.\n",
        "La respuesta debe ser clara, rigurosa y formal, como si fuera escrita por un profesional del derecho.\n",
        "No hagas suposiciones. No generalices. No repitas ideas.\n",
        "\"\"\"\n",
        "\n",
        "user_prompt_rag = \"\"\"\n",
        "Fragmentos recuperados:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "A continuación, presenta el análisis legal y la conclusión en tres partes:\n",
        "1. **Introducción breve** del problema legal.\n",
        "2. **Análisis jurídico** apoyado en los fragmentos recuperados, citando tus fuentes.\n",
        "3. **Conclusión clara**, con un juicio legal concreto.\n",
        "\"\"\"\n",
        "\n",
        "chat_prompt_rag = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_prompt_rag),\n",
        "    HumanMessagePromptTemplate.from_template(user_prompt_rag)\n",
        "])\n",
        "\n",
        "# --- 3. Definiciones para la respuesta conversacional ---\n",
        "conversational_prompt_template = \"\"\"\n",
        "Eres un asistente de IA amigable y conversador. Tu tarea es mantener una conversación fluida y coherente con el usuario.\n",
        "Responde de forma natural y cercana, teniendo en cuenta lo que ya se ha dicho.\n",
        "\n",
        "Historial reciente de la conversación:\n",
        "{chat_history}\n",
        "\n",
        "Usuario: {user_input}\n",
        "Asistente:\"\"\"\n",
        "\n",
        "PROMPT_CONVERSATIONAL = PromptTemplate(\n",
        "    template=conversational_prompt_template,\n",
        "    input_variables=[\"chat_history\", \"user_input\"]\n",
        ")\n",
        "\n",
        "# --- 4. Crear la cadena usando load_qa_chain para el modelo de RAG ---\n",
        "chain_rag = load_qa_chain(llm, chain_type=\"stuff\", prompt=chat_prompt_rag)\n",
        "\n",
        "# --- 5. Bucle de Consulta Interactivo ---\n",
        "\n",
        "print(\"-\"*50)\n",
        "print(\"Asistente listo. Escribe 'Exit' para salir.\")\n",
        "print(\"-\"*50)\n",
        "memory = ConversationMemory(max_turns=6)\n",
        "\n",
        "while True:\n",
        "    query = input(\"\\nPregunta: \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "    if not query.strip():\n",
        "        continue\n",
        "\n",
        "    intent = classify_intent_with_classifier(query, llm_evaluador)\n",
        "    print(f\"--- Intención Detectada: {intent} ---\")\n",
        "\n",
        "    if intent == \"USAR_RAG\":\n",
        "        print(\"--- Ejecutando RAG para consulta legal ---\")\n",
        "        try:\n",
        "            retriever.search_kwargs = {'k': 3}\n",
        "            docs_retrieved = retriever.invoke(query)\n",
        "\n",
        "            input_data = {\n",
        "                \"input_documents\": docs_retrieved,\n",
        "                \"question\": query\n",
        "            }\n",
        "\n",
        "            result = chain_rag.invoke(input_data)\n",
        "            response_text = result.get('output_text', result if isinstance(result, str) else str(result))\n",
        "\n",
        "            print(\"\\nRespuesta (Legal):\")\n",
        "            print(response_text)\n",
        "\n",
        "            memory.add_turn(query, response_text)\n",
        "\n",
        "            print(\"\\nFuentes Recuperadas:\")\n",
        "            for i, doc in enumerate(docs_retrieved):\n",
        "                source = doc.metadata.get('source', 'N/A')\n",
        "                page = doc.metadata.get('page', 'N/A')\n",
        "                print(f\"  [{i+1}] Fuente: {source}, Página: {page}\")\n",
        "                print(f\"     Fragmento: {doc.page_content[:150]}...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError durante la ejecución de la cadena RAG: {e}\")\n",
        "\n",
        "    elif intent == \"NO_RAG\":\n",
        "        print(\"--- Generando respuesta sin usar RAG ---\")\n",
        "        try:\n",
        "            prompt_with_memory = PROMPT_CONVERSATIONAL.format(\n",
        "                chat_history=memory.get_memory_as_text(),\n",
        "                user_input=query\n",
        "            )\n",
        "\n",
        "            response = llm.invoke(prompt_with_memory)\n",
        "\n",
        "            if isinstance(response, dict):\n",
        "                response_text = response.get('text', str(response))\n",
        "            elif isinstance(response, str):\n",
        "                response_text = response\n",
        "            else:\n",
        "                response_text = str(response)\n",
        "\n",
        "            if \"Asistente:\" in response_text:\n",
        "                response_text = response_text.split(\"Asistente:\", 1)[-1].strip()\n",
        "\n",
        "            print(\"\\nRespuesta (Conversacional):\")\n",
        "            print(response_text)\n",
        "\n",
        "            memory.add_turn(query, response_text)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError durante la llamada conversacional al LLM: {e}\")\n",
        "    else:\n",
        "        print(f\"--- ERROR: Intención desconocida '{intent}' recibida de la clasificación ---\")\n",
        "\n",
        "print(\"\\nSaliendo del asistente.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdnreWql09_8"
      },
      "source": [
        "# APLICACION FINAL MEJORADA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoRlZhzgsDe7",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip install -U langchain langchain-huggingface transformers\n",
        "!pip install -U langchain-ollama\n",
        "!pip install sentence-transformers\n",
        "!pip install pypdf\n",
        "!pip install -qU langchain-community faiss-cpu\n",
        "!pip install -U scikit-learn\n",
        "!pip install tiktoken\n",
        "import logging\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Configuración de logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(\"asistente_legal.log\"),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(\"asistente_legal\")\n",
        "logger.info(\"Iniciando asistente legal de protección de datos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXAmHnOPsDe7"
      },
      "outputs": [],
      "source": [
        "# Celda 2: Carga y procesamiento de documentos mejorado\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import re\n",
        "import tiktoken\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Lista de rutas de los PDFs\n",
        "pdf_paths = [\n",
        "    \"BOE-A-1978-31229-consolidado.pdf\",\n",
        "    \"BOE-A-1995-25444-consolidado_CodigoPenal.pdf\",\n",
        "    \"Protección de Datos Personales y garantia de los derechos digitales.pdf\",\n",
        "    \"RGPD_boe.pdf\"\n",
        "]\n",
        "\n",
        "# Función para identificar tipo de documento y mejorar metadatos\n",
        "def enrich_metadata(doc):\n",
        "    filename = doc.metadata.get('source', '')\n",
        "\n",
        "    # Añadir información sobre tipo de documento\n",
        "    if \"RGPD\" in filename:\n",
        "        doc.metadata['tipo'] = 'RGPD'\n",
        "        doc.metadata['jerarquia'] = 'Reglamento Europeo'\n",
        "    elif \"BOE-A-1978\" in filename:\n",
        "        doc.metadata['tipo'] = 'Constitución'\n",
        "        doc.metadata['jerarquia'] = 'Constitución Española'\n",
        "    elif \"BOE-A-1995\" in filename:\n",
        "        doc.metadata['tipo'] = 'Código Penal'\n",
        "        doc.metadata['jerarquia'] = 'Ley Orgánica'\n",
        "    elif \"Protección de Datos\" in filename:\n",
        "        doc.metadata['tipo'] = 'LOPDGDD'\n",
        "        doc.metadata['jerarquia'] = 'Ley Orgánica'\n",
        "\n",
        "    # Extraer información de artículos si está disponible\n",
        "    article_match = re.search(r'Artículo (\\d+)', doc.page_content)\n",
        "    if article_match:\n",
        "        doc.metadata['tipo_contenido'] = 'artículo'\n",
        "        doc.metadata['num_articulo'] = article_match.group(1)\n",
        "\n",
        "    return doc\n",
        "\n",
        "# Cargar documentos de todos los PDFs con manejo de errores\n",
        "documents = []\n",
        "for path in pdf_paths:\n",
        "    try:\n",
        "        logger.info(f\"Cargando documento: {path}\")\n",
        "        loader = PyPDFLoader(path)\n",
        "        documents.extend(loader.load())\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error cargando {path}: {e}\")\n",
        "\n",
        "# Limpiar y normalizar contenido\n",
        "for doc in documents:\n",
        "    doc.page_content = doc.page_content.replace('\\r\\n', '\\n').replace('\\n\\n', '\\n').strip()\n",
        "    doc = enrich_metadata(doc)\n",
        "\n",
        "logger.info(f\"Documentos cargados: {len(documents)}\")\n",
        "\n",
        "# Splitter optimizado para textos legales\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=800,  # Tamaño optimizado para capturar contexto legal completo\n",
        "    chunk_overlap=200,  # Mayor overlap para mantener coherencia\n",
        "    separators=[\"\\n\\n\\n\", \"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
        "    encoding_name=\"cl100k_base\"  # Compatible con modelos modernos\n",
        ")\n",
        "\n",
        "docs = text_splitter.split_documents(documents=documents)\n",
        "logger.info(f\"Documentos divididos en {len(docs)} fragmentos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SkrBhqrsDe7"
      },
      "outputs": [],
      "source": [
        "# Celda 3: Embeddings mejorados y sistema de recuperación\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "\n",
        "# Configuración de embeddings para español y textos jurídicos\n",
        "embedding_model_name = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
        "model_kwargs = {\"device\": \"cpu\"}\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=embedding_model_name,\n",
        "    model_kwargs=model_kwargs\n",
        ")\n",
        "\n",
        "# Crear vector store\n",
        "logger.info(\"Creando índice vectorial FAISS\")\n",
        "vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "# Guardar y recargar el vector store\n",
        "vectorstore.save_local(\"faiss_index_\")\n",
        "persisted_vectorstore = FAISS.load_local(\"faiss_index_\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "# Crear retriever base con MMR para diversidad de resultados\n",
        "base_retriever = persisted_vectorstore.as_retriever(\n",
        "    search_type=\"mmr\",  # Maximum Marginal Relevance para diversidad\n",
        "    search_kwargs={\n",
        "        \"k\": 5,  # Recuperar más documentos inicialmente\n",
        "        \"fetch_k\": 10,  # Considerar más candidatos\n",
        "        \"lambda_mult\": 0.7  # Balance entre relevancia y diversidad\n",
        "    }\n",
        ")\n",
        "\n",
        "logger.info(f\"Se indexaron {len(docs)} fragmentos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53Lwzj1esDe7"
      },
      "outputs": [],
      "source": [
        "# Celda 4: Configuración del modelo LLM\n",
        "from langchain_ollama import OllamaLLM\n",
        "import time\n",
        "\n",
        "# Inicializar el modelo LLaMA con timeout\n",
        "llm_legal = OllamaLLM(\n",
        "    model=\"hf.co/serdom02/Leyeneitor_8bitQ8_0\",\n",
        "    base_url=\"http://127.0.0.1:11434\",\n",
        "    temperature=0.3,  # Menor temperatura para respuestas legales más precisas\n",
        "    timeout=60  # Timeout de 60 segundos para evitar bloqueos\n",
        ")\n",
        "llm_conversacion = OllamaLLM(\n",
        "    model=\"llama3:8b\",\n",
        "    base_url=\"http://127.0.0.1:11434\",\n",
        "    temperature=0.6, # temperatura más alta para conversación\n",
        "    timeout=60\n",
        ")\n",
        "# Inicializar un modelo para evaluación/clasificación (puede ser el mismo)\n",
        "llm_evaluador = llm_conversacion\n",
        "\n",
        "llm=llm_legal #El modelo principal va a ser nuestro LLM Finetuneado\n",
        "\n",
        "# Test rápido del modelo\n",
        "try:\n",
        "    response = llm_legal.invoke(\"Un breve ejemplo sobre protección de datos\")\n",
        "    print(\"Test del modelo exitoso:\")\n",
        "    print(response)\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error al probar el modelo: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lgzgx2VYsDe7"
      },
      "outputs": [],
      "source": [
        "# --- 7. Prompts mejorados según tipología de consulta ---\n",
        "\n",
        "# Prompt base/genérico (fallback)\n",
        "system_prompt_rag = \"\"\"\n",
        "Sistema: Eres un asistente legal especializado en protección de datos en España (RGPD y LOPDGDD). Tu objetivo es proporcionar respuestas claras, precisas y fundamentadas en la ley.\n",
        "\n",
        "Contexto legal relevante extraído de la documentación:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Consulta del usuario: {question}\n",
        "\n",
        "Instrucciones:\n",
        "1. Basa tu respuesta EXCLUSIVAMENTE en el contexto legal proporcionado arriba.\n",
        "2. Si el contexto no es suficiente para responder, indica que la información no se encuentra en los documentos proporcionados.\n",
        "3. Responde de forma directa y estructurada a la consulta del usuario.\n",
        "4. Cita las fuentes (ej. \"Según el Artículo X del RGPD...\") si es posible basándote en el contexto.\n",
        "5. Evita dar opiniones personales o información no verificada.\n",
        "\"\"\"\n",
        "\n",
        "# Prompt para citas legales\n",
        "legal_citation_prompt = \"\"\"\n",
        "Sistema: Eres un asistente jurídico experto en el Reglamento General de Protección de Datos (RGPD). Tu tarea es citar artículos legales de forma textual y precisa cuando el usuario lo solicita.\n",
        "\n",
        "---------------------\n",
        "Documentos disponibles:\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Consulta del usuario:\n",
        "{question}\n",
        "\n",
        "Instrucciones:\n",
        "1. Extrae **todos los artículos del RGPD** que traten sobre el tema mencionado en la consulta.\n",
        "2. Para cada artículo encontrado, incluye:\n",
        "   - El número del artículo (ej. Artículo 6 del RGPD)\n",
        "   - El **texto literal más relevante** (puede ser un apartado si es muy largo)\n",
        "   - La **fuente** (ej. “Fuente: RGPD, pág. 12”)\n",
        "3. Usa este formato:\n",
        "   Artículo X del RGPD:\n",
        "   “Texto legal...”\n",
        "   Fuente: RGPD, pág. X\n",
        "\n",
        "4. Si hay varios artículos relevantes, enuméralos claramente.\n",
        "5. No inventes. Si el texto literal no está en el contexto, indícalo explícitamente.\n",
        "6. No incluyas artículos de otras leyes (como LOPDGDD o Constitución) salvo que se mencione expresamente.\n",
        "\n",
        "Ejemplo:\n",
        "Artículo 6.1 del RGPD:\n",
        "“El tratamiento será lícito solo si se cumple al menos una de las siguientes condiciones: [...]”\n",
        "Fuente: RGPD, pág. 8\n",
        "\"\"\"\n",
        "\n",
        "legal_multi_citation_prompt = \"\"\"\n",
        "Eres un asistente jurídico experto en el RGPD. El usuario solicita una lista de artículos relacionados con un tema específico.\n",
        "\n",
        "Instrucciones:\n",
        "1. Identifica todos los artículos del RGPD que se relacionen con el tema de la consulta.\n",
        "2. Si el artículo aparece en el contexto, cita su número y el **texto literal completo** relevante.\n",
        "3. Si no aparece el texto completo, menciona el número del artículo y una breve descripción basada en el título o lo que sepas del RGPD.\n",
        "4. No inventes textos. Si hay dudas, di que no tienes acceso al contenido exacto.\n",
        "5. Formatea la respuesta como una lista clara y numerada.\n",
        "\n",
        "Consulta: {question}\n",
        "Fragmentos del RGPD disponibles:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "Respuesta:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Prompt para análisis legal\n",
        "legal_analysis_prompt = \"\"\"\n",
        "Sistema: Eres un jurista experto analizando situaciones bajo la ley de protección de datos española (RGPD, LOPDGDD). Razonas jurídicamente paso a paso.\n",
        "\n",
        "Contexto legal relevante extraído de la documentación:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Consulta del usuario que requiere análisis legal: {question}\n",
        "\n",
        "Instrucciones para tu análisis:\n",
        "1. **Identifica la cuestión jurídica principal** planteada en la consulta.\n",
        "2. **Selecciona las normas aplicables** del contexto legal proporcionado que sean pertinentes para la cuestión.\n",
        "3. **Analiza los hechos implícitos o explícitos** en la consulta a la luz de las normas seleccionadas.\n",
        "4. **Aplica las normas a los hechos**, explicando tu razonamiento paso a paso basado únicamente en el contexto proporcionado.\n",
        "5. **Formula una conclusión jurídica** clara y fundamentada en el análisis anterior y el contexto. Si el contexto es insuficiente, señala las limitaciones.\n",
        "\n",
        "Estructura tu respuesta:\n",
        "* **Cuestión planteada:** (Resume la pregunta legal)\n",
        "* **Normativa aplicable (según contexto):** (Menciona artículos/disposiciones relevantes del contexto)\n",
        "* **Análisis jurídico:** (Desarrolla tu razonamiento aquí, conectando contexto y consulta)\n",
        "* **Conclusión:** (Respuesta final basada en el análisis)\n",
        "\"\"\"\n",
        "\n",
        "# Prompt para consultas procedimentales\n",
        "procedural_prompt = \"\"\"\n",
        "Sistema: Eres un consultor experto en los procedimientos y trámites relacionados con la protección de datos en España (AEPD, derechos ARSULIPO, etc.). Proporcionas información práctica.\n",
        "\n",
        "Contexto legal relevante extraído de la documentación (puede contener información sobre procedimientos):\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Consulta del usuario sobre un procedimiento o trámite: {question}\n",
        "\n",
        "Instrucciones:\n",
        "1. Identifica claramente el **procedimiento o trámite** consultado.\n",
        "2. Busca en el contexto información relevante sobre los **pasos a seguir, plazos, requisitos, o autoridad competente**.\n",
        "3. Explica el procedimiento de forma **clara, secuencial y práctica**, basándote en la información del contexto.\n",
        "4. Si el contexto menciona la base legal, puedes indicarla brevemente, pero prioriza la **descripción del proceso**.\n",
        "5. Si la información específica sobre el procedimiento no está en el contexto, indica que no se puede detallar con la documentación disponible. NO inventes pasos o plazos.\n",
        "\"\"\"\n",
        "\n",
        "# Prompt para información general\n",
        "general_info_prompt = \"\"\"\n",
        "Sistema: Eres un asistente informativo sobre protección de datos. Explicas conceptos generales de forma clara y sencilla.\n",
        "\n",
        "Contexto legal relevante extraído de la documentación:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Consulta general del usuario: {question}\n",
        "\n",
        "Instrucciones:\n",
        "1. Identifica el concepto o tema general sobre el que pregunta el usuario.\n",
        "2. Busca definiciones, explicaciones o principios relevantes en el contexto proporcionado.\n",
        "3. Explica el concepto de forma clara y concisa, utilizando la información del contexto.\n",
        "4. Puedes usar ejemplos si el contexto los proporciona o si son derivados directos de la explicación legal.\n",
        "5. Cita la fuente si es relevante (ej. \"El RGPD define X como...\").\n",
        "6. Si la información no está en el contexto, indícalo.\n",
        "\"\"\"\n",
        "\n",
        "# Prompt para conversación general (cuando no se usa RAG)\n",
        "conversation_prompt = \"\"\"\n",
        "Sistema: Eres un asistente legal amable y conversacional llamado Leyeneitor. Tu especialidad es la protección de datos, PERO ahora estás en modo conversacional. Ignora tu rol legal por un momento y responde directamente a la pregunta o comentario del usuario de forma natural y breve como un asistente general.\n",
        "NO des respuestas sobre protección de datos o leyes a menos que la pregunta sea específicamente sobre eso. Sé breve y directo.\n",
        "\n",
        "Historial reciente de la conversación (para contexto):\n",
        "{memory}\n",
        "Asistente:\"\"\"\n",
        "\n",
        "action_oriented_prompt = \"\"\"\n",
        "Sistema: Eres un asesor jurídico especializado en protección de datos (RGPD, LOPDGDD) y derechos digitales. Tu tarea es explicar de manera clara qué **acciones, derechos o reclamaciones** puede ejercer el usuario en la situación planteada.\n",
        "\n",
        "Contexto legal relevante extraído de la documentación:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Nueva consulta (enfocada en qué puede hacer el usuario): {question}\n",
        "\n",
        "Instrucciones específicas para tu respuesta:\n",
        "1. **Identifica los derechos relevantes** (acceso, oposición, supresión, portabilidad, etc.) que el usuario podría ejercer según el contexto.\n",
        "2. **Describe las acciones prácticas** que puede realizar el usuario, incluyendo:\n",
        "   - Cómo ejercer su derecho (por ejemplo: “solicitarlo por escrito”, “presentar reclamación ante la AEPD”).\n",
        "   - Ante quién debe dirigirse (empresa, delegado de protección de datos, AEPD...).\n",
        "   - Qué requisitos o pasos debe seguir.\n",
        "3. **Si procede,** menciona los artículos legales que respaldan las acciones propuestas (sin recargar la respuesta).\n",
        "4. Explica de forma **estructurada y práctica**, usando pasos o listas si facilita la comprensión.\n",
        "5. Si no hay suficiente información en el contexto para dar un procedimiento concreto, **indícalo claramente**. No inventes.\n",
        "\n",
        "Estructura sugerida para tu respuesta:\n",
        "* **Derechos aplicables:** (Enumera los derechos relevantes)\n",
        "* **Acciones que puede realizar:** (Pasos claros y prácticos)\n",
        "* **Normativa de respaldo:** (Artículos relevantes, si es aplicable)\n",
        "* **Notas importantes:** (Advertencias o limitaciones si las hubiera)\n",
        "\n",
        "Evita explicaciones teóricas largas. Sé claro, útil y orientado a lo que el usuario puede **hacer**.\n",
        "\"\"\"\n",
        "refinement_module_prompt = \"\"\"\n",
        "Eres un jurista experto en derecho de protección de datos.\n",
        "Tu tarea es **corregir y mejorar** una respuesta legal manteniendo su estructura original.\n",
        "\n",
        "\n",
        "Basándote en los siguientes fallos identificados por un validador, **ajusta cada sección que lo requiera** para mejorar la precisión y completitud jurídica.\n",
        "No elimines secciones ni cambies su orden. Si una sección no requiere corrección, déjala intacta.\n",
        "Si alguno de los fallos detectados se refiere a consentimiento, derechos fundamentales o proporcionalidad, asegúrate de explicarlos de forma técnica y precisa en la sección correspondiente.\n",
        "\n",
        "--- FALLAS DETECTADAS ---\n",
        "{fallos}\n",
        "\n",
        "Es obligatorio que sigas la Estructura de la respuesta original\n",
        "\n",
        "--- RESPUESTA ORIGINAL ---\n",
        "{respuesta}\n",
        "\n",
        "--- INSTRUCCIONES ---\n",
        "Corrige las secciones necesarias dentro de la estructura, sin añadir otras partes nuevas. Usa lenguaje jurídico claro y fundamentado.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ds2AfDdrsDe7",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Celda 5: Sistema completo con mejoras y correcciones\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain.chains import LLMChain # Aunque no se usa directamente, puede ser útil\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "import unicodedata\n",
        "import logging # Asegúrate de que el logger está configurado\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "# Obtener el logger configurado en la Celda 1\n",
        "logger = logging.getLogger(\"asistente_legal\")\n",
        "\n",
        "# --- 0. Para que cada usuario tenga su memoria ---\n",
        "class UsuarioSession:\n",
        "    def __init__(self):\n",
        "        self.memory = ImprovedMemory(max_turns=4)\n",
        "        self.last_query = None\n",
        "\n",
        "# --- 1. Memoria Conversacional Mejorada ---\n",
        "class ImprovedMemory:\n",
        "    def __init__(self, max_turns=6):\n",
        "        self.summary = \"\"\n",
        "        self.recent_turns = []\n",
        "        self.max_turns = max_turns\n",
        "        self.entities = {}  # Seguimiento de entidades legales\n",
        "\n",
        "    def get_all_turns(self):\n",
        "        \"\"\"Devuelve todos los turnos almacenados en la memoria\"\"\"\n",
        "        return self.recent_turns\n",
        "\n",
        "    def get_relevant_memory(self, query):\n",
        "        \"\"\"Devuelve memoria relevante para la consulta actual\"\"\"\n",
        "        # Versión sencilla: devolver todo el historial\n",
        "        return self.get_memory_as_text()\n",
        "        # Versión avanzada (requiere embeddings): pendiente\n",
        "\n",
        "    def add_turn(self, user_input, assistant_response, llm=None):\n",
        "        self.recent_turns.append((user_input, assistant_response))\n",
        "\n",
        "        # Extraer y rastrear entidades legales mencionadas\n",
        "        self._extract_entities(user_input + \" \" + assistant_response)\n",
        "\n",
        "        if len(self.recent_turns) > self.max_turns:\n",
        "            if llm_evaluador:  # Si tenemos acceso al LLM, generamos resumen, no usamos el modelo finetuneado para evitar sus sesgos legales\n",
        "                try:\n",
        "                    # Generar resumen del contexto anterior (turnos más antiguos) para ahorrar espacio del contexto\n",
        "                    context_to_summarize = \"\\n\".join([f\"U: {u}\\nA: {a}\" for u, a in self.recent_turns[:-self.max_turns]]) # Corregido: resumir los que se van a quitar\n",
        "                    if context_to_summarize: # Solo si hay algo que resumir\n",
        "                         prompt = f\"Resume brevemente los puntos legales clave de esta conversación anterior:\\n{context_to_summarize}\"\n",
        "                         new_summary = llm.invoke(prompt)\n",
        "                         # Concatenar resumen nuevo con el anterior si existe\n",
        "                         self.summary = f\"{self.summary}\\n{new_summary}\".strip()\n",
        "                         logger.info(\"Resumen de memoria actualizado.\")\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Error al generar resumen de memoria: {e}\")\n",
        "\n",
        "            # Mantener solo los turnos más recientes\n",
        "            self.recent_turns = self.recent_turns[-self.max_turns:]\n",
        "\n",
        "    def _extract_entities(self, text):\n",
        "        # Detección simple de entidades legales\n",
        "        patterns = {\n",
        "            'articulos': r'art(?:ículo|\\.)\\s+(\\d+)',\n",
        "            'leyes': r'(?:RGPD|LOPDGDD|Reglamento|Ley Orgánica|Constitución|Código Penal)'\n",
        "        }\n",
        "\n",
        "        for entity_type, pattern in patterns.items():\n",
        "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "            for match in matches:\n",
        "                entity = f\"{entity_type}_{match.upper()}\" # Normalizar nombre\n",
        "                self.entities[entity] = self.entities.get(entity, 0) + 1\n",
        "\n",
        "    def get_memory_as_text(self):\n",
        "        memory_text = \"\"\n",
        "        if self.summary:\n",
        "            memory_text += f\"Resumen de puntos legales anteriores:\\n{self.summary}\\n\\n\"\n",
        "\n",
        "        if self.recent_turns:\n",
        "             memory_text += \"Historial reciente de la conversación:\\n\"\n",
        "             memory_text += \"\\n\".join([f\"Usuario: {u}\\nAsistente: {a}\" for u, a in self.recent_turns])\n",
        "\n",
        "        # Añadir entidades más relevantes si existen\n",
        "        if self.entities:\n",
        "            top_entities = sorted(self.entities.items(), key=lambda x: x[1], reverse=True)[:3]\n",
        "            if top_entities:\n",
        "                memory_text += \"\\n\\nTemas legales clave mencionados: \" + \", \".join([e[0].replace('_', ' ') for e in top_entities])\n",
        "\n",
        "        return memory_text.strip()\n",
        "\n",
        "# --- 2. Sistema de Clasificación Avanzado (Versión única y completa) ---\n",
        "def classify_intent_advanced(query, llm_evaluador):\n",
        "\n",
        "    \"\"\"Sistema de clasificación avanzado con múltiples categorías y confianza\"\"\"\n",
        "    #Sistema cache para no reclasificar preguntas parecidas\n",
        "    cache_key = ' '.join(query.lower().split()[:10])  # Simplificamos clave: primeras 10 palabras\n",
        "    if cache_key in intent_classification_cache:\n",
        "        logger.info(f\"Resultado de clasificación obtenido de caché para: {cache_key}\")\n",
        "        return intent_classification_cache[cache_key]\n",
        "\n",
        "\n",
        "    classification_prompt_text = \"\"\"\n",
        "    Analiza la siguiente consulta legal y clasifícala en UNA de estas categorías:\n",
        "\n",
        "    1. LEGAL_CITATION: Requiere citar artículos específicos o textos legales exactos (e.g., \"¿Qué dice el artículo 5 del RGPD?\", \"Cita el artículo 18.4 de la Constitución\")\n",
        "    2. LEGAL_ANALYSIS: Requiere análisis jurídico basado en leyes/normativas (e.g., \"¿Es legal tratar datos de salud sin consentimiento explícito?\", \"¿Qué implicaciones tiene la sentencia X?\")\n",
        "    3. GENERAL_INFO: Información general sobre protección de datos (e.g., \"¿Qué es el RGPD?\", \"¿Cuáles son los derechos de los ciudadanos?\")\n",
        "    4. PROCEDURAL: Preguntas sobre procedimientos o trámites (e.g., \"¿Cómo puedo ejercer mi derecho de acceso?\", \"¿Qué pasos seguir para una reclamación en la AEPD?\")\n",
        "    5. CONVERSATION: Diálogo general, saludos, agradecimientos o consulta no legal (e.g., \"Hola\", \"Gracias\", \"¿Qué tiempo hace?\")\n",
        "\n",
        "    Responde SOLO con la categoría y un número del 1-100 que indique tu confianza, separados por un guion.\n",
        "    Formato: [CATEGORÍA] - [CONFIANZA]\n",
        "\n",
        "    Ejemplos:\n",
        "    Consulta: \"¿Qué dice el artículo 6 del RGPD?\" -> Respuesta: LEGAL_CITATION - 95\n",
        "    Consulta: \"¿Es legal que una empresa comparta mis datos sin permiso?\" -> Respuesta: LEGAL_ANALYSIS - 85\n",
        "    Consulta: \"¿Qué es la protección de datos?\" -> Respuesta: GENERAL_INFO - 90\n",
        "    Consulta: \"¿Cómo presento una reclamación a la AEPD?\" -> Respuesta: PROCEDURAL - 88\n",
        "    Consulta: \"Gracias por tu ayuda\" -> Respuesta: CONVERSATION - 92\n",
        "    Consulta: \"Hola buenos días\" -> Respuesta: CONVERSATION - 99\n",
        "    Consulta: \"Qué tal estás?\" -> Respuesta: CONVERSATION - 95\n",
        "    Consulta: \"Me llamo Paula\" -> Respuesta: CONVERSATION - 98\n",
        "    Consulta: \"Ok gracias\" -> Respuesta: CONVERSATION - 90\n",
        "    Consulta: \"¿Recuerdas mi nombre?\" -> Respuesta: CONVERSATION - 98\n",
        "    Consulta: \"¿De qué hemos hablado antes?\" -> Respuesta: CONVERSATION - 95\n",
        "    Consulta: \"¿Puedes resumir nuestra conversación?\" -> Respuesta: CONVERSATION - 90\n",
        "    Consulta: \"¿Eres un abogado?\" -> Respuesta: CONVERSATION - 92\n",
        "\n",
        "    Consulta del usuario: \"{user_query}\"\n",
        "    Clasificación y confianza:\"\"\"\n",
        "\n",
        "    prompt = PromptTemplate(template=classification_prompt_text, input_variables=[\"user_query\"])\n",
        "    # No se necesita crear una cadena completa aquí, solo invocar el LLM con el prompt formateado.\n",
        "    # classification_runnable = prompt | llm_evaluador # Esto crea una cadena, innecesario si solo invocas\n",
        "\n",
        "    try:\n",
        "        formatted_prompt = prompt.format(user_query=query)\n",
        "        result = llm_evaluador.invoke(formatted_prompt) # Invocar directamente\n",
        "\n",
        "        # Extraer texto y procesar la respuesta\n",
        "        answer_text = result.strip() if isinstance(result, str) else str(result).strip()\n",
        "\n",
        "        # Intentar extraer categoría y confianza\n",
        "        pattern = r'([A-Z_]+)\\s*-\\s*(\\d+)'\n",
        "        match = re.search(pattern, answer_text)\n",
        "\n",
        "        if match:\n",
        "            category = match.group(1)\n",
        "            confidence = int(match.group(2))\n",
        "            logger.info(f\"Clasificación: {category}, Confianza: {confidence}\")\n",
        "\n",
        "            # Determinar si usar RAG (basado en categoría O baja confianza)\n",
        "            # Ajusta esta lógica si prefieres usar RAG para GENERAL_INFO o PROCEDURAL también\n",
        "            use_rag = category in [\"LEGAL_CITATION\", \"LEGAL_ANALYSIS\", \"PROCEDURAL\"] or confidence < 75\n",
        "\n",
        "            return {\n",
        "                \"category\": category,\n",
        "                \"confidence\": confidence,\n",
        "                \"use_rag\": use_rag\n",
        "            }\n",
        "        else:\n",
        "            logger.warning(f\"No se pudo extraer categoría/confianza de la respuesta del clasificador: '{answer_text}'. Se usará RAG por defecto.\")\n",
        "            return {\n",
        "                \"category\": \"UNKNOWN\",\n",
        "                \"confidence\": 0,\n",
        "                \"use_rag\": True # Default seguro\n",
        "            }\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error en clasificación avanzada: {e}\")\n",
        "        return {\n",
        "            \"category\": \"ERROR\",\n",
        "            \"confidence\": 0,\n",
        "            \"use_rag\": True # Default seguro\n",
        "        }\n",
        "\n",
        "# --- 3. Estrategia de retrieval adaptativo ---\n",
        "def get_adaptive_retriever(query, base_retriever, llm_evaluador):\n",
        "    \"\"\"Devuelve un retriever configurado dinámicamente según la consulta\"\"\"\n",
        "\n",
        "    # Analizar complejidad y especificidad de la consulta\n",
        "    complexity_prompt = \"\"\"\n",
        "    Evalúa la complejidad y especificidad de esta consulta legal:\n",
        "    \"{query}\"\n",
        "\n",
        "    Responde solo con una de estas opciones:\n",
        "    - SIMPLE: Consulta general o introductoria\n",
        "    - MEDIA: Consulta moderadamente específica\n",
        "    - COMPLEJA: Consulta muy específica o técnica\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        complexity_result = llm_evaluador.invoke(complexity_prompt.replace(\"{query}\", query))\n",
        "        complexity = complexity_result.strip().upper()\n",
        "\n",
        "        filter_dict = None # Inicializar filtro\n",
        "\n",
        "        # Ajustar parámetros según complejidad\n",
        "        if \"SIMPLE\" in complexity:\n",
        "            k_value = 3 # Aumentado ligeramente para consultas simples\n",
        "            fetch_k = 6\n",
        "            lambda_mult = 0.6\n",
        "        elif \"MEDIA\" in complexity:\n",
        "            k_value = 5 # Aumentado\n",
        "            fetch_k = 10\n",
        "            lambda_mult = 0.7\n",
        "        else:  # COMPLEJA\n",
        "            k_value = 7 # Aumentado\n",
        "            fetch_k = 15\n",
        "            lambda_mult = 0.8\n",
        "\n",
        "            # Intentar determinar qué ley es más relevante para filtrar (si es compleja)\n",
        "            law_prompt = \"\"\"\n",
        "            Para esta consulta compleja: \"{query}\"\n",
        "            ¿Qué normativa parece MÁS relevante? Responde solo con UNA palabra clave:\n",
        "            - RGPD\n",
        "            - LOPDGDD\n",
        "            - CONSTITUCION\n",
        "            - CODIGO_PENAL\n",
        "            - OTRO (si no encaja claramente o requiere múltiples)\n",
        "            \"\"\"\n",
        "\n",
        "            law_result = llm_evaluador.invoke(law_prompt.replace(\"{query}\", query)).strip().upper()\n",
        "\n",
        "            # Configurar filtro si hay una ley específica y clara\n",
        "            if law_result == \"RGPD\":\n",
        "                filter_dict = {\"tipo\": \"RGPD\"}\n",
        "            elif law_result == \"LOPDGDD\":\n",
        "                filter_dict = {\"tipo\": \"LOPDGDD\"}\n",
        "            elif law_result == \"CONSTITUCION\":\n",
        "                filter_dict = {\"tipo\": \"Constitución\"}\n",
        "            elif law_result == \"CODIGO_PENAL\":\n",
        "                filter_dict = {\"tipo\": \"Código Penal\"}\n",
        "            # else: filtro sigue siendo None\n",
        "\n",
        "        # Configurar retriever con los parámetros adaptativos\n",
        "        # ¡Importante! Crear una *nueva* instancia o clonar para no modificar el base_retriever original globalmente\n",
        "        # Nota: as_retriever() crea una nueva instancia configurada\n",
        "        adaptive_retriever = persisted_vectorstore.as_retriever( # Asume persisted_vectorstore es global o pasado como argumento\n",
        "            search_type=\"mmr\",\n",
        "            search_kwargs={\n",
        "                'k': k_value,\n",
        "                'fetch_k': fetch_k,\n",
        "                'lambda_mult': lambda_mult,\n",
        "                # Aplicar filtro si se determinó uno\n",
        "                **({'filter': filter_dict} if filter_dict else {})\n",
        "            }\n",
        "        )\n",
        "\n",
        "        logger.info(f\"Retriever adaptativo configurado: k={k_value}, fetch_k={fetch_k}, lambda={lambda_mult}, filtro={filter_dict}\")\n",
        "        return adaptive_retriever\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Error configurando retriever adaptativo: {e}. Usando configuración por defecto.\")\n",
        "        # Devolver el retriever base original o uno con config por defecto\n",
        "        return base_retriever # O reconfigurar base_retriever con valores por defecto\n",
        "\n",
        "\n",
        "# --- 4. Sistema de validación y reintento ---\n",
        "def validate_legal_response(response, query, docs_used, llm): #Aqui usamos el modelo finetuneado porque es mas preciso para temas legales\n",
        "    \"\"\"Valida la calidad de una respuesta legal\"\"\"\n",
        "\n",
        "    validation_prompt = f\"\"\"\n",
        "    Evalúa la calidad de esta respuesta legal:\n",
        "\n",
        "    Consulta: {query}\n",
        "    Respuesta: {response}\n",
        "\n",
        "    Verifica SOLO estos 3 aspectos:\n",
        "    1. ¿Es jurídicamente precisa según la legislación española y europea de protección de datos? (Sí/No)\n",
        "    2. ¿Responde completamente a la consulta realizada? (Sí/No)\n",
        "    3. ¿Contiene contradicciones internas o errores evidentes? (Sí/No)\n",
        "\n",
        "    Responde estrictamente en este formato: [PRECISIÓN: Sí/No], [COMPLETITUD: Sí/No], [ERRORES: Sí/No]\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        result = llm.invoke(validation_prompt)\n",
        "\n",
        "        # Patrones de validación más robustos (ignorando mayúsculas/minúsculas y espacios)\n",
        "        precision_match = re.search(r'PRECISIÓN:\\s*(Sí|No)', result, re.IGNORECASE)\n",
        "        completitud_match = re.search(r'COMPLETITUD:\\s*(Sí|No)', result, re.IGNORECASE)\n",
        "        errores_match = re.search(r'ERRORES:\\s*(Sí|No)', result, re.IGNORECASE) # Busca 'No' para sin_errores\n",
        "\n",
        "        # Extraer resultados\n",
        "        precision_ok = precision_match and \"sí\" in precision_match.group(1).lower()\n",
        "        completitud_ok = completitud_match and \"sí\" in completitud_match.group(1).lower()\n",
        "        sin_errores = errores_match and \"no\" in errores_match.group(1).lower() # Es bueno si NO hay errores\n",
        "\n",
        "        # Calcular validez general\n",
        "        is_valid = precision_ok and completitud_ok and sin_errores\n",
        "\n",
        "        validation_result = {\n",
        "            \"valid\": is_valid,\n",
        "            \"precision\": precision_ok,\n",
        "            \"completitud\": completitud_ok,\n",
        "            \"sin_errores\": sin_errores,\n",
        "            \"raw_validation_output\": result.strip() # Guardar la salida cruda para depuración\n",
        "        }\n",
        "\n",
        "        logger.info(f\"Validación: {validation_result}\")\n",
        "        return validation_result\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error en validación de respuesta: {e}\")\n",
        "        return {\"valid\": True, \"error\": str(e)} # Asumir validez en caso de error para no bloquear\n",
        "\n",
        "# --- 6. Sistema de reordenamiento de documentos ---\n",
        "def rerank_documents(query, docs, llm, top_n=5):\n",
        "    \"\"\"Reordena documentos por relevancia usando LLM, devuelve los top_n\"\"\"\n",
        "    if not docs:\n",
        "        return []\n",
        "\n",
        "    logger.info(f\"Iniciando reranking para {len(docs)} documentos recuperados...\")\n",
        "    try:\n",
        "        results = []\n",
        "        # Limitar el número de documentos a reordenar para eficiencia\n",
        "        docs_to_rerank = docs[:min(len(docs), 8)] # Reordenar hasta 8 documentos\n",
        "\n",
        "        for i, doc in enumerate(docs_to_rerank):\n",
        "            # Truncar contenido del documento para el prompt\n",
        "            content_preview = doc.page_content[:500] # Usar un fragmento más largo para evaluación\n",
        "\n",
        "            prompt = f\"\"\"\n",
        "            Evalúa la relevancia de este fragmento de texto legal para responder a la siguiente consulta específica:\n",
        "\n",
        "            Consulta del usuario: \"{query}\"\n",
        "\n",
        "            Fragmento del documento ({doc.metadata.get('source', 'N/A')} - Pág. {doc.metadata.get('page', 'N/A')}):\n",
        "            \"{content_preview}...\"\n",
        "\n",
        "            Asigna una puntuación de relevancia del 0 al 10, donde 10 es extremadamente relevante y 0 es irrelevante.\n",
        "            Responde SOLAMENTE con el número de la puntuación:\"\"\"\n",
        "\n",
        "            score_text = llm.invoke(prompt).strip()\n",
        "\n",
        "            # Extraer puntuación de forma más robusta\n",
        "            score_match = re.search(r'\\b(10|[0-9])\\b', score_text) # Busca un número del 0-10 como palabra completa\n",
        "            if score_match:\n",
        "                score = float(score_match.group(0))\n",
        "            else:\n",
        "                logger.warning(f\"No se pudo extraer puntuación de reranking de: '{score_text}'. Usando 5.0 por defecto.\")\n",
        "                score = 5.0 # Valor neutral por defecto\n",
        "\n",
        "            results.append((doc, score))\n",
        "            logger.debug(f\"Doc {i} puntuado con {score}\")\n",
        "\n",
        "        # Ordenar por puntuación descendente\n",
        "        sorted_results = sorted(results, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Devolver los top_n documentos reordenados\n",
        "        reranked_docs = [doc for doc, score in sorted_results[:top_n]]\n",
        "        logger.info(f\"Reranking completado. {len(reranked_docs)} documentos seleccionados.\")\n",
        "\n",
        "        # Opcional: Añadir documentos no reordenados si top_n es mayor que los reordenados\n",
        "        # if len(reranked_docs) < top_n:\n",
        "        #     remaining_docs = docs[len(docs_to_rerank):]\n",
        "        #     reranked_docs.extend(remaining_docs[:top_n - len(reranked_docs)])\n",
        "\n",
        "        return reranked_docs\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error durante el reranking: {e}\")\n",
        "        return docs[:top_n] # Devolver los primeros N documentos originales en caso de error\n",
        "\n",
        "\n",
        "# --- 9. Selección de prompt según clasificación ---\n",
        "def get_prompt_by_category(category, memory_text=\"\", query=\"\"):\n",
        "    \"\"\"Devuelve el ChatPromptTemplate adecuado según la categoría de consulta\"\"\"\n",
        "    system_template = system_prompt_rag # Default\n",
        "    human_template = \"{question}\" # Input directo del usuario para RAG\n",
        "    prompt_used =\"\"\n",
        "    patterns_multi = [\"todos los artículos\", \"artículos relacionados\", \"varios artículos\", \"artículos del rgpd\", \"citame los artículos\", \"cítame varios\", \"cítame todos\"]\n",
        "\n",
        "    if is_follow_up_query(query) and is_action_request(query):\n",
        "        logger.info(\"Consulta de seguimiento orientada a acciones detectada. Usando action_oriented_prompt.\")\n",
        "        system_template = action_oriented_prompt\n",
        "    else:\n",
        "        if category == \"LEGAL_CITATION\" and any(pat in query.lower() for pat in patterns_multi):\n",
        "            system_template = legal_citation_prompt\n",
        "            prompt_used = \"legal_citation_prompt\"\n",
        "        elif category == \"LEGAL_CITATION\":\n",
        "            system_template = legal_multi_citation_prompt\n",
        "            prompt_used = \"legal_multi_citation_prompt\"\n",
        "        elif category == \"LEGAL_ANALYSIS\" and is_action_request(query):\n",
        "            system_template = action_oriented_prompt\n",
        "            prompt_used = \"action_oriented_prompt\"\n",
        "        elif category == \"LEGAL_ANALYSIS\":\n",
        "            system_template = legal_analysis_prompt\n",
        "            prompt_used = \"legal_analysis_prompt\"\n",
        "        elif category == \"PROCEDURAL\":\n",
        "            system_template = procedural_prompt\n",
        "            prompt_used = \"procedural_prompt\"\n",
        "        elif category == \"GENERAL_INFO\":\n",
        "            system_template = general_info_prompt # Usar prompt específico para info general\n",
        "            prompt_used = \"general_info_prompt\"\n",
        "        elif category == \"CONVERSATION\":\n",
        "            # Para conversación, no usamos contexto RAG, usamos memoria\n",
        "            system_template = conversation_prompt.format(memory=memory_text) # Inyectar memoria aquí\n",
        "            human_template = \"{question}\" # Sigue siendo la pregunta del usuario\n",
        "            # Devolver directamente el prompt formateado para conversación, ya que no pasará por load_qa_chain\n",
        "            # OJO: Esto requiere que el flujo principal maneje esto diferente.\n",
        "            # Por simplicidad ahora, devolvemos estructura similar, pero el flujo debe saber NO usar RAG.\n",
        "            # Alternativa: devolver None o un identificador especial.\n",
        "            # Vamos a devolver la estructura estándar por ahora, asumiendo que el flujo principal lo maneja.\n",
        "            # PERO, el prompt de conversación NO tiene variable {context}.\n",
        "            # => Mejor devolver None para indicar que no se use RAG/load_qa_chain.\n",
        "            logger.info(\"Categoría CONVERSATION: No se usará RAG. Se generará respuesta directa.\")\n",
        "            prompt_used = \"conversation_prompt\"\n",
        "            # Construir un prompt simple para LLM directo\n",
        "            return ChatPromptTemplate.from_messages([\n",
        "                 (\"system\", system_template), # Ya formateado con memoria\n",
        "                 (\"human\", human_template)\n",
        "            ])\n",
        "\n",
        "    logger.info(f\"Usando el prompt: {prompt_used}\")\n",
        "    # Para las categorías que usan RAG (con contexto)\n",
        "    # El human_template debe incluir la pregunta del usuario\n",
        "    # El system_template incluye {context} y {question}\n",
        "    # load_qa_chain se encargará de llenar {context} y {question}\n",
        "    return ChatPromptTemplate.from_messages([\n",
        "        SystemMessagePromptTemplate.from_template(system_template),\n",
        "        HumanMessagePromptTemplate.from_template(human_template) # Solo {question} aquí, load_qa_chain lo maneja\n",
        "    ])\n",
        "\n",
        "\n",
        "# --- ¡¡¡DEFINICIÓN NECESARIA DE LA CADENA RAG!!! ---\n",
        "# Necesitamos definir cómo se combinarán el LLM y el Prompt con los documentos.\n",
        "# Usamos load_qa_chain. El tipo de cadena (\"stuff\", \"map_reduce\", etc.) puede variar.\n",
        "# \"stuff\" es simple pero puede exceder el límite de tokens si hay muchos documentos.\n",
        "# \"map_reduce\" o \"refine\" son más robustos para contextos largos.\n",
        "# Probemos con \"stuff\" inicialmente dado el chunk_size de 800.\n",
        "\n",
        "# Nota: La cadena se crea aquí, pero el *prompt específico* se pasará en cada invocación.\n",
        "# Esto es más flexible que crear una cadena diferente cada vez.\n",
        "# OJO: load_qa_chain espera un prompt específico en su creación.\n",
        "# Vamos a crear una función que genere la cadena CON el prompt adecuado CADA VEZ.\n",
        "\n",
        "def create_rag_chain(llm, prompt):\n",
        "    \"\"\"Crea la cadena load_qa_chain con el LLM y el prompt específicos.\"\"\"\n",
        "    # El prompt debe ser un BasePromptTemplate (como ChatPromptTemplate)\n",
        "    if not isinstance(prompt, (PromptTemplate, ChatPromptTemplate)):\n",
        "         logger.error(\"El prompt proporcionado a create_rag_chain no es válido.\")\n",
        "         # Se puede lanzar un error o devolver None/cadena por defecto\n",
        "         return None\n",
        "\n",
        "    # Selecciona el tipo de cadena. 'stuff' es bueno para empezar.\n",
        "    # Ajusta 'chain_type' si tienes problemas de longitud de contexto.\n",
        "    qa_chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=prompt, verbose=False) # verbose=True para debug\n",
        "    return qa_chain\n",
        "\n",
        "def is_follow_up_query(query):\n",
        "    \"\"\"Detecta si la nueva pregunta es una continuación (seguimiento) del contexto anterior.\"\"\"\n",
        "    query_lower = query.strip().lower()\n",
        "    follow_up_starts = [\n",
        "        \"entonces\", \"y\", \"pero\", \"por qué\", \"qué pueden hacer\",\n",
        "        \"qué derechos tienen\", \"cómo reclamar\", \"porque\",\"qué consecuencias\",\n",
        "        \"qué opciones tienen\", \"cómo actuar\", \"qué recursos tienen\"\n",
        "    ]\n",
        "    return any(query_lower.startswith(start) for start in follow_up_starts) or len(query.split()) <= 8\n",
        "\n",
        "\n",
        "def build_contextual_query(last_query, current_query):\n",
        "    \"\"\"Construye una consulta combinando la anterior y la nueva\"\"\"\n",
        "    return f\"Respecto a la situación planteada previamente: {last_query}\\nNueva pregunta: {current_query}\"\n",
        "\n",
        "def classify_intent_with_cache(query, llm_evaluador):\n",
        "    \"\"\"\n",
        "    Clasifica la intención de una consulta usando caché para evitar llamadas repetidas al LLM.\n",
        "    \"\"\"\n",
        "    cache_key = ' '.join(query.lower().split()[:10])  # Usamos las primeras 10 palabras para normalizar claves\n",
        "\n",
        "    if cache_key in intent_classification_cache:\n",
        "        logger.info(f\"Resultado de clasificación obtenido de caché para: {cache_key}\")\n",
        "        return intent_classification_cache[cache_key]\n",
        "\n",
        "    # No está en caché: clasificamos\n",
        "    classification_result = classify_intent_advanced(query, llm_evaluador)\n",
        "\n",
        "    # Guardamos el resultado en caché\n",
        "    intent_classification_cache[cache_key] = classification_result\n",
        "\n",
        "    return classification_result\n",
        "\n",
        "def adapt_retriever(base_retriever, attempt):\n",
        "    \"\"\"Adapta el recuperador según el intento para mejorar recuperación de documentos.\"\"\"\n",
        "    if attempt == 1:\n",
        "        logger.info(\"Reintento 1: aumentando k y activando reranking.\")\n",
        "        return persisted_vectorstore.as_retriever(\n",
        "            search_type=\"mmr\",\n",
        "            search_kwargs={'k': 8, 'fetch_k': 16, 'lambda_mult': 0.75}\n",
        "        )\n",
        "    elif attempt == 2:\n",
        "        logger.info(\"Reintento 2: creando un retriever más flexible (MMR + reformulación posible).\")\n",
        "        return persisted_vectorstore.as_retriever(\n",
        "            search_type=\"mmr\",\n",
        "            search_kwargs={'k': 10, 'fetch_k': 20, 'lambda_mult': 0.6}\n",
        "        )\n",
        "    else:\n",
        "        return base_retriever\n",
        "\n",
        "\n",
        "# --- 5. Sistema de reintento con estrategias alternativas ---\n",
        "def get_response_with_retry(query, llm, base_retriever, memory: ImprovedMemory, last_query=None, max_attempts=3):\n",
        "    \"\"\"Obtiene respuesta con sistema de reintentos, clasificación con caché, y detección de continuidad conversacional.\"\"\"\n",
        "    from sentence_transformers import SentenceTransformer, util\n",
        "    reformulation_embedder = SentenceTransformer(\"paraphrase-multilingual-mpnet-base-v2\")\n",
        "\n",
        "    memory_text = memory.get_relevant_memory(query)\n",
        "\n",
        "    # --- Detectar si la pregunta depende de la anterior ---\n",
        "    if is_follow_up_query(query) and last_query:\n",
        "        logger.info(\"Detectada pregunta de seguimiento. Incorporando última consulta como contexto.\")\n",
        "        query = f\"Contexto anterior: {last_query}\\n\\nNueva consulta: {query}\"\n",
        "\n",
        "    # --- 3. Clasificar la intención usando caché ---\n",
        "    classifier_llm = llm_evaluador if 'llm_evaluador' in globals() and llm_evaluador else llm\n",
        "    classification_result = classify_intent_with_cache(query, classifier_llm)\n",
        "    category = classification_result[\"category\"]\n",
        "    confidence = classification_result[\"confidence\"]\n",
        "\n",
        "    use_rag = category in [\"LEGAL_CITATION\", \"LEGAL_ANALYSIS\", \"PROCEDURAL\"] or (category == \"GENERAL_INFO\" and confidence < 85) or confidence < 75\n",
        "    if category == \"CONVERSATION\":\n",
        "        use_rag = False\n",
        "\n",
        "    logger.info(f\"Intención clasificada como: {category} (Confianza: {confidence}%) - Usar RAG: {use_rag}\")\n",
        "\n",
        "    response_data = None\n",
        "\n",
        "    # --- 4. Generar respuesta directa o RAG ---\n",
        "    if not use_rag:\n",
        "        # --- RESPUESTA DIRECTA ---\n",
        "        try:\n",
        "            system_prompt = conversation_prompt.format(memory=memory_text) if category == \"CONVERSATION\" else f\"\"\"\n",
        "            Sistema: Eres un asistente experto que responde preguntas con precisión y amabilidad.\n",
        "            Contexto conversacional previo:\n",
        "            {memory_text}\n",
        "\n",
        "            Usuario: {{question}}\n",
        "            \"\"\"\n",
        "            prompt = ChatPromptTemplate.from_messages([\n",
        "                (\"system\", system_prompt),\n",
        "                (\"human\", \"{question}\")\n",
        "            ])\n",
        "            llm_chain = LLMChain(llm=llm_conversacion, prompt=prompt, output_key='text')\n",
        "            result = llm_chain.invoke({\"question\": query})\n",
        "            response_text = result.get('text', str(result)).strip()\n",
        "\n",
        "            response_data = {\"response\": response_text, \"docs\": [], \"attempt\": 1, \"category\": category, \"validation\": {\"valid\": True}}\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generando respuesta directa: {e}\", exc_info=True)\n",
        "            response_data = {\"response\": f\"Lo siento, no pude generar una respuesta directa.\", \"docs\": [], \"attempt\": 1, \"category\": category, \"error\": str(e)}\n",
        "\n",
        "    else:\n",
        "        # --- RESPUESTA RAG ---\n",
        "        selected_prompt_template = get_prompt_by_category(category, memory_text, query)\n",
        "\n",
        "        for attempt in range(max_attempts):\n",
        "            try:\n",
        "                logger.info(f\"Intento RAG {attempt+1} para consulta: '{query}' (Categoría: {category})\")\n",
        "\n",
        "                retriever = adapt_retriever(base_retriever, attempt)\n",
        "                retrieved_docs = retriever.get_relevant_documents(query)\n",
        "                logger.info(f\"Recuperados {len(retrieved_docs)} documentos.\")\n",
        "\n",
        "                if attempt > 0 and retrieved_docs:\n",
        "                    retrieved_docs = rerank_documents(query, retrieved_docs, llm, top_n=4)\n",
        "                    logger.info(f\"Documentos después de reranking: {len(retrieved_docs)}\")\n",
        "\n",
        "                if not retrieved_docs:\n",
        "                    logger.warning(\"No se encontraron documentos relevantes.\")\n",
        "\n",
        "                    if attempt == max_attempts - 1:\n",
        "                        response_data = {\n",
        "                            \"response\": \"No he encontrado información específica para responder con seguridad, pero puedo intentar darte una orientación general.\",\n",
        "                            \"docs\": [],\n",
        "                            \"attempt\": attempt + 1,\n",
        "                            \"category\": category,\n",
        "                            \"validation\": {\"valid\": False, \"error\": \"Sin documentos relevantes\"}\n",
        "                        }\n",
        "                        break  # Salir del bucle\n",
        "                    else:\n",
        "                        continue\n",
        "\n",
        "                chain = create_rag_chain(llm, selected_prompt_template)\n",
        "                result = chain.invoke({\"input_documents\": retrieved_docs, \"question\": query})\n",
        "                response_text = result.get('output_text', result if isinstance(result, str) else str(result)).strip()\n",
        "\n",
        "                validation = validate_legal_response(response_text, query, retrieved_docs, llm)\n",
        "                if validation.get(\"valid\", False):\n",
        "                    logger.info(f\"Respuesta validada en intento {attempt+1}.\")\n",
        "                    response_data = {\n",
        "                        \"response\": response_text,\n",
        "                        \"docs\": retrieved_docs,\n",
        "                        \"attempt\": attempt+1,\n",
        "                        \"category\": category,\n",
        "                        \"validation\": validation\n",
        "                    }\n",
        "                    break\n",
        "                else:\n",
        "                    logger.warning(f\"Respuesta no válida en intento {attempt+1}: {validation}\")\n",
        "\n",
        "                    # Intentar mejorar el prompt si no es el último intento\n",
        "                    if attempt < max_attempts - 1:\n",
        "                        # Extraer sugerencias de mejora\n",
        "                        hints = extract_improvement_hints(validation.get(\"raw_validation_output\", \"\"))\n",
        "                        reformulated_query = reformulate_user_query(query, hints)\n",
        "                        # Volver a obtener el prompt base original\n",
        "                        selected_prompt_template = get_prompt_by_category(category, memory_text, reformulated_query)\n",
        "\n",
        "                        # Si es un prompt estándar con {context}, entonces podemos modificarlo\n",
        "                        if isinstance(selected_prompt_template, ChatPromptTemplate):\n",
        "                            try:\n",
        "                                # Tomar solo el system prompt como texto\n",
        "                                original_system_msg = selected_prompt_template.messages[0].prompt.template\n",
        "                                new_system_msg = augment_prompt_with_validation(original_system_msg, hints)\n",
        "\n",
        "                                selected_prompt_template = ChatPromptTemplate.from_messages([\n",
        "                                    SystemMessagePromptTemplate.from_template(new_system_msg),\n",
        "                                    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
        "                                ])\n",
        "\n",
        "                                logger.info(\"Prompt adaptado con sugerencias del validador.\")\n",
        "                            except Exception as e:\n",
        "                                logger.warning(f\"No se pudo adaptar el prompt con sugerencias: {e}\")\n",
        "\n",
        "                    if attempt == max_attempts - 1:\n",
        "                        response_data = {\n",
        "                            \"response\": response_text,\n",
        "                            \"docs\": retrieved_docs,\n",
        "                            \"attempt\": attempt+1,\n",
        "                            \"category\": category,\n",
        "                            \"validation\": validation\n",
        "                        }\n",
        "                    debug=1\n",
        "                    if attempt == max_attempts - 1 and not validation.get(\"valid\", False) and debug ==1:\n",
        "                        logger.info(\"Iniciando auto-refinamiento de respuesta con feedback del validador.\")\n",
        "\n",
        "                        refinement_prompt = ChatPromptTemplate.from_messages([\n",
        "                            SystemMessagePromptTemplate.from_template(\n",
        "                                refinement_module_prompt\n",
        "                            ),\n",
        "                            HumanMessagePromptTemplate.from_template(\"Corrige la respuesta anterior respetando la estructura exacta y ajustándola según los fallos detectados.\")\n",
        "                        ])\n",
        "\n",
        "                        refinement_chain = LLMChain(llm=llm, prompt=refinement_prompt)\n",
        "                        refined_result = refinement_chain.invoke({\n",
        "                            \"fallos\": validation.get(\"raw_validation_output\", \"\"),\n",
        "                            \"respuesta\": response_text\n",
        "                        })\n",
        "\n",
        "                        response_data[\"response\"] = refined_result.get(\"text\", \"\").strip()\n",
        "                        response_data[\"validation\"][\"refinado\"] = True\n",
        "\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error en intento RAG {attempt+1}: {e}\", exc_info=True)\n",
        "                if attempt == max_attempts-1:\n",
        "                    response_data = {\"response\": f\"Ocurrió un error grave al procesar tu consulta.\", \"docs\": [], \"attempt\": attempt+1, \"category\": category, \"error\": str(e)}\n",
        "\n",
        "            query = reformulated_query\n",
        "\n",
        "    # --- 5. Guardar turno en la memoria ---\n",
        "    if response_data and \"response\" in response_data:\n",
        "        memory.add_turn(query, response_data[\"response\"], llm)\n",
        "    else:\n",
        "        memory.add_turn(query, \"[Error al generar respuesta]\", llm=None)\n",
        "\n",
        "    return response_data\n",
        "\n",
        "# --- 8. Sistema de feedback y evaluación ---\n",
        "def collect_feedback(query, response):\n",
        "    \"\"\"Solicita feedback al usuario sobre la respuesta\"\"\"\n",
        "    print(\"\\n\" + \"=\"*20 + \" FEEDBACK \" + \"=\"*20)\n",
        "    print(f\"Consulta: '{query}'\")\n",
        "    print(f\"\\nRespuesta proporcionada:\\n{response}\")\n",
        "    print(\"=\"*50)\n",
        "    while True:\n",
        "        try:\n",
        "            rating = input(\"¿Qué tan útil fue esta respuesta? (1=Nada útil, 5=Muy útil, 0=Saltar): \")\n",
        "            rating = int(rating)\n",
        "            if 0 <= rating <= 5:\n",
        "                 return rating if rating > 0 else None # Devolver None si es 0\n",
        "            else:\n",
        "                 print(\"Por favor, introduce un número entre 0 y 5.\")\n",
        "        except ValueError:\n",
        "            print(\"Entrada inválida. Por favor, introduce un número.\")\n",
        "\n",
        "def extract_improvement_hints(validation_output):\n",
        "    # Extrae frases después de los puntos de fallo\n",
        "    hints = []\n",
        "    for match in re.finditer(r'\\[\\w+:\\s*No\\](.*?)\\n', validation_output, re.IGNORECASE):\n",
        "        hint = match.group(1).strip()\n",
        "        if hint:\n",
        "            hints.append(hint)\n",
        "    return hints\n",
        "\n",
        "def augment_prompt_with_validation(prompt_text, improvement_hints):\n",
        "    if not improvement_hints:\n",
        "        return prompt_text\n",
        "    hint_block = \"\\nIMPORTANTE: Al responder, asegúrate de abordar también los siguientes aspectos:\\n\"\n",
        "    for h in improvement_hints:\n",
        "        hint_block += f\"- {h}\\n\"\n",
        "    return prompt_text + hint_block\n",
        "\n",
        "def reformulate_user_query(original_query: str, improvement_hints: list[str]) -> str:\n",
        "    \"\"\"Añade instrucciones explícitas a la consulta original usando sugerencias del validador.\"\"\"\n",
        "    if not improvement_hints:\n",
        "        return original_query\n",
        "    reformulation = \"\\n\\n También responde específicamente a:\\n\"\n",
        "    for hint in improvement_hints:\n",
        "        reformulation += f\"- {hint.strip()}\\n\"\n",
        "    return original_query + reformulation\n",
        "\n",
        "def save_interaction(query, result_data, feedback=None):\n",
        "    \"\"\"Guarda la interacción completa para análisis y mejora\"\"\"\n",
        "    try:\n",
        "        interaction = {\n",
        "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            \"query\": query,\n",
        "            \"response\": result_data.get(\"response\"),\n",
        "            \"category\": result_data.get(\"category\"),\n",
        "            \"attempt\": result_data.get(\"attempt\"),\n",
        "            \"validation\": result_data.get(\"validation\"),\n",
        "            \"error\": result_data.get(\"error\"),\n",
        "            \"feedback\": feedback, # Añadir feedback del usuario\n",
        "            \"retrieved_docs\": [\n",
        "                {\n",
        "                    \"content_preview\": doc.page_content[:200] + \"...\", # Preview\n",
        "                    \"metadata\": doc.metadata,\n",
        "                    #\"score\": doc.score # Añadir si el retriever devuelve score\n",
        "                }\n",
        "                # Limitar el número de documentos guardados para no hacer el log enorme\n",
        "                for doc in result_data.get(\"docs\", [])[:5] # Guardar metadata de los primeros 5 docs usados\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        with open(\"interacciones_legales.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(json.dumps(interaction, ensure_ascii=False, default=str) + \"\\n\") # default=str para manejar tipos no serializables\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error guardando interacción: {e}\")\n",
        "\n",
        "def evaluate_response_quality(query, response, llm):\n",
        "    \"\"\"Evalúa la calidad de una respuesta utilizando el LLM\"\"\"\n",
        "    # (Se mantiene la estructura de evaluación con prompts, pero se simplifica la llamada)\n",
        "    metrics = {}\n",
        "    prompts = {\n",
        "        \"legal_accuracy\": \"\"\"\n",
        "        Evalúa la PRECISIÓN JURÍDICA de esta respuesta sobre protección de datos (España/UE):\n",
        "        Consulta: {query}\n",
        "        Respuesta: {response}\n",
        "        ¿La respuesta es correcta según RGPD/LOPDGDD? ¿Interpreta bien las normas?\n",
        "        Puntuación (0-10, solo número):\"\"\",\n",
        "        \"relevance\": \"\"\"\n",
        "        Evalúa la RELEVANCIA de la respuesta respecto a la consulta:\n",
        "        Consulta: {query}\n",
        "        Respuesta: {response}\n",
        "        ¿Responde directamente a lo preguntado? ¿Evita información superflua?\n",
        "        Puntuación (0-10, solo número):\"\"\",\n",
        "        \"completeness\": \"\"\"\n",
        "        Evalúa la COMPLETITUD de esta respuesta legal:\n",
        "        Consulta: {query}\n",
        "        Respuesta: {response}\n",
        "        ¿Cubre todos los aspectos clave? ¿Ofrece suficiente detalle/fundamento?\n",
        "        Puntuación (0-10, solo número):\"\"\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        for metric, prompt_template in prompts.items():\n",
        "            eval_prompt = prompt_template.format(query=query, response=response)\n",
        "            eval_resp = llm.invoke(eval_prompt)\n",
        "            score_match = re.search(r'\\b(10|[0-9])\\b', eval_resp)\n",
        "            if score_match:\n",
        "                metrics[metric] = float(score_match.group(0)) / 10.0\n",
        "            else:\n",
        "                logger.warning(f\"No se pudo extraer puntuación para métrica '{metric}' de: '{eval_resp}'\")\n",
        "                metrics[metric] = 0.5 # Default neutral\n",
        "\n",
        "        # Calcular métrica general (si todas las métricas están presentes)\n",
        "        if len(metrics) == 3:\n",
        "             metrics[\"overall\"] = sum(metrics.values()) / 3.0\n",
        "        else:\n",
        "             metrics[\"overall\"] = 0.5 # Default si faltan métricas\n",
        "\n",
        "        logger.info(f\"Evaluación de calidad automática: {metrics}\")\n",
        "        return metrics\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error evaluando calidad de respuesta: {e}\")\n",
        "        return {\"overall\": 0.5, \"error\": str(e)} # Default en caso de error\n",
        "\n",
        "def save_interaction_for_training(query, response, feedback_score, docs_retrieved):\n",
        "     \"\"\"Guarda interacción con feedback positivo para posible entrenamiento futuro.\"\"\"\n",
        "     # Solo guardar si el feedback es bueno (ej. 4 o 5)\n",
        "     if feedback_score is None or feedback_score < 4:\n",
        "         return\n",
        "\n",
        "     try:\n",
        "         training_example = {\n",
        "             \"query\": query,\n",
        "             \"positive_response\": response, # Marcado como positivo por el feedback\n",
        "             \"feedback_score\": feedback_score,\n",
        "             # Opcional: incluir contexto relevante si se quiere entrenar RAG-finetuning\n",
        "             \"relevant_context\": [doc.page_content for doc in docs_retrieved[:2]], # Ej: 2 docs más relevantes\n",
        "             \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "         }\n",
        "\n",
        "         with open(\"training_data_positive.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
        "             f.write(json.dumps(training_example, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "         logger.info(f\"Guardada interacción con feedback {feedback_score} para posible entrenamiento.\")\n",
        "\n",
        "     except Exception as e:\n",
        "         logger.error(f\"Error guardando interacción para entrenamiento: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "def normalize_text(text):\n",
        "    \"\"\"Normaliza el texto: elimina tildes, minúsculas, quita signos de puntuación.\"\"\"\n",
        "    text = unicodedata.normalize('NFD', text)\n",
        "    text = ''.join(c for c in text if unicodedata.category(c) != 'Mn')  # quitar tildes\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # elimina puntuación\n",
        "    return text\n",
        "\n",
        "def is_action_request(query):\n",
        "    \"\"\"Detecta si la pregunta sugiere solicitud de acciones, derechos o pasos a seguir.\"\"\"\n",
        "    normalized_query = normalize_text(query)\n",
        "\n",
        "    action_phrases = [\n",
        "        \"que pueden hacer\",\n",
        "        \"que medidas pueden tomar\",\n",
        "        \"que derechos tienen\",\n",
        "        \"como pueden reclamar\",\n",
        "        \"como pueden actuar\",\n",
        "        \"como reclamar\",\n",
        "        \"como actuar\",\n",
        "        \"que opciones tienen\",\n",
        "        \"que pasos pueden seguir\",\n",
        "        \"que pueden solicitar\",\n",
        "        \"que acciones pueden emprender\",\n",
        "        \"como defenderse\",\n",
        "        \"como denunciar\",\n",
        "        \"como protegerse\",\n",
        "        \"como impugnar\",\n",
        "        \"como negarse\",\n",
        "        \"que recurso tienen\",\n",
        "        \"que alternativas tienen\",\n",
        "        \"que pueden exigir\",\n",
        "        \"que sanciones puede haber\",\n",
        "        \"que consecuencias hay\",\n",
        "    ]\n",
        "\n",
        "    return any(phrase in normalized_query for phrase in action_phrases)\n",
        "\n",
        "\n",
        "# --- Bloque Principal de Interacción ---\n",
        "intent_classification_cache = {}\n",
        "last_user_query = None #Para preguntas continuistas\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\": # Poner el código de ejecución aquí para evitar que se ejecute al importar\n",
        "\n",
        "    # Verificar que las variables necesarias existen\n",
        "    if 'llm' not in locals() or 'llm_evaluador' not in locals() or 'base_retriever' not in locals() or 'persisted_vectorstore' not in locals():\n",
        "         print(\"ERROR: Asegúrate de haber ejecutado las celdas anteriores para inicializar llm, llm_evaluador, base_retriever y persisted_vectorstore.\")\n",
        "    else:\n",
        "         print(\"Modelo LLM y retriever listos.\")\n",
        "\n",
        "         # Inicializar memoria conversacional\n",
        "         conversation_memory = ImprovedMemory(max_turns=4) # Guardar 4 turnos\n",
        "         last_query=\"\"\n",
        "         print(\"\\nBienvenido al Asistente Legal de Protección de Datos.\")\n",
        "         print(\"Escribe 'salir' para terminar.\")\n",
        "\n",
        "         while True:\n",
        "             user_query = input(\"\\nTu consulta: \")\n",
        "             if user_query.lower() == 'salir':\n",
        "                 break\n",
        "             if not user_query:\n",
        "                 continue\n",
        "\n",
        "             start_time = time.time()\n",
        "\n",
        "             # --- Llamada principal al sistema RAG con reintentos ---\n",
        "             result = get_response_with_retry(\n",
        "                 query=user_query,\n",
        "                 llm=llm,\n",
        "                 base_retriever=base_retriever,\n",
        "                 memory=conversation_memory, # Pasar la instancia de memoria\n",
        "                 last_query=last_query\n",
        "             )\n",
        "             last_query = user_query\n",
        "             end_time = time.time()\n",
        "\n",
        "             # --- Mostrar Resultados ---\n",
        "             print(\"\\n--- Respuesta del Asistente ---\")\n",
        "             print(result.get(\"response\", \"No se pudo generar respuesta.\"))\n",
        "             print(\"-\" * 30)\n",
        "             logger.info(f\"Respuesta generada en {end_time - start_time:.2f} segundos.\")\n",
        "             logger.info(f\"Categoría: {result.get('category', 'N/A')}, Intentos: {result.get('attempt', 'N/A')}\")\n",
        "             if result.get(\"validation\"):\n",
        "                 logger.info(f\"Validación: {result['validation']}\")\n",
        "             if result.get(\"error\"):\n",
        "                 logger.error(f\"Error reportado: {result['error']}\")\n",
        "\n",
        "             # Mostrar documentos fuente (opcional, para depuración)\n",
        "             if result.get(\"docs\"):\n",
        "                 print(f\"\\nFuentes consultadas ({len(result['docs'])} documentos):\")\n",
        "                 for i, doc in enumerate(result[\"docs\"]):\n",
        "                     source = doc.metadata.get('source', 'Desconocido')\n",
        "                     page = doc.metadata.get('page', '?')\n",
        "                     tipo = doc.metadata.get('tipo', '')\n",
        "                     print(f\"  [{i+1}] {source} (Pág: {page}, Tipo: {tipo})\") # Preview más corto\n",
        "\n",
        "             # --- Feedback y Evaluación (Opcional) ---\n",
        "             user_feedback = collect_feedback(user_query, result.get(\"response\"))\n",
        "             save_interaction(user_query, result, user_feedback)\n",
        "\n",
        "             if user_feedback:\n",
        "                 # Si hubo feedback positivo, guardar para posible entrenamiento\n",
        "                 save_interaction_for_training(user_query, result.get(\"response\"), user_feedback, result.get(\"docs\", []))\n",
        "\n",
        "             # Evaluar calidad automáticamente (opcional, consume tokens)\n",
        "             # quality_metrics = evaluate_response_quality(user_query, result.get(\"response\"), llm)\n",
        "             # logger.info(f\"Métricas de calidad automáticas: {quality_metrics}\")\n",
        "\n",
        "         print(\"\\n¡Hasta luego!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoxVqhOEWj5b"
      },
      "source": [
        "# Comparaciones de modelos\n",
        "\n",
        "A continuación esta el codigo para comparar las respuestas entre modelos\n",
        "\n",
        "Hay que resetear ollama entre consultas para evitar que use pueda recordar datos de la conversación entre pruebas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ior64B0AfA6Y"
      },
      "outputs": [],
      "source": [
        "#Aqui pegamos nuestra pregunta\n",
        "pregunta=\"¿Cuáles son los principios fundamentales que deben cumplirse para garantizar el tratamiento adecuado de datos personales según la normativa de protección de datos en España?\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWBx9qAlajxd"
      },
      "outputs": [],
      "source": [
        "!pip install gputil\n",
        "!pip install psutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRlKHsidWshH"
      },
      "source": [
        "## Modelo Base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z-7dmDgZEpe"
      },
      "source": [
        "Descargamos el modelo base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ygs3fkqTY_1Y",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "!ollama run llama3:8b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRA0PR0yWjIu"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import psutil\n",
        "import GPUtil\n",
        "from statistics import mean\n",
        "import threading\n",
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# --- Reiniciar Ollama ---\n",
        "!kill -9 $(ps aux | grep '[o]llama' | awk '{print $2}')\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "time.sleep(3)  # Esperar a que Ollama esté listo\n",
        "\n",
        "# --- Configuración de prueba ---\n",
        "model_name = \"llama3:8b\"\n",
        "\n",
        "# --- Monitorear uso de RAM ---\n",
        "def get_system_metrics():\n",
        "    ram_used = psutil.virtual_memory().used / (1024 ** 3)  # GB\n",
        "    return ram_used\n",
        "\n",
        "# --- Monitorear uso de GPU (VRAM) ---\n",
        "def get_gpu_metrics():\n",
        "    gpus = GPUtil.getGPUs()\n",
        "    if gpus:\n",
        "        gpu = gpus[0]\n",
        "        return gpu.memoryUsed / 1024, gpu.load * 100  # GB, %\n",
        "    return 0, 0\n",
        "\n",
        "# --- Iniciar modelo ---\n",
        "llm = OllamaLLM(model=model_name, max_tokens=250)\n",
        "\n",
        "# --- Medición de rendimiento ---\n",
        "start_total = time.time()\n",
        "\n",
        "ram_before = get_system_metrics()\n",
        "gpu_mem_before, gpu_load_before = get_gpu_metrics()\n",
        "\n",
        "# --- Medición activa de CPU en paralelo ---\n",
        "cpu_usage_during_infer = []\n",
        "stop_cpu_monitor = False\n",
        "\n",
        "def monitor_cpu():\n",
        "    while not stop_cpu_monitor:\n",
        "        cpu = psutil.cpu_percent(interval=0.1)\n",
        "        cpu_usage_during_infer.append(cpu)\n",
        "\n",
        "cpu_thread = threading.Thread(target=monitor_cpu)\n",
        "cpu_thread.start()\n",
        "\n",
        "# --- Inferencia ---\n",
        "start_infer = time.time()\n",
        "response = llm.invoke(pregunta)\n",
        "end_infer = time.time()\n",
        "\n",
        "# --- Detener CPU monitor ---\n",
        "stop_cpu_monitor = True\n",
        "cpu_thread.join()\n",
        "\n",
        "ram_after = get_system_metrics()\n",
        "gpu_mem_after, gpu_load_after = get_gpu_metrics()\n",
        "end_total = time.time()\n",
        "\n",
        "# --- Métricas calculadas ---\n",
        "total_time = end_total - start_total\n",
        "infer_time = end_infer - start_infer\n",
        "token_count = len(response.split())\n",
        "avg_cpu = mean(cpu_usage_during_infer) if cpu_usage_during_infer else 0\n",
        "\n",
        "# --- Salida ---\n",
        "print(\"\\n--- RESPUESTA ---\")\n",
        "print(response)\n",
        "\n",
        "print(\"\\n--- MÉTRICAS ---\")\n",
        "print(f\"Modelo: {model_name}\")\n",
        "print(f\"Tokens generados: {token_count}\")\n",
        "print(f\"Tiempo total (incluye carga): {total_time:.2f} s\")\n",
        "print(f\"Tiempo de inferencia (solo generación): {infer_time:.2f} s\")\n",
        "print(f\"Velocidad de generación: {token_count / infer_time:.2f} tokens/seg\")\n",
        "print(f\"RAM usada: {ram_after - ram_before:.2f} GB\")\n",
        "print(f\"Uso CPU promedio durante inferencia: {avg_cpu:.1f}%\")\n",
        "print(f\"VRAM usada: {gpu_mem_after - gpu_mem_before:.2f} GB\")\n",
        "print(f\"Carga GPU: {gpu_load_after:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoxPozkRXX33"
      },
      "source": [
        "## Modelo Base + RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ix4heZm2XaKT"
      },
      "outputs": [],
      "source": [
        "#Para resetear ollama\n",
        "!kill -9 $(ps aux | grep '[o]llama' | awk '{print $2}')\n",
        "\n",
        "import subprocess\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3) # Esperar a que Ollama se cargue\n",
        "\n",
        "\n",
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# Initialize the LLaMA model\n",
        "llm = OllamaLLM(model=\"llama3:8b\") #aqui poinemos el modelo base\n",
        "\n",
        "\n",
        "\n",
        "# Importación correcta para versiones recientes de Langchain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "import torch\n",
        "\n",
        "\n",
        "# Define la plantilla del prompt DETALLADA\n",
        "prompt_template = \"\"\"\n",
        "Eres un asistente legal experto en protección de datos en España.\n",
        "Tu tarea es analizar la legalidad de la situación descrita en la 'Pregunta' basándote **principalmente** en los siguientes 'Textos Legales Recuperados'.\n",
        "Si los textos recuperados contienen suficiente información, responde únicamente basándote en ellos.\n",
        "Si los textos no contienen una respuesta clara pero la pregunta se refiere a conceptos básicos del RGPD, usa lo aprendido en el entrenamiento para dar una respuesta general, especificando que no proviene de los textos recuperados.\n",
        "Si la pregunta requiere información específica que no está en los textos recuperados ni en tu entrenamiento, indícalo claramente.\n",
        "\n",
        "Textos Legales Recuperados:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "Análisis Legal y Conclusión:\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Crea la cadena usando load_qa_chain con el prompt personalizado\n",
        "# chain_type=\"stuff\" es adecuado si los chunks recuperados + pregunta caben en la ventana de contexto del LLM.\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=PROMPT)\n",
        "\n",
        "print (\"-\"*50)\n",
        "print (\"Asistente legal de protección de datos listo (usando load_qa_chain). Escribe 'Exit' para salir.\")\n",
        "print (\"-\"*50)\n",
        "\n",
        "\n",
        "query=pregunta\n",
        "# 1. Recupera los documentos relevantes\n",
        "# Se puede ajustar 'k' aquí si quieres más/menos contexto: retriever.search_kwargs = {'k': 5}\n",
        "docs_retrieved = retriever.invoke(query)\n",
        "\n",
        "# Prepara el input para la cadena\n",
        "input_data = {\n",
        "    \"input_documents\": docs_retrieved,\n",
        "    \"question\": query\n",
        "}\n",
        "\n",
        "# 2. Ejecuta la cadena de generación con los documentos recuperados y la pregunta\n",
        "# Si usas una versión de Langchain > 0.1.0, invoke devuelve un diccionario.\n",
        "# Si usas una versión anterior, podría devolver directamente el string.\n",
        "# El parámetro return_only_outputs=True ya no es necesario/válido en invoke para versiones > 0.1.0\n",
        "result = chain.invoke(input_data)\n",
        "\n",
        "# Imprime la salida del LLM (la clave suele ser 'output_text' en versiones recientes)\n",
        "if isinstance(result, dict) and 'output_text' in result:\n",
        "    print(\"\\nRespuesta:\")\n",
        "    print(result['output_text'])\n",
        "elif isinstance(result, str): # Compatibilidad con versiones anteriores\n",
        "      print(\"\\nRespuesta:\")\n",
        "      print(result)\n",
        "else:\n",
        "      print(\"\\nRespuesta recibida (formato inesperado):\")\n",
        "      print(result)\n",
        "\n",
        "\n",
        "print(\"\\nSaliendo del asistente.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8VmL4S2W7Zf"
      },
      "source": [
        "## Finetunning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkurbP2nW_9W"
      },
      "source": [
        "### 8bit Q8_0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWCIODDTLDQ3"
      },
      "outputs": [],
      "source": [
        "!ollama run hf.co/serdom02/Leyeneitor_8bitQ8_0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sI2VEYV6XWmD"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import psutil\n",
        "import GPUtil\n",
        "from statistics import mean\n",
        "import threading\n",
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# --- Reiniciar Ollama ---\n",
        "!kill -9 $(ps aux | grep '[o]llama' | awk '{print $2}')\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "time.sleep(3)  # Esperar a que Ollama esté listo\n",
        "\n",
        "# --- Configuración de prueba ---\n",
        "model_name = \"hf.co/serdom02/Leyeneitor_8bitQ8_0\"\n",
        "\n",
        "# --- Monitorear uso de RAM ---\n",
        "def get_system_metrics():\n",
        "    ram_used = psutil.virtual_memory().used / (1024 ** 3)  # GB\n",
        "    return ram_used\n",
        "\n",
        "# --- Monitorear uso de GPU (VRAM) ---\n",
        "def get_gpu_metrics():\n",
        "    gpus = GPUtil.getGPUs()\n",
        "    if gpus:\n",
        "        gpu = gpus[0]\n",
        "        return gpu.memoryUsed / 1024, gpu.load * 100  # GB, %\n",
        "    return 0, 0\n",
        "\n",
        "# --- Iniciar modelo ---\n",
        "llm = OllamaLLM(model=model_name, max_tokens=250)\n",
        "\n",
        "# --- Medición de rendimiento ---\n",
        "start_total = time.time()\n",
        "\n",
        "ram_before = get_system_metrics()\n",
        "gpu_mem_before, gpu_load_before = get_gpu_metrics()\n",
        "\n",
        "# --- Medición activa de CPU en paralelo ---\n",
        "cpu_usage_during_infer = []\n",
        "stop_cpu_monitor = False\n",
        "\n",
        "def monitor_cpu():\n",
        "    while not stop_cpu_monitor:\n",
        "        cpu = psutil.cpu_percent(interval=0.1)\n",
        "        cpu_usage_during_infer.append(cpu)\n",
        "\n",
        "cpu_thread = threading.Thread(target=monitor_cpu)\n",
        "cpu_thread.start()\n",
        "\n",
        "# --- Inferencia ---\n",
        "start_infer = time.time()\n",
        "response = llm.invoke(pregunta)\n",
        "end_infer = time.time()\n",
        "\n",
        "# --- Detener CPU monitor ---\n",
        "stop_cpu_monitor = True\n",
        "cpu_thread.join()\n",
        "\n",
        "ram_after = get_system_metrics()\n",
        "gpu_mem_after, gpu_load_after = get_gpu_metrics()\n",
        "end_total = time.time()\n",
        "\n",
        "# --- Métricas calculadas ---\n",
        "total_time = end_total - start_total\n",
        "infer_time = end_infer - start_infer\n",
        "token_count = len(response.split())\n",
        "avg_cpu = mean(cpu_usage_during_infer) if cpu_usage_during_infer else 0\n",
        "\n",
        "# --- Salida ---\n",
        "print(\"\\n--- RESPUESTA ---\")\n",
        "print(response)\n",
        "\n",
        "print(\"\\n--- MÉTRICAS ---\")\n",
        "print(f\"Modelo: {model_name}\")\n",
        "print(f\"Tokens generados: {token_count}\")\n",
        "print(f\"Tiempo total (incluye carga): {total_time:.2f} s\")\n",
        "print(f\"Tiempo de inferencia (solo generación): {infer_time:.2f} s\")\n",
        "print(f\"Velocidad de generación: {token_count / infer_time:.2f} tokens/seg\")\n",
        "print(f\"RAM usada: {ram_after - ram_before:.2f} GB\")\n",
        "print(f\"Uso CPU promedio durante inferencia: {avg_cpu:.1f}%\")\n",
        "print(f\"VRAM usada: {gpu_mem_after - gpu_mem_before:.2f} GB\")\n",
        "print(f\"Carga GPU: {gpu_load_after:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBPrNkBgXSRg"
      },
      "source": [
        "### 16bit GGUF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMDE2J2MLN42"
      },
      "outputs": [],
      "source": [
        "!ollama run hf.co/serdom02/model_16bitGGUF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTWJXFTKXXHU"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import psutil\n",
        "import GPUtil\n",
        "from statistics import mean\n",
        "import threading\n",
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# --- Reiniciar Ollama ---\n",
        "!kill -9 $(ps aux | grep '[o]llama' | awk '{print $2}')\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "time.sleep(3)  # Esperar a que Ollama esté listo\n",
        "\n",
        "# --- Configuración de prueba ---\n",
        "model_name = \"hf.co/serdom02/model_16bitGGUF\"\n",
        "\n",
        "# --- Monitorear uso de RAM ---\n",
        "def get_system_metrics():\n",
        "    ram_used = psutil.virtual_memory().used / (1024 ** 3)  # GB\n",
        "    return ram_used\n",
        "\n",
        "# --- Monitorear uso de GPU (VRAM) ---\n",
        "def get_gpu_metrics():\n",
        "    gpus = GPUtil.getGPUs()\n",
        "    if gpus:\n",
        "        gpu = gpus[0]\n",
        "        return gpu.memoryUsed / 1024, gpu.load * 100  # GB, %\n",
        "    return 0, 0\n",
        "\n",
        "# --- Iniciar modelo ---\n",
        "llm = OllamaLLM(model=model_name, max_tokens=250)\n",
        "\n",
        "# --- Medición de rendimiento ---\n",
        "start_total = time.time()\n",
        "\n",
        "ram_before = get_system_metrics()\n",
        "gpu_mem_before, gpu_load_before = get_gpu_metrics()\n",
        "\n",
        "# --- Medición activa de CPU en paralelo ---\n",
        "cpu_usage_during_infer = []\n",
        "stop_cpu_monitor = False\n",
        "\n",
        "def monitor_cpu():\n",
        "    while not stop_cpu_monitor:\n",
        "        cpu = psutil.cpu_percent(interval=0.1)\n",
        "        cpu_usage_during_infer.append(cpu)\n",
        "\n",
        "cpu_thread = threading.Thread(target=monitor_cpu)\n",
        "cpu_thread.start()\n",
        "\n",
        "# --- Inferencia ---\n",
        "start_infer = time.time()\n",
        "response = llm.invoke(pregunta)\n",
        "end_infer = time.time()\n",
        "\n",
        "# --- Detener CPU monitor ---\n",
        "stop_cpu_monitor = True\n",
        "cpu_thread.join()\n",
        "\n",
        "ram_after = get_system_metrics()\n",
        "gpu_mem_after, gpu_load_after = get_gpu_metrics()\n",
        "end_total = time.time()\n",
        "\n",
        "# --- Métricas calculadas ---\n",
        "total_time = end_total - start_total\n",
        "infer_time = end_infer - start_infer\n",
        "token_count = len(response.split())\n",
        "avg_cpu = mean(cpu_usage_during_infer) if cpu_usage_during_infer else 0\n",
        "\n",
        "# --- Salida ---\n",
        "print(\"\\n--- RESPUESTA ---\")\n",
        "print(response)\n",
        "\n",
        "print(\"\\n--- MÉTRICAS ---\")\n",
        "print(f\"Modelo: {model_name}\")\n",
        "print(f\"Tokens generados: {token_count}\")\n",
        "print(f\"Tiempo total (incluye carga): {total_time:.2f} s\")\n",
        "print(f\"Tiempo de inferencia (solo generación): {infer_time:.2f} s\")\n",
        "print(f\"Velocidad de generación: {token_count / infer_time:.2f} tokens/seg\")\n",
        "print(f\"RAM usada: {ram_after - ram_before:.2f} GB\")\n",
        "print(f\"Uso CPU promedio durante inferencia: {avg_cpu:.1f}%\")\n",
        "print(f\"VRAM usada: {gpu_mem_after - gpu_mem_before:.2f} GB\")\n",
        "print(f\"Carga GPU: {gpu_load_after:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8SWdO38XT5k"
      },
      "source": [
        "### q4_k_m GGUF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d4j3eDmLRzH"
      },
      "outputs": [],
      "source": [
        "!ollama run hf.co/serdom02/model_q4_k_mGGUF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8VuZ5VWXXi1"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import psutil\n",
        "import GPUtil\n",
        "from statistics import mean\n",
        "import threading\n",
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# --- Reiniciar Ollama ---\n",
        "!kill -9 $(ps aux | grep '[o]llama' | awk '{print $2}')\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "time.sleep(3)  # Esperar a que Ollama esté listo\n",
        "\n",
        "# --- Configuración de prueba ---\n",
        "model_name = \"hf.co/serdom02/model_q4_k_mGGUF\"\n",
        "\n",
        "# --- Monitorear uso de RAM ---\n",
        "def get_system_metrics():\n",
        "    ram_used = psutil.virtual_memory().used / (1024 ** 3)  # GB\n",
        "    return ram_used\n",
        "\n",
        "# --- Monitorear uso de GPU (VRAM) ---\n",
        "def get_gpu_metrics():\n",
        "    gpus = GPUtil.getGPUs()\n",
        "    if gpus:\n",
        "        gpu = gpus[0]\n",
        "        return gpu.memoryUsed / 1024, gpu.load * 100  # GB, %\n",
        "    return 0, 0\n",
        "\n",
        "# --- Iniciar modelo ---\n",
        "llm = OllamaLLM(model=model_name, max_tokens=250)\n",
        "\n",
        "# --- Medición de rendimiento ---\n",
        "start_total = time.time()\n",
        "\n",
        "ram_before = get_system_metrics()\n",
        "gpu_mem_before, gpu_load_before = get_gpu_metrics()\n",
        "\n",
        "# --- Medición activa de CPU en paralelo ---\n",
        "cpu_usage_during_infer = []\n",
        "stop_cpu_monitor = False\n",
        "\n",
        "def monitor_cpu():\n",
        "    while not stop_cpu_monitor:\n",
        "        cpu = psutil.cpu_percent(interval=0.1)\n",
        "        cpu_usage_during_infer.append(cpu)\n",
        "\n",
        "cpu_thread = threading.Thread(target=monitor_cpu)\n",
        "cpu_thread.start()\n",
        "\n",
        "# --- Inferencia ---\n",
        "start_infer = time.time()\n",
        "response = llm.invoke(pregunta)\n",
        "end_infer = time.time()\n",
        "\n",
        "# --- Detener CPU monitor ---\n",
        "stop_cpu_monitor = True\n",
        "cpu_thread.join()\n",
        "\n",
        "ram_after = get_system_metrics()\n",
        "gpu_mem_after, gpu_load_after = get_gpu_metrics()\n",
        "end_total = time.time()\n",
        "\n",
        "# --- Métricas calculadas ---\n",
        "total_time = end_total - start_total\n",
        "infer_time = end_infer - start_infer\n",
        "token_count = len(response.split())\n",
        "avg_cpu = mean(cpu_usage_during_infer) if cpu_usage_during_infer else 0\n",
        "\n",
        "# --- Salida ---\n",
        "print(\"\\n--- RESPUESTA ---\")\n",
        "print(response)\n",
        "\n",
        "print(\"\\n--- MÉTRICAS ---\")\n",
        "print(f\"Modelo: {model_name}\")\n",
        "print(f\"Tokens generados: {token_count}\")\n",
        "print(f\"Tiempo total (incluye carga): {total_time:.2f} s\")\n",
        "print(f\"Tiempo de inferencia (solo generación): {infer_time:.2f} s\")\n",
        "print(f\"Velocidad de generación: {token_count / infer_time:.2f} tokens/seg\")\n",
        "print(f\"RAM usada: {ram_after - ram_before:.2f} GB\")\n",
        "print(f\"Uso CPU promedio durante inferencia: {avg_cpu:.1f}%\")\n",
        "print(f\"VRAM usada: {gpu_mem_after - gpu_mem_before:.2f} GB\")\n",
        "print(f\"Carga GPU: {gpu_load_after:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWMJxQ09X0f2"
      },
      "source": [
        "## Finetunning + RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFcUQV9HX2sX"
      },
      "source": [
        "### 8bit Q8_0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sA2brIQWYDVE"
      },
      "outputs": [],
      "source": [
        "#Para resetear ollama\n",
        "!kill -9 $(ps aux | grep '[o]llama' | awk '{print $2}')\n",
        "\n",
        "import subprocess\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3) # Esperar a que Ollama se cargue\n",
        "\n",
        "\n",
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# Initialize the LLaMA model\n",
        "llm = OllamaLLM(model=\"hf.co/serdom02/model_8bitQ8_0\") #aqui poinemos el modelo base\n",
        "\n",
        "# Importación correcta para versiones recientes de Langchain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "import torch\n",
        "\n",
        "\n",
        "# Define la plantilla del prompt DETALLADA\n",
        "prompt_template = \"\"\"\n",
        "Eres un asistente legal experto en protección de datos en España.\n",
        "Tu tarea es analizar la legalidad de la situación descrita en la 'Pregunta' basándote **principalmente** en los siguientes 'Textos Legales Recuperados'.\n",
        "Si los textos recuperados contienen suficiente información, responde únicamente basándote en ellos.\n",
        "Si los textos no contienen una respuesta clara pero la pregunta se refiere a conceptos básicos del RGPD, usa lo aprendido en el entrenamiento para dar una respuesta general, especificando que no proviene de los textos recuperados.\n",
        "Si la pregunta requiere información específica que no está en los textos recuperados ni en tu entrenamiento, indícalo claramente.\n",
        "\n",
        "Textos Legales Recuperados:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "Análisis Legal y Conclusión:\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Crea la cadena usando load_qa_chain con el prompt personalizado\n",
        "# chain_type=\"stuff\" es adecuado si los chunks recuperados + pregunta caben en la ventana de contexto del LLM.\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=PROMPT)\n",
        "\n",
        "print (\"-\"*50)\n",
        "print (\"Asistente legal de protección de datos listo (usando load_qa_chain). Escribe 'Exit' para salir.\")\n",
        "print (\"-\"*50)\n",
        "\n",
        "\n",
        "query=pregunta\n",
        "# 1. Recupera los documentos relevantes\n",
        "# Se puede ajustar 'k' aquí si quieres más/menos contexto: retriever.search_kwargs = {'k': 5}\n",
        "docs_retrieved = retriever.invoke(query)\n",
        "\n",
        "# Prepara el input para la cadena\n",
        "input_data = {\n",
        "    \"input_documents\": docs_retrieved,\n",
        "    \"question\": query\n",
        "}\n",
        "\n",
        "# 2. Ejecuta la cadena de generación con los documentos recuperados y la pregunta\n",
        "# Si usas una versión de Langchain > 0.1.0, invoke devuelve un diccionario.\n",
        "# Si usas una versión anterior, podría devolver directamente el string.\n",
        "# El parámetro return_only_outputs=True ya no es necesario/válido en invoke para versiones > 0.1.0\n",
        "result = chain.invoke(input_data)\n",
        "\n",
        "# Imprime la salida del LLM (la clave suele ser 'output_text' en versiones recientes)\n",
        "if isinstance(result, dict) and 'output_text' in result:\n",
        "    print(\"\\nRespuesta:\")\n",
        "    print(result['output_text'])\n",
        "elif isinstance(result, str): # Compatibilidad con versiones anteriores\n",
        "      print(\"\\nRespuesta:\")\n",
        "      print(result)\n",
        "else:\n",
        "      print(\"\\nRespuesta recibida (formato inesperado):\")\n",
        "      print(result)\n",
        "\n",
        "\n",
        "print(\"\\nSaliendo del asistente.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz5jjApKYDsc"
      },
      "source": [
        "### 16bit GGUF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLKZ3HsTYG4l"
      },
      "outputs": [],
      "source": [
        "#Para resetear ollama\n",
        "!kill -9 $(ps aux | grep '[o]llama' | awk '{print $2}')\n",
        "\n",
        "import subprocess\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3) # Esperar a que Ollama se cargue\n",
        "\n",
        "\n",
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# Initialize the LLaMA model\n",
        "llm = OllamaLLM(model=\"hf.co/serdom02/model_16bitGGUF\") #aqui poinemos el modelo base\n",
        "\n",
        "# Importación correcta para versiones recientes de Langchain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "import torch\n",
        "\n",
        "\n",
        "# Define la plantilla del prompt DETALLADA\n",
        "prompt_template = \"\"\"\n",
        "Eres un asistente legal experto en protección de datos en España.\n",
        "Tu tarea es analizar la legalidad de la situación descrita en la 'Pregunta' basándote **principalmente** en los siguientes 'Textos Legales Recuperados'.\n",
        "Si los textos recuperados contienen suficiente información, responde únicamente basándote en ellos.\n",
        "Si los textos no contienen una respuesta clara pero la pregunta se refiere a conceptos básicos del RGPD, usa lo aprendido en el entrenamiento para dar una respuesta general, especificando que no proviene de los textos recuperados.\n",
        "Si la pregunta requiere información específica que no está en los textos recuperados ni en tu entrenamiento, indícalo claramente.\n",
        "\n",
        "Textos Legales Recuperados:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "Análisis Legal y Conclusión:\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Crea la cadena usando load_qa_chain con el prompt personalizado\n",
        "# chain_type=\"stuff\" es adecuado si los chunks recuperados + pregunta caben en la ventana de contexto del LLM.\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=PROMPT)\n",
        "\n",
        "print (\"-\"*50)\n",
        "print (\"Asistente legal de protección de datos listo (usando load_qa_chain). Escribe 'Exit' para salir.\")\n",
        "print (\"-\"*50)\n",
        "\n",
        "\n",
        "query=pregunta\n",
        "# 1. Recupera los documentos relevantes\n",
        "# Se puede ajustar 'k' aquí si quieres más/menos contexto: retriever.search_kwargs = {'k': 5}\n",
        "docs_retrieved = retriever.invoke(query)\n",
        "\n",
        "# Prepara el input para la cadena\n",
        "input_data = {\n",
        "    \"input_documents\": docs_retrieved,\n",
        "    \"question\": query\n",
        "}\n",
        "\n",
        "# 2. Ejecuta la cadena de generación con los documentos recuperados y la pregunta\n",
        "# Si usas una versión de Langchain > 0.1.0, invoke devuelve un diccionario.\n",
        "# Si usas una versión anterior, podría devolver directamente el string.\n",
        "# El parámetro return_only_outputs=True ya no es necesario/válido en invoke para versiones > 0.1.0\n",
        "result = chain.invoke(input_data)\n",
        "\n",
        "# Imprime la salida del LLM (la clave suele ser 'output_text' en versiones recientes)\n",
        "if isinstance(result, dict) and 'output_text' in result:\n",
        "    print(\"\\nRespuesta:\")\n",
        "    print(result['output_text'])\n",
        "elif isinstance(result, str): # Compatibilidad con versiones anteriores\n",
        "      print(\"\\nRespuesta:\")\n",
        "      print(result)\n",
        "else:\n",
        "      print(\"\\nRespuesta recibida (formato inesperado):\")\n",
        "      print(result)\n",
        "\n",
        "\n",
        "print(\"\\nSaliendo del asistente.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QYAWjBEYFDn"
      },
      "source": [
        "### q4_k_m GGUF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dL5mYhqYEzo"
      },
      "outputs": [],
      "source": [
        "#Para resetear ollama\n",
        "!kill -9 $(ps aux | grep '[o]llama' | awk '{print $2}')\n",
        "\n",
        "import subprocess\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3) # Esperar a que Ollama se cargue\n",
        "\n",
        "\n",
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# Initialize the LLaMA model\n",
        "llm = OllamaLLM(model=\"hf.co/serdom02/model_q4_k_mGGUF\") #aqui poinemos el modelo base\n",
        "\n",
        "# Importación correcta para versiones recientes de Langchain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "import torch\n",
        "\n",
        "\n",
        "# Define la plantilla del prompt DETALLADA\n",
        "prompt_template = \"\"\"\n",
        "Eres un asistente legal experto en protección de datos en España.\n",
        "Tu tarea es analizar la legalidad de la situación descrita en la 'Pregunta' basándote **principalmente** en los siguientes 'Textos Legales Recuperados'.\n",
        "Si los textos recuperados contienen suficiente información, responde únicamente basándote en ellos.\n",
        "Si los textos no contienen una respuesta clara pero la pregunta se refiere a conceptos básicos del RGPD, usa lo aprendido en el entrenamiento para dar una respuesta general, especificando que no proviene de los textos recuperados.\n",
        "Si la pregunta requiere información específica que no está en los textos recuperados ni en tu entrenamiento, indícalo claramente.\n",
        "\n",
        "Textos Legales Recuperados:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "Análisis Legal y Conclusión:\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Crea la cadena usando load_qa_chain con el prompt personalizado\n",
        "# chain_type=\"stuff\" es adecuado si los chunks recuperados + pregunta caben en la ventana de contexto del LLM.\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=PROMPT)\n",
        "\n",
        "print (\"-\"*50)\n",
        "print (\"Asistente legal de protección de datos listo (usando load_qa_chain). Escribe 'Exit' para salir.\")\n",
        "print (\"-\"*50)\n",
        "\n",
        "\n",
        "query=pregunta\n",
        "# 1. Recupera los documentos relevantes\n",
        "# Se puede ajustar 'k' aquí si quieres más/menos contexto: retriever.search_kwargs = {'k': 5}\n",
        "docs_retrieved = retriever.invoke(query)\n",
        "\n",
        "# Prepara el input para la cadena\n",
        "input_data = {\n",
        "    \"input_documents\": docs_retrieved,\n",
        "    \"question\": query\n",
        "}\n",
        "\n",
        "# 2. Ejecuta la cadena de generación con los documentos recuperados y la pregunta\n",
        "# Si usas una versión de Langchain > 0.1.0, invoke devuelve un diccionario.\n",
        "# Si usas una versión anterior, podría devolver directamente el string.\n",
        "# El parámetro return_only_outputs=True ya no es necesario/válido en invoke para versiones > 0.1.0\n",
        "result = chain.invoke(input_data)\n",
        "\n",
        "# Imprime la salida del LLM (la clave suele ser 'output_text' en versiones recientes)\n",
        "if isinstance(result, dict) and 'output_text' in result:\n",
        "    print(\"\\nRespuesta:\")\n",
        "    print(result['output_text'])\n",
        "elif isinstance(result, str): # Compatibilidad con versiones anteriores\n",
        "      print(\"\\nRespuesta:\")\n",
        "      print(result)\n",
        "else:\n",
        "      print(\"\\nRespuesta recibida (formato inesperado):\")\n",
        "      print(result)\n",
        "\n",
        "\n",
        "print(\"\\nSaliendo del asistente.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVn5G4j93f2x"
      },
      "source": [
        "# Servidor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VzaBVm53lVJ"
      },
      "source": [
        "En este apartado se configura el backend que da servicio a la pagina web que usa el usuario para interactuar con el modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9r5qu7R3lwa"
      },
      "source": [
        "## Paso 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQ3wKgOb3oN_"
      },
      "source": [
        "Creamos la función que vamos a utilizar para interactuar con el modelo (Hay que ejecutar antes las celdas de la parte Aplicación Final Mejorada)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1In3ms-6fe-V"
      },
      "source": [
        "Se Asocia una instancia de ImprovedMemory a cada nombre de usuario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwsZfjXJk6x-"
      },
      "outputs": [],
      "source": [
        "def responder_web(pregunta: str, usuario: str, app) -> str:\n",
        "    try:\n",
        "        sesiones = app.state.user_sessions\n",
        "        usuario = usuario.strip().lower()\n",
        "\n",
        "        # Obtener o crear sesión del usuario\n",
        "        if usuario not in sesiones:\n",
        "            sesiones[usuario] = UsuarioSession()\n",
        "\n",
        "        sesion = sesiones[usuario]\n",
        "\n",
        "        # Inyectar el last_query en get_response_with_retry (requiere adaptación)\n",
        "        result = get_response_with_retry(\n",
        "            query=pregunta,\n",
        "            llm=llm,\n",
        "            base_retriever=base_retriever,\n",
        "            memory=sesion.memory,\n",
        "            last_query=sesion.last_query\n",
        "        )\n",
        "\n",
        "        # Actualizar last_query\n",
        "        sesion.last_query = pregunta\n",
        "\n",
        "        return result.get(\"response\", \"No se pudo generar una respuesta.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error en responder_web: {e}\", exc_info=True)\n",
        "        return f\"Ocurrió un error al procesar tu consulta: {e}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rw4mRKzz36rJ"
      },
      "source": [
        "## Paso 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E448E2T39bL"
      },
      "source": [
        "Instalamos la dependencias y creamos la API usando FastAPI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLqVQo1j37cT"
      },
      "outputs": [],
      "source": [
        "!pip install fastapi uvicorn nest_asyncio pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdBIkRHy4B_y"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "import uvicorn\n",
        "from fastapi import FastAPI, Request\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Necesario para que FastAPI funcione dentro del notebook\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Crea la app\n",
        "app = FastAPI()\n",
        "app.state.user_sessions = {}\n",
        "\n",
        "# CORS para permitir acceso desde GitHub Pages u otro frontend\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # O restringe a tu dominio exacto\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "@app.post(\"/reset\") #peticiones HTTP POST que lleguen a la ruta /reset por ej POST https://ip.ngrok.io/reset\n",
        "async def reset_memoria(request: Request):\n",
        "    datos = await request.json()\n",
        "    usuario = datos.get(\"usuario\", \"\").strip().lower()\n",
        "    sesiones = request.app.state.user_sessions\n",
        "\n",
        "    logger.info(f\"Solicitud de reseteo para: {usuario}\")\n",
        "    if usuario in sesiones:\n",
        "        del sesiones[usuario]\n",
        "        logger.info(f\"Memoria borrada para {usuario}\")\n",
        "        return {\"status\": \"ok\", \"mensaje\": f\"Memoria reiniciada para {usuario}\"}\n",
        "    else:\n",
        "        logger.warning(f\"No se encontró memoria activa para {usuario}\")\n",
        "        return {\"status\": \"no-op\", \"mensaje\": \"Usuario no tenía memoria activa\"}\n",
        "\n",
        "\n",
        "\n",
        "# Ruta del chat\n",
        "@app.post(\"/chat\")\n",
        "async def chat(request: Request):\n",
        "    datos = await request.json()\n",
        "    pregunta = datos.get(\"mensaje\", \"\")\n",
        "    usuario = datos.get(\"usuario\", \"invitado\").strip().lower()\n",
        "\n",
        "    respuesta = responder_web(pregunta, usuario, request.app)\n",
        "    return {\"respuesta\": respuesta}\n",
        "\n",
        "# Crear túnel ngrok (puedes copiar la URL que imprime)\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"Tu servidor está disponible en: {public_url}\")\n",
        "\n",
        "# Lanzar el servidor\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VFohj9Ife-W"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "NgUHfiVbwTfQ"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}