{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgUHfiVbwTfQ"
      },
      "source": [
        "# GENERAR DATASET FORMA AUTOMATICA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUqzE0k2U1lb"
      },
      "source": [
        "> **Para ejecutar el siguiente codigo no hace falta estar conectados a una GPU**\n",
        "\n",
        "\n",
        "El siguiente codigo nos permite generar preguntas para nuestro dataset usando OpenRouter, en esta web podemos usar casi todos los modelos del mercado pero para generar el dataset vamos a limitarnos a las versiones gratuitas de DeepSeek-R1 y DeepSeek-V3\n",
        "\n",
        "La API de OpenRouter tiene limitaciones cuando se usan modelos gratuitos:\n",
        "\n",
        "\n",
        "> Free limit: If you are using a free model variant (with an ID ending in :free), then you will be limited to 20 requests per minute and 200 requests per day.\n",
        "\n",
        "\n",
        "\n",
        "El siguiente codigo tiene dos modos:\n",
        "\n",
        "*   Modo de √∫nico Prompt: Le metemos solo un prompt del excel ([Prompts para generar el dataset usando distintos modelos](https://docs.google.com/spreadsheets/d/1MQF8Z5_HqVSOzDXD8Ya7zEljJ13rD1Kw9FsUd6xhEJo/edit?pli=1&gid=0#gid=0)) y nos va a generar las preguntas y las respuestas siguiendo las instrucciones del prompt\n",
        "\n",
        "*   Modo M√∫ltiples Prompts: Copiamos distintas preguntas creadas por nosotros o por otros modelos LLM a un archivo de texto o csv, este modo adem√°s permite darle un prompt inicial de contexto antes de que empiece a generar las respuestas a nuestras preguntas, hay un ejemplo de prompt de contexto en el ([excel de prompts](https://docs.google.com/spreadsheets/d/1MQF8Z5_HqVSOzDXD8Ya7zEljJ13rD1Kw9FsUd6xhEJo/edit?pli=1&gid=0#gid=0))\n",
        "\n",
        "Una vez genera el contenido, nos descarga el csv con las preguntas, las respuestas, y el modelo que ha utilizado\n",
        "\n",
        "Se pueden generar varios modelos para que ambos generen preguntas y se vayan alternando.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBo74hgrStBc"
      },
      "source": [
        "**Paquita dice que antes de dar a play lo pienses dos veces a la hora de agregar cuantas consultas quieres hacer, ya que si no tienes un conteo de las restantes en la API el codigo mostrar√° que no se pueden generar m√°s consultas y el csv no se generar√° con las consultas antes del error**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw-k3y8dY9c5"
      },
      "source": [
        "## Codigo para generar el contenido del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZh0d5RstAcX"
      },
      "outputs": [],
      "source": [
        "!pip install requests tqdm python-dotenv\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import time\n",
        "import requests\n",
        "import json\n",
        "import re\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "def extract_question_answer(response_text: str) -> Tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Extrae la pregunta y respuesta de un texto generado por el modelo usando varios m√©todos.\n",
        "\n",
        "    Args:\n",
        "        response_text: Texto generado por el modelo.\n",
        "\n",
        "    Returns:\n",
        "        Tupla con (pregunta, respuesta).\n",
        "    \"\"\"\n",
        "    # Normalizar el texto para lidiar con diferentes formatos\n",
        "    text = response_text.strip()\n",
        "\n",
        "    # M√©todo 1: Buscar el delimitador exacto\n",
        "    if \"RESPUESTAMODELO\" in text:\n",
        "        parts = text.split(\"RESPUESTAMODELO\", 1)\n",
        "        return parts[0].strip(), parts[1].strip()\n",
        "\n",
        "    # M√©todo 2: Buscar variaciones del delimitador\n",
        "    for delimiter in [\"RESPUESTA MODELO\", \"Respuesta Modelo\", \"Respuesta:\", \"Respuesta del modelo:\"]:\n",
        "        if delimiter in text:\n",
        "            parts = text.split(delimiter, 1)\n",
        "            return parts[0].strip(), parts[1].strip()\n",
        "\n",
        "    # M√©todo 3: Buscar un patr√≥n de pregunta y respuesta\n",
        "    # Asumiendo que la pregunta termina con un signo de interrogaci√≥n\n",
        "    # y la respuesta comienza en la siguiente l√≠nea\n",
        "    if \"?\" in text:\n",
        "        # Encontrar la √∫ltima pregunta en el texto\n",
        "        question_parts = text.split(\"?\")\n",
        "        # La pregunta es todo hasta el √∫ltimo signo de interrogaci√≥n\n",
        "        all_but_last = \"?\".join(question_parts[:-1]) + \"?\"\n",
        "        last_part = question_parts[-1]\n",
        "\n",
        "        # Si hay m√°s de una l√≠nea despu√©s del signo de interrogaci√≥n,\n",
        "        # la primera l√≠nea podr√≠a ser parte de la pregunta\n",
        "        lines_after = last_part.strip().split(\"\\n\")\n",
        "\n",
        "        if len(lines_after) > 1 and not lines_after[0]:\n",
        "            # La respuesta comienza despu√©s de una l√≠nea en blanco\n",
        "            question = all_but_last.strip()\n",
        "            answer = \"\\n\".join(lines_after[1:]).strip()\n",
        "        else:\n",
        "            # La respuesta comienza inmediatamente despu√©s del signo de interrogaci√≥n\n",
        "            question = all_but_last.strip()\n",
        "            answer = last_part.strip()\n",
        "\n",
        "        return question, answer\n",
        "\n",
        "    # Si nada funciona, intenta una divisi√≥n por la mitad (√∫ltima opci√≥n)\n",
        "    lines = text.strip().split(\"\\n\")\n",
        "    if len(lines) >= 2:\n",
        "        # Asumimos que la mitad es pregunta y la otra mitad respuesta\n",
        "        midpoint = len(lines) // 2\n",
        "        return \"\\n\".join(lines[:midpoint]).strip(), \"\\n\".join(lines[midpoint:]).strip()\n",
        "\n",
        "    # Si todo falla, devuelve un error\n",
        "    print(f\"ERROR: No se pudo extraer la pregunta y respuesta. Texto completo:\\n{text}\")\n",
        "    return \"ERROR: No se pudo extraer la pregunta\", \"ERROR: No se pudo extraer la respuesta\"\n",
        "\n",
        "def validate_qa_pair(question: str, answer: str) -> bool:\n",
        "    \"\"\"\n",
        "    Valida que el par pregunta-respuesta sea coherente.\n",
        "\n",
        "    Args:\n",
        "        question: La pregunta extra√≠da.\n",
        "        answer: La respuesta extra√≠da.\n",
        "\n",
        "    Returns:\n",
        "        True si parece un par v√°lido, False en caso contrario.\n",
        "    \"\"\"\n",
        "    # La pregunta deber√≠a terminar con signo de interrogaci√≥n\n",
        "    has_question_mark = \"?\" in question\n",
        "\n",
        "    # Verificar longitudes m√≠nimas\n",
        "    question_length_ok = len(question.split()) >= 3\n",
        "    answer_length_ok = len(answer.split()) >= 5\n",
        "\n",
        "    # La respuesta no debe contener la palabra \"pregunta\" o \"question\"\n",
        "    no_question_in_answer = \"pregunta\" not in answer.lower() and \"question\" not in answer.lower()\n",
        "\n",
        "    # Verificar que no contenga instrucciones del formato\n",
        "    no_instructions = \"formato\" not in question.lower() and \"instrucciones\" not in question.lower()\n",
        "\n",
        "    # Verificar que no haya mensajes de error\n",
        "    no_errors = \"ERROR:\" not in question and \"ERROR:\" not in answer\n",
        "\n",
        "    return has_question_mark and question_length_ok and answer_length_ok and no_question_in_answer and no_instructions and no_errors\n",
        "\n",
        "def is_similar_to_existing(question: str, existing_questions: List[str], threshold: float = 0.7) -> bool:\n",
        "    \"\"\"\n",
        "    Comprueba si una pregunta es similar a las existentes usando una comparaci√≥n simple.\n",
        "\n",
        "    Args:\n",
        "        question: Pregunta a comprobar\n",
        "        existing_questions: Lista de preguntas existentes\n",
        "        threshold: Umbral de similitud (0-1)\n",
        "\n",
        "    Returns:\n",
        "        True si es similar, False si no\n",
        "    \"\"\"\n",
        "    # Normalizar la pregunta (min√∫sculas, sin puntuaci√≥n)\n",
        "    normalized_question = re.sub(r'[^\\w\\s]', '', question.lower())\n",
        "    words = set(normalized_question.split())\n",
        "\n",
        "    for existing in existing_questions:\n",
        "        normalized_existing = re.sub(r'[^\\w\\s]', '', existing.lower())\n",
        "        existing_words = set(normalized_existing.split())\n",
        "\n",
        "        # Calcular similitud Jaccard (proporci√≥n de palabras en com√∫n)\n",
        "        intersection = len(words.intersection(existing_words))\n",
        "        union = len(words.union(existing_words))\n",
        "\n",
        "        if union > 0 and intersection / union > threshold:\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def call_openrouter_api(\n",
        "    prompt: str,\n",
        "    model: str,\n",
        "    api_key: str,\n",
        "    system_message: Optional[str] = None,\n",
        "    max_retries: int = 3\n",
        ") -> Optional[Dict[Any, Any]]:\n",
        "    \"\"\"\n",
        "    Realiza una llamada a la API de OpenRouter.\n",
        "\n",
        "    Args:\n",
        "        prompt: Prompt a enviar.\n",
        "        model: Nombre del modelo a utilizar.\n",
        "        api_key: Clave de API de OpenRouter.\n",
        "        system_message: Mensaje de sistema opcional.\n",
        "        max_retries: N√∫mero m√°ximo de reintentos en caso de fallo.\n",
        "\n",
        "    Returns:\n",
        "        Respuesta de la API o None si hubo un error.\n",
        "    \"\"\"\n",
        "    url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": \"https://colab.research.google.com/\"\n",
        "    }\n",
        "\n",
        "    messages = []\n",
        "    if system_message:\n",
        "        messages.append({\"role\": \"system\", \"content\": system_message})\n",
        "\n",
        "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    data = {\n",
        "        \"model\": model,\n",
        "        \"messages\": messages\n",
        "    }\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.post(url, headers=headers, json=data)\n",
        "            response.raise_for_status()\n",
        "            return response.json()\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error en la llamada a la API (intento {attempt+1}/{max_retries}): {e}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                # Esperar un tiempo exponencial antes de reintentar\n",
        "                wait_time = 2 ** attempt\n",
        "                print(f\"Reintentando en {wait_time} segundos...\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                print(\"Se alcanz√≥ el n√∫mero m√°ximo de reintentos.\")\n",
        "                return None\n",
        "\n",
        "def extract_response(api_response: Dict[Any, Any]) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Extrae la respuesta del modelo desde la respuesta de la API.\n",
        "\n",
        "    Args:\n",
        "        api_response: Respuesta de la API.\n",
        "\n",
        "    Returns:\n",
        "        Texto de la respuesta o None si no se puede extraer.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return api_response['choices'][0]['message']['content']\n",
        "    except (KeyError, IndexError, TypeError) as e:\n",
        "        print(f\"Error al extraer la respuesta: {e}\")\n",
        "        print(f\"Respuesta completa de la API: {json.dumps(api_response, indent=2)}\")\n",
        "        return None\n",
        "\n",
        "def save_to_csv(data: List[Dict[str, str]], output_file: str) -> None:\n",
        "    \"\"\"\n",
        "    Guarda los datos en un archivo CSV y permite descargarlo desde Colab.\n",
        "\n",
        "    Args:\n",
        "        data: Lista de diccionarios con los datos a guardar.\n",
        "        output_file: Ruta al archivo de salida.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(output_file, 'w', encoding='utf-8', newline='') as file:\n",
        "            fieldnames = ['instruction', 'output', 'model']\n",
        "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "\n",
        "            writer.writeheader()\n",
        "            for item in data:\n",
        "                writer.writerow(item)\n",
        "\n",
        "        print(f\"Dataset guardado exitosamente en {output_file}\")\n",
        "\n",
        "        # Permitir la descarga del archivo desde Colab\n",
        "        files.download(output_file)\n",
        "    except Exception as e:\n",
        "        print(f\"Error al guardar el dataset: {e}\")\n",
        "\n",
        "def generate_dataset_single_prompt(\n",
        "    prompt: str,\n",
        "    output_file: str,\n",
        "    models: List[str],\n",
        "    api_key: str,\n",
        "    num_iterations: int = 10\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Genera un dataset usando un √∫nico prompt general m√∫ltiples veces.\n",
        "\n",
        "    Args:\n",
        "        prompt: Prompt general para generar preguntas y respuestas.\n",
        "        output_file: Ruta al archivo de salida.\n",
        "        models: Lista de modelos a utilizar.\n",
        "        api_key: Clave de API de OpenRouter.\n",
        "        num_iterations: N√∫mero de veces que se utilizar√° el prompt.\n",
        "    \"\"\"\n",
        "    print(f\"Modo: Prompt √∫nico - Generando {num_iterations} ejemplos por modelo\")\n",
        "\n",
        "    # Definir temas para rotar\n",
        "    topics = [\n",
        "        \"cookies y navegaci√≥n web\",\n",
        "        \"derechos ARCO\",\n",
        "        \"redes sociales\",\n",
        "        \"videovigilancia\",\n",
        "        \"menores y consentimiento\",\n",
        "        \"geolocalizaci√≥n\",\n",
        "        \"datos biom√©tricos\",\n",
        "        \"marketing directo\",\n",
        "        \"filtraciones de datos\",\n",
        "        \"transferencias internacionales\",\n",
        "        \"derecho al olvido\",\n",
        "        \"uso de IA con datos personales\",\n",
        "        \"aplicaciones m√≥viles\",\n",
        "        \"datos en el √°mbito laboral\",\n",
        "        \"informaci√≥n de salud\",\n",
        "        \"datos bancarios y financieros\"\n",
        "    ]\n",
        "\n",
        "    # Crear archivo para ejemplos fallidos\n",
        "    fallidos_file = f\"ejemplos_fallidos_{int(time.time())}.txt\"\n",
        "\n",
        "    # Preparar el dataset\n",
        "    dataset = []\n",
        "    existing_questions = []\n",
        "    fallidos = 0\n",
        "\n",
        "    # Procesar cada modelo y generar m√∫ltiples ejemplos\n",
        "    total_iterations = len(models) * num_iterations\n",
        "    with tqdm(total=total_iterations, desc=\"Generando dataset\") as pbar:\n",
        "        for model in models:\n",
        "            print(f\"\\nProcesando modelo: {model}\")\n",
        "\n",
        "            for i in range(num_iterations):\n",
        "                # Seleccionar tema para esta iteraci√≥n\n",
        "                current_topic = topics[i % len(topics)]\n",
        "\n",
        "                # Crear un prompt espec√≠fico para esta iteraci√≥n\n",
        "                iteration_prompt = f\"\"\"\n",
        "{prompt}\n",
        "\n",
        "‚ö†Ô∏è INSTRUCCI√ìN CRUCIAL: Genera UN √öNICO par de pregunta-respuesta sobre protecci√≥n de datos.\n",
        "\n",
        "TEMA ESPEC√çFICO: Genera una pregunta relacionada con \"{current_topic}\".\n",
        "Aseg√∫rate de que sea una pregunta concreta y relevante para usuarios espa√±oles.\n",
        "\n",
        "FORMATO EXACTO A SEGUIR:\n",
        "[Escribe aqu√≠ UNA √öNICA pregunta sobre {current_topic}]\n",
        "RESPUESTAMODELO\n",
        "[Escribe aqu√≠ la respuesta a esa √∫nica pregunta]\n",
        "\n",
        "RECUERDA: Solo UN par pregunta-respuesta. Termina tu respuesta despu√©s de contestar la pregunta.\n",
        "\"\"\"\n",
        "\n",
        "                # Intentar hasta 3 veces si obtenemos duplicados\n",
        "                max_attempts = 3\n",
        "                success = False\n",
        "\n",
        "                for attempt in range(max_attempts):\n",
        "                    # Llamar a la API\n",
        "                    response_data = call_openrouter_api(iteration_prompt, model, api_key)\n",
        "\n",
        "                    if response_data:\n",
        "                        # Extraer la respuesta\n",
        "                        response_text = extract_response(response_data)\n",
        "\n",
        "                        if response_text:\n",
        "                            # Extraer pregunta y respuesta del texto generado\n",
        "                            question, answer = extract_question_answer(response_text)\n",
        "\n",
        "                            # Validar el par pregunta-respuesta\n",
        "                            if validate_qa_pair(question, answer):\n",
        "                                # Verificar si es similar a preguntas existentes\n",
        "                                if not is_similar_to_existing(question, existing_questions):\n",
        "                                    # A√±adir al dataset\n",
        "                                    dataset.append({\n",
        "                                        'instruction': question,\n",
        "                                        'output': answer,\n",
        "                                        'model': model\n",
        "                                    })\n",
        "                                    # Guardar para futuras comparaciones\n",
        "                                    existing_questions.append(question)\n",
        "\n",
        "                                    print(f\"\\nIteraci√≥n {i+1}/{num_iterations}:\")\n",
        "                                    print(f\"Tema: {current_topic}\")\n",
        "                                    print(f\"Pregunta: {question[:100]}...\")\n",
        "                                    print(f\"Respuesta: {answer[:100]}...\")\n",
        "\n",
        "                                    success = True\n",
        "                                    break\n",
        "                                else:\n",
        "                                    print(f\"\\n‚ö†Ô∏è Intento {attempt+1}: Pregunta similar ya existe, reintentando...\")\n",
        "                                    # A√±adir m√°s variaci√≥n al prompt\n",
        "                                    iteration_prompt += f\"\\n\\nIMPORTANTE: Aseg√∫rate de que la pregunta sea ORIGINAL y DIFERENTE a esta: \\\"{question}\\\"\"\n",
        "                            else:\n",
        "                                print(f\"\\n‚ö†Ô∏è Intento {attempt+1}: Par pregunta-respuesta no v√°lido, reintentando...\")\n",
        "                                print(f\"\\nRespuesta completa del modelo:\")\n",
        "                                print(\"-\" * 50)\n",
        "                                print(response_text)\n",
        "                                print(\"-\" * 50)\n",
        "\n",
        "                if not success:\n",
        "                    fallidos += 1\n",
        "                    print(f\"\\n‚ùå No se pudo generar un par √∫nico en {max_attempts} intentos para el tema: {current_topic}\")\n",
        "\n",
        "                    # Guardar ejemplo fallido\n",
        "                    with open(fallidos_file, \"a\", encoding=\"utf-8\") as f:\n",
        "                        f.write(f\"--- EJEMPLO FALLIDO {fallidos} ---\\n\")\n",
        "                        f.write(f\"MODELO: {model}\\n\")\n",
        "                        f.write(f\"TEMA: {current_topic}\\n\")\n",
        "                        if 'response_text' in locals():\n",
        "                            f.write(f\"√öLTIMO INTENTO:\\n{response_text}\\n\\n\")\n",
        "                        f.write(\"-\" * 50 + \"\\n\\n\")\n",
        "\n",
        "                pbar.update(1)\n",
        "\n",
        "                # Peque√±a pausa para evitar sobrecargar la API\n",
        "                time.sleep(0.5)\n",
        "\n",
        "    # Guardar el dataset\n",
        "    if dataset:\n",
        "        save_to_csv(dataset, output_file)\n",
        "        print(f\"\\n‚úÖ Se han generado {len(dataset)} ejemplos √∫nicos para el dataset.\")\n",
        "        print(f\"‚ùå Se han detectado {fallidos} ejemplos con formato incorrecto o duplicados.\")\n",
        "        if fallidos > 0:\n",
        "            print(f\"üìù Los ejemplos fallidos se han guardado en '{fallidos_file}'.\")\n",
        "    else:\n",
        "        print(\"‚ùå No se pudieron generar ejemplos v√°lidos para el dataset.\")\n",
        "\n",
        "# Ejecutar en Colab (interfaz interactiva)\n",
        "def main_colab():\n",
        "    from IPython.display import display, HTML\n",
        "\n",
        "    display(HTML(\"<h3>Generador de Dataset para Fine-Tuning</h3>\"))\n",
        "\n",
        "    # Solicitar los par√°metros\n",
        "    api_key = input(\"Introduce tu clave API de OpenRouter: \")\n",
        "\n",
        "\n",
        "    # Entrada directa del prompt\n",
        "    print(\"\\nIntroduce el prompt general (escribe o pega el texto y presiona Enter dos veces para finalizar):\")\n",
        "    print(\"Para terminar la entrada, escribe una l√≠nea que solo contenga '***FIN***'\")\n",
        "\n",
        "    lines = []\n",
        "    while True:\n",
        "        line = input()\n",
        "        if line == \"***FIN***\":\n",
        "            break\n",
        "        lines.append(line)\n",
        "\n",
        "    general_prompt = \"\\n\".join(lines)\n",
        "\n",
        "    if not general_prompt.strip():\n",
        "        print(\"El prompt no puede estar vac√≠o.\")\n",
        "        return\n",
        "\n",
        "\n",
        "    # Solicitar el n√∫mero de iteraciones\n",
        "    num_iterations = input(\"Introduce el n√∫mero de ejemplos a generar por modelo [10]: \")\n",
        "    num_iterations = int(num_iterations) if num_iterations.strip() else 10\n",
        "\n",
        "    # Solicitar los modelos\n",
        "    models_input = input(\"Introduce los modelos a utilizar (separados por comas) [deepseek/deepseek-chat:free]: \")\n",
        "    models = [m.strip() for m in models_input.split(\",\")] if models_input.strip() else [\"deepseek/deepseek-chat:free\"]\n",
        "\n",
        "    # Solicitar el nombre del archivo de salida\n",
        "    output_file = input(\"Introduce el nombre del archivo de salida [dataset.csv]: \")\n",
        "    output_file = output_file if output_file.strip() else \"dataset.csv\"\n",
        "\n",
        "    print(\"\\nResumen de la configuraci√≥n:\")\n",
        "    print(f\"- Prompt general: '{general_prompt[:100]}...'\")\n",
        "    print(f\"- N√∫mero de iteraciones por modelo: {num_iterations}\")\n",
        "    print(f\"- Modelos: {', '.join(models)}\")\n",
        "    print(f\"- Archivo de salida: {output_file}\")\n",
        "\n",
        "    confirm = input(\"\\n¬øConfirmar y comenzar la generaci√≥n? (s/n): \")\n",
        "    if confirm.lower() in [\"s\", \"si\", \"s√≠\", \"y\", \"yes\"]:\n",
        "        generate_dataset_single_prompt(\n",
        "            prompt=general_prompt,\n",
        "            output_file=output_file,\n",
        "            models=models,\n",
        "            api_key=api_key,\n",
        "            num_iterations=num_iterations\n",
        "        )\n",
        "    else:\n",
        "        print(\"Operaci√≥n cancelada.\")\n",
        "\n",
        "\n",
        "\n",
        "# C√≥digo para ejecutar directamente en una celda de Colab\n",
        "main_colab()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn823uUfWxYC"
      },
      "source": [
        "Prompt √∫nico para generar el dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0t6fjKVrwSWJ"
      },
      "outputs": [],
      "source": [
        "üí° Objetivo: Generar UN √öNICO par de pregunta-respuesta sobre privacidad y protecci√≥n de datos en Espa√±a para entrenar un modelo con RAG.\n",
        "\n",
        "‚ö†Ô∏è INSTRUCCI√ìN CRUCIAL: Genera S√ìLO UN par pregunta-respuesta en cada ejecuci√≥n. No incluyas m√∫ltiples ejemplos ni numeraci√≥n.\n",
        "\n",
        "üìä Temas a cubrir (diversidad tem√°tica):\n",
        "üîπ Derechos digitales: acceso, rectificaci√≥n, supresi√≥n, oposici√≥n, portabilidad, limitaci√≥n, olvido.\n",
        "üîπ Consentimiento: validez, revocaci√≥n, menores, excepciones.\n",
        "üîπ Cookies y tracking: tipos, banners, rechazos, perfilado.\n",
        "üîπ Datos sensibles: salud, biom√©tricos, ideolog√≠a, orientaci√≥n sexual.\n",
        "üîπ Contextos espec√≠ficos: laboral, educativo, sanitario, financiero, comercial.\n",
        "üîπ Tecnolog√≠as emergentes: IA, reconocimiento facial, IoT, blockchain.\n",
        "üîπ Seguridad: brechas, notificaciones, medidas t√©cnicas.\n",
        "üîπ Transferencias: internacionales, entre empresas, cesiones.\n",
        "üîπ Videovigilancia: √°mbito privado, p√∫blico, laboral.\n",
        "üîπ Responsabilidades: empresas, DPO, encargados, autoridades.\n",
        "üîπ Sanciones: sin mencionar cantidades espec√≠ficas.\n",
        "\n",
        "üîπ Caracter√≠sticas de las preguntas:\n",
        "‚úîÔ∏è Pregunta concreta (10-30 palabras) enfocada en un √∫nico tema\n",
        "‚úîÔ∏è Evita preguntas gen√©ricas; usa casos pr√°cticos realistas\n",
        "‚úîÔ∏è Incluye contexto espec√≠fico (qui√©n, d√≥nde, situaci√≥n concreta)\n",
        "‚úîÔ∏è Var√≠a la formulaci√≥n: usa \"¬øEs legal...?\", \"¬øQu√© derechos tengo si...?\", \"¬øQu√© ocurre cuando...?\"\n",
        "‚úîÔ∏è Aseg√∫rate de que sea una pregunta ORIGINAL y diferente a los ejemplos\n",
        "\n",
        "üîπ Caracter√≠sticas de las respuestas:\n",
        "‚úÖ 2-4 frases concisas pero completas\n",
        "‚úÖ Explica condiciones y matices (no solo \"s√≠/no\")\n",
        "‚úÖ Lenguaje claro sin jerga legal\n",
        "‚úÖ No menciones art√≠culos o leyes espec√≠ficas\n",
        "‚úÖ Tono profesional pero accesible\n",
        "‚úÖ Informaci√≥n actualizada seg√∫n RGPD y LOPDGDD\n",
        "\n",
        "üîπ Precisi√≥n de las respuestas:\n",
        "‚úÖ Aseg√∫rate de que la respuesta refleje con precisi√≥n el marco legal espa√±ol actual (RGPD y LOPDGDD)\n",
        "‚úÖ Si existe ambig√ºedad legal, menci√≥nalo expl√≠citamente\n",
        "‚úÖ Evita respuestas que puedan resultar enga√±osas por simplificar en exceso\n",
        "‚úÖ No incluyas opiniones personales o interpretaciones controvertidas\n",
        "\n",
        "\n",
        "üîπ Tono de la respuesta:\n",
        "‚úÖ Profesional pero accesible\n",
        "‚úÖ Objetivo y no alarmista\n",
        "‚úÖ Informativo sin ser condescendiente\n",
        "‚úÖ Directo sin ser brusco\n",
        "\n",
        "üîπ Escenarios espec√≠ficos a considerar (Estos son solo ejemplos puedes generar tus propios escenarios, se original):\n",
        "- Una persona intentando ejercer sus derechos frente a una gran empresa\n",
        "- Un empleado con preocupaciones sobre la privacidad en su trabajo\n",
        "- Un padre/madre preocupado por los datos de sus hijos menores\n",
        "- Un usuario de aplicaciones m√≥viles o servicios online\n",
        "- Un propietario de peque√±o negocio que necesita cumplir con la normativa\n",
        "- Situaciones en comunidades de vecinos o espacios compartidos\n",
        "- Interacciones con administraciones p√∫blicas\n",
        "- Espionaje entre compa√±eros de trabajo\n",
        "- Empresas comerciando con datos privador\n",
        "\n",
        "‚ö†Ô∏è IMPORTANTE: Aseg√∫rate de que la pregunta sea ORIGINAL y no se parezca demasiado a los ejemplos proporcionados. Busca √°ngulos o situaciones novedosas dentro del tema elegido.\n",
        "\n",
        "\n",
        "‚ö†Ô∏è FORMATO EXACTO A SEGUIR:\n",
        "\n",
        "\n",
        "Pregunta sobre los temas nombrados anteriormente\n",
        "RESPUESTAMODELO\n",
        "Respuesta clara, razonada, y convincente\n",
        "\n",
        "üìù Ejemplos (para referencia):\n",
        "\n",
        "\n",
        "\"Un comercio online me instal√≥ cookies de seguimiento sin avisarme. ¬øEsto es legal?\"\n",
        "RESPUESTAMODELO\n",
        "\"No, en Espa√±a un sitio web solo puede instalar cookies de seguimiento si obtiene tu consentimiento previo. Deben informarte de manera clara sobre su uso y permitirte aceptarlas o rechazarlas f√°cilmente. Solo las cookies estrictamente necesarias pueden activarse sin tu permiso.\"\n",
        "\n",
        "\"Una empresa de cr√©dito ha rechazado mi solicitud bas√°ndose en un algoritmo. ¬øPuedo exigir una explicaci√≥n?\"\n",
        "RESPUESTAMODELO\n",
        "\"S√≠, cuando una decisi√≥n automatizada afecta significativamente tus derechos, puedes solicitar una explicaci√≥n clara sobre los criterios utilizados. Tambi√©n puedes pedir una revisi√≥n manual si crees que la decisi√≥n ha sido injusta o err√≥nea.\"\n",
        "\n",
        "\"Trabajo desde casa y mi empresa usa software de monitoreo en mi ordenador. ¬øHasta qu√© punto es legal?\"\n",
        "RESPUESTAMODELO\n",
        "\"El monitoreo es legal si est√° justificado y comunicado de manera transparente. Tu empresa debe informarte sobre qu√© datos recopila, con qu√© finalidad y durante cu√°nto tiempo. Adem√°s, el control debe ser proporcional y no invadir tu privacidad m√°s all√° de lo necesario para evaluar tu rendimiento laboral.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHSZV3dZ9X0Y"
      },
      "source": [
        "Para detectar preguntas duplicadas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NId9n1E59ZmX"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv('/content/FinalData - Hoja 1 (4).csv')\n",
        "\n",
        "# Calcular similitud entre preguntas\n",
        "vectorizer = TfidfVectorizer(stop_words='english')  # Puedes ajustar a espa√±ol si necesitas\n",
        "vectors = vectorizer.fit_transform(df['instruction'])\n",
        "similarity = cosine_similarity(vectors)\n",
        "\n",
        "# Umbral de similaridad (ajustar seg√∫n necesidad)\n",
        "threshold = 0.8\n",
        "\n",
        "# Identificar grupos de preguntas similares\n",
        "grupos_similares = []\n",
        "ya_procesadas = set()\n",
        "\n",
        "for i in range(len(df)):\n",
        "    if i in ya_procesadas:\n",
        "        continue\n",
        "\n",
        "    similar_indices = []\n",
        "    for j in range(len(df)):\n",
        "        if i != j and similarity[i, j] > threshold:\n",
        "            similar_indices.append(j)\n",
        "\n",
        "    if similar_indices:\n",
        "        grupo = [i] + similar_indices\n",
        "        grupos_similares.append(grupo)\n",
        "        ya_procesadas.update(similar_indices)\n",
        "\n",
        "# Mostrar los grupos de preguntas similares\n",
        "print(f\"Se encontraron {len(grupos_similares)} grupos de preguntas similares:\\n\")\n",
        "\n",
        "for idx, grupo in enumerate(grupos_similares):\n",
        "    print(f\"Grupo {idx+1} (Similaridad > {threshold}):\")\n",
        "    for i, ind in enumerate(grupo):\n",
        "        print(f\"  {i+1}. √çndice {ind}: {df['instruction'].iloc[ind][:100]}...\")\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "# Opcional: Exportar los resultados a un CSV para revisar m√°s f√°cilmente\n",
        "output_rows = []\n",
        "\n",
        "for group_idx, grupo in enumerate(grupos_similares):\n",
        "    for idx in grupo:\n",
        "        output_rows.append({\n",
        "            'grupo': group_idx + 1,\n",
        "            'indice_original': idx,\n",
        "            'pregunta': df['instruction'].iloc[idx],\n",
        "            'respuesta': df['output'].iloc[idx],\n",
        "            'modelo': df['model'].iloc[idx] if 'model' in df.columns else 'Unknown'\n",
        "        })\n",
        "\n",
        "grupos_df = pd.DataFrame(output_rows)\n",
        "output_file = 'grupos_similares.csv'\n",
        "grupos_df.to_csv(output_file, index=False)\n",
        "print(f\"Resultados exportados a {output_file}\")\n",
        "\n",
        "# Descargar el archivo\n",
        "files.download(output_file)\n",
        "\n",
        "# Contar cu√°ntas preguntas est√°n en los grupos (posibles duplicados)\n",
        "num_duplicados = sum(len(grupo) for grupo in grupos_similares) - len(grupos_similares)\n",
        "print(f\"\\nN√∫mero total de posibles preguntas duplicadas: {num_duplicados}\")\n",
        "print(f\"N√∫mero de filas en el dataset original: {len(df)}\")\n",
        "print(f\"N√∫mero estimado de filas despu√©s de eliminar duplicados: {len(df) - num_duplicados}\")\n",
        "\n",
        "grupos_similares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ7qPeJLTl28"
      },
      "source": [
        "# Finetunning Llama 3\n",
        "---Resumen\n",
        "Aqui poner que es unsloth que usamos y que modelos se pueden usar para hacer finetunning\n",
        "\n",
        "La biblioteca Unsloth permite completar el proceso de entrenamiento y entrenamiento fino 2x m√°s r√°pido y requiere mucha menos VRAM gracias a derivaciones matem√°ticas complejas y kernels de GPUs optimizados manualmente\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSX6xhJ5TbIZ"
      },
      "outputs": [],
      "source": [
        "#%%Capture para evitar que genere salida en collab el comando de install\n",
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install unsloth\n",
        "# Get latest Unsloth\n",
        "!pip install --upgrade --no-deps \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS0fAdKLiWlX"
      },
      "source": [
        "Debido a que tenemos recursos limitados vamos a usar modelos cuantizados en 4 bits para reducir el consumo de memoria y el espacio en disco que utilizan.\n",
        "\n",
        "¬øQue significa cuantizar modelos?\n",
        "\n",
        "Normalmente los modelos almacenan sus par√°metros en Punto Flotante de 16 Bits, al cuantizar los modelos de 16 Bits a 4 Bits conseguimos:\n",
        "\n",
        "‚úÖ Menos uso de VRAM/RAM ‚Üí Nos permite utilizar modelos m√°s grandes que debido a limitaciones de google collab no podr√≠amos ejecutar en este entorno.\n",
        "\n",
        "‚úÖ Inferencia m√°s r√°pida ‚Üí Al reducir los datos aumenta la velocidad de c√°lculo.\n",
        "\n",
        "‚úÖ Descarga m√°s r√°pida ‚Üí Al reducir el tama√±o en disco que ocupan conseguimos ahorrarnos tiempo en descargar/subir los distintos modelos adem√°s de ahorrar espacio en google collab, el cual esta muy limitado.\n",
        "\n",
        "¬øQue desventajas tiene?\n",
        "\n",
        "\n",
        "üî¥ P√©rdida de precisi√≥n ‚Üí Reducir los bits disminuye la calidad de las respuestas, afectando tareas complejas.\n",
        "\n",
        "üî¥ Problemas en c√°lculos precisos ‚Üí Modelos cuantizados pueden fallar en matem√°ticas avanzadas o generaci√≥n de c√≥digo detallado.\n",
        "\n",
        "üü†  Fine-tuning m√°s dif√≠cil ‚Üí La cuantizaci√≥n a 4 bits reduce la precisi√≥n de los pesos. Unsloth utiliza t√©cnicas optimizadas para reducir el efecto negativo que esto produce en el finetunning, pero sigue habiendo ligeras restricciones comparado con modelos en FP16 o FP32."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g4aEbvOzUK-d"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Se puede poner la longitud que se quiera, Unsloth utiliza RoPE Scaling\n",
        "dtype = None # None para detecci√≥n autom√°tica de la GPU. Float16 para Tesla T4, V100, Bfloat16 para Ampere+\n",
        "load_in_4bit = True # Usamos 4bit para reducir el uso de memoria. Can be False.\n",
        "\n",
        "\n",
        "#Modelos 4bit cuantizados por unsloth\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
        "    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGPMaCTDqUyj"
      },
      "source": [
        "Ahora vamos a utilizar LoRA (Low-Rank Adapter), esto nos permite re-entrenar un modelo sin modificar toda su estructura. En lugar de ajustar todos los par√°metros del modelo base, LoRA introduce dos nuevas matrices que se encargan de aprender y almacenar las actualizaciones espec√≠ficas necesarias durante el fine-tuning.\n",
        "\n",
        "Una matriz se especializa en capturar las modificaciones necesarias durante el entrenamiento y la otra ayuda a combinarlas con los par√°metros originales del modelo.\n",
        "\n",
        "Gracias a que solo se actualizan estas nuevas matrices en lugar de todos los par√°metros del modelo original, podemos realizar fine-tunning de modelos m√°s grandes con un hardware limitado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TNWXr-KltxGq"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEZyUecvuras"
      },
      "source": [
        "## Preparaci√≥n del dataset\n",
        "\n",
        "Para que Ollama y llama.cpp funcionen como un chatbot personalizado como por ejemplo ChatGPT, solo debemos tener 2 columnas: una de instrucciones y una de salida, por lo que nuestro dataset solo consta de esta dos columnas.\n",
        "\n",
        "Vamos a utilizar la librer√≠a de load_dataset del paquete datasets de hugging face, la cual nos facilita el uso de datasets para fine-tunning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmWNeFp-vHGZ"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\n",
        "    \"csv\",\n",
        "    data_files = \"/content/input.csv\",\n",
        "    split = \"train\",\n",
        ")\n",
        "print(dataset.column_names)\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-tD_iyYxTR4"
      },
      "source": [
        "Una vez ya tenemos cargado el dataset le vamos a dar un formato adecuado para fine-tunning.\n",
        "\n",
        "Vamos a utilizar la libreria to_sharegpt de Unsloth, para generar conversaciones largas combinando los inputs y outputs del dataset.\n",
        "\n",
        "Gracias a esto en vez de hacer el finetunning con preguntas sueltas, realizamos el entrenamiento con conversaciones simuladas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8H4EsAxxS1U"
      },
      "outputs": [],
      "source": [
        "from unsloth import to_sharegpt\n",
        "dataset = to_sharegpt(\n",
        "    dataset,\n",
        "    merged_prompt = \\\n",
        "        \"[[La pregunta es: {instruction}.]]\"\n",
        "        ,\n",
        "    conversation_extension = 5, # Este par√°metro agrupa aleatoriamente hasta x preguntas y respuestas en una sola conversaci√≥n. Esto es √∫til para simular conversaciones m√°s largas.\n",
        "    output_column_name = \"output\",\n",
        ")\n",
        "#Para imprimir como se ve ahora el dataset\n",
        "from pprint import pprint\n",
        "pprint(dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXKKSvVgzYB-"
      },
      "source": [
        "El dataset ahora utiliza etiquetas como \"human\" y \"gpt\":\n",
        "\n",
        "```\n",
        "{'conversations': [{'from': 'human',\n",
        "                    'value': \"La pregunta es: ('Antonio instala c√°maras en su \"\n",
        "                             'tienda sin avisar a empleados y clientes. ¬øQu√© '\n",
        "                             \"consecuencias puede tener?',).\"},\n",
        "                   {'from': 'gpt',\n",
        "                    'value': 'Seg√∫n el Art√≠culo 22 de la Ley Org√°nica 3/2018 '\n",
        "                             'de Protecci√≥n de Datos Personales y Garant√≠a de '\n",
        "                             'los Derechos Digitales, debe informar a los '\n",
        "                             'afectados sobre la videovigilancia. Si no lo '\n",
        "                             'hace, puede enfrentar sanciones de la AEPD.'},\n",
        "```\n",
        "Sin embargo para un modelo de OpenAI o Hugging Face, se requieren etiquetas est√°ndar como **user** para el usuario y **assistant** para el modelo.\n",
        "\n",
        "Para arreglar esto vamos a utilizar la libreria standardize_sharegpt de Unsloth que cambia todas las etiquetas como human, gpt, system, etc... a **user** y **assistant**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HR7_HxXysv7"
      },
      "outputs": [],
      "source": [
        "from unsloth import standardize_sharegpt\n",
        "dataset = standardize_sharegpt(dataset)\n",
        "\n",
        "#Para imprimir como se ve ahora el dataset\n",
        "from pprint import pprint\n",
        "pprint(dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo0Yv5Mt00NF"
      },
      "source": [
        "## Plantilla de conversaci√≥n con el modelo\n",
        "\n",
        "Una plantilla de chat es √∫til para el fine-tuning porque proporciona una estructura coherente que ense√±a al modelo c√≥mo interactuar de una mejor manera en las conversaciones, y por lo tanto mejorar la calidad de sus respuestas.\n",
        "\n",
        "Este es el formato de  un Prompt de Llama-3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2SRFD38096V"
      },
      "outputs": [],
      "source": [
        "#chat_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "#{SYSTEM}<|end_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "#{INPUT}<|end_of_text|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "#{OUTPUT}<|end_of_text|>\"\"\"\n",
        "\n",
        "chat_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "{SYSTEM}<|eot_id|>\n",
        "<|start_header_id|>user<|end_header_id|>\n",
        "{INPUT}<|eot_id|>\n",
        "<|start_header_id|>assistant<|end_header_id|>\n",
        "{OUTPUT}<|eot_id|>\"\"\"\n",
        "\n",
        "from unsloth import apply_chat_template\n",
        "dataset = apply_chat_template(\n",
        "    dataset,\n",
        "    tokenizer = tokenizer,\n",
        "    chat_template = chat_template,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htc-yqdK6Ny_"
      },
      "source": [
        "## Entrenamiento del modelo\n",
        "\n",
        "Vamos a utilizar el Transformers Reinforcement Learning (TRL) de Hugginface **SFTTrainer** (Supervised Fine-Tuning) el cual permite entrenar modelos preexistentes con datos etiquetados para mejorar su desempe√±o en tareas espec√≠ficas.\n",
        "\n",
        "Tambien se puede utilizar DPOTrainer: Utiliza el aprendizaje por refuerzo directo (Direct Preference Optimization) para mejorar las respuestas generadas por el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6R0i0OqB597z"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = 2048,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 40,\n",
        "        #max_steps=None,\n",
        "        num_train_epochs=4,\n",
        "        learning_rate =  5e-5,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 10,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "#Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdUVpTcH7ThB"
      },
      "source": [
        "Es necesario tener una cuenta en https://wandb.ai/authorize y obtener la clave API para poder hacer el entrenamiento\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3SWM2RF7Vu7"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8iM7gau7ZYo"
      },
      "source": [
        "### Memoria Final y estadisticas de tiempo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvKJml8M7eBY"
      },
      "outputs": [],
      "source": [
        "\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRBXK1wG7u4U"
      },
      "source": [
        "\n",
        "### Inferencia\n",
        "Vamos a ejecutar el modelo, Unsloth hace la inferencia de manera nativa 2 veces m√°s r√°pida. Hay que usar promtp similares al finetunning para obtener buenos resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w-i_hr1J73N0"
      },
      "outputs": [],
      "source": [
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "messages = [                    # Change below!\n",
        "    #{\"role\": \"user\", \"content\": '¬øCu√°l es el objeto de la Ley Org√°nica 3/2018 seg√∫n el Art√≠culo 1? \\n'},\n",
        "    {\"role\": \"user\", \"content\": '\"¬øPuede el colegio hacer fotos a los alumnos y publicarlas en la web del colegio?\"\\n'},\n",
        "    #{\"role\": \"user\", \"content\": '\"¬øCu√°l es el deber de confidencialidad seg√∫n el Art√≠culo 5 de la Ley Org√°nica 3/2018?\"\\n'},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGohnNGT8cLm"
      },
      "source": [
        "## Guardar y Cargar el modelo finetuneado\n",
        "\n",
        "Para guardar el modelo final como adaptadores LoRA, utiliza `push_to_hub` de Huggingface para guardarlo en l√≠nea o `save_pretrained` para guardarlo localmente.\n",
        "\n",
        "[NOTA] Esto SOLO guarda los adaptadores LoRA, no el modelo completo. ¬°Para guardarlo en 16bit o GGUF, baja m√°s abajo!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXD4le6y94uc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"lora_model\") # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29AH6HEI98bC"
      },
      "source": [
        "Ahora, si deseas cargar los adaptadores LoRA que acabamos de guardar para inferencia, cambia False a True:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJma3s7D97yP"
      },
      "outputs": [],
      "source": [
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "pass\n",
        "\n",
        "messages = [                    # Change below!\n",
        "    {\"role\": \"user\", \"content\": '¬øPuede un trabajador de un supermercado pedirme el DNI?\\n'\\\n",
        "                                '\\n'\\\n",
        "                                ''},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XnT-Voc_6Hr"
      },
      "source": [
        "A continuaci√≥n, guardaremos el modelo en GGUF / llama.cpp.\n",
        "\n",
        "Clonamos llama.cpp y por defecto lo guardamos en q8_0. Permitimos todos los m√©todos como q4_k_m. Utiliza `save_pretrained_gguf` para guardarlo localmente y `push_to_hub_gguf` para subirlo a Hugging Face.\n",
        "\n",
        "Algunos m√©todos de cuantificaci√≥n compatibles (lista completa en nuestra p√°gina de Wiki):\n",
        "\n",
        "- q8_0: Conversi√≥n r√°pida. Uso de recursos alto, pero generalmente aceptable.\n",
        "- q4_k_m: Recomendado. Utiliza Q6_K para la mitad de los tensores `attention.wv` y `feed_forward.w2`, el resto usa Q4_K.\n",
        "- q5_k_m: Recomendado. Utiliza Q6_K para la mitad de los tensores `attention.wv` y `feed_forward.w2`, el resto usa Q5_K.\n",
        "\n",
        "¬°Tambi√©n soportamos guardar en m√∫ltiples opciones de GGUF en formato de lista! Esto puede acelerar el proceso en 10 minutos o m√°s si deseas varios formatos de exportaci√≥n.\n",
        "\n",
        "El siguiente codigo es para guardar el modelo en la carpeta /model, es necesario tener una api key en hugginface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "0TIoxYH5__Au",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if True: model.push_to_hub_gguf(\"serdom02/Leyeneitor_8bitQ8_0\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"serdom02/model_16bitGGUF\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"serdom02/model_q4_k_mGGUF\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"serdom02/model\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTjohupyAgKP"
      },
      "source": [
        "## Si no subimos el modelo a Hugging Face tenemos que crear el modelo de Ollama\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cXYtIa2EMH3"
      },
      "source": [
        "Ollama necesita un archivo de modelo (Modelfile), que especifica el formato del prompt del modelo. Vamos a imprimir el generado autom√°ticamente por Unsloth:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmgNLwsbERBS"
      },
      "outputs": [],
      "source": [
        "print(tokenizer._ollama_modelfile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5P8Ddl2E5wL"
      },
      "source": [
        "Ahora crearemos un modelo de Ollama llamado `unsloth_model` utilizando el archivo de modelo (Modelfile) que generamos autom√°ticamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBlAipXXE6C3"
      },
      "outputs": [],
      "source": [
        "!ollama create unsloth_model -f ./model/Modelfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGlR5y7pBkQ0"
      },
      "source": [
        "## Descargar el modelo a nuestro ordenador\n",
        "\n",
        "Con este codigo creamos un zip con el modelo para poder descargarlo todo junto a nuestro ordenador local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GhWuu_nBnQ3"
      },
      "outputs": [],
      "source": [
        "!zip -r /content/file.zip /content/model\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"/content/file.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I0aeb-eD26h"
      },
      "source": [
        "Si se ejecuta en el ordenador personal en vez de en colab hay que cambiar la ruta dentro del archivo Modelfile: ![Sin t√≠tulo.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAABA0AAAHMCAYAAAC+8VFbAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAADsMAAA7DAcdvqGQAALNiSURBVHhe7f1ttG3ZWd8HFjgxesPGUAIBVZLAo9GtKgEfPEZsI8gXjx5OGidp7rlVJTnGId1BEmCnJRy3dM8tqXC75ag/dBJbbscJqntu4eEewQG7jUrYUqnq7uIlJHZAUBiJF9U9RwkSCAPt7tEGpKp7Vs9nvqz1zLmeuV72+z7nN2v8Ru215jPnfOZca5+9/v+91r73fM3XfE0D6+erv/qrAQAuPa95zWsAAAAALi3W9dGhUTUNrGAAmI71RwMAAAAAAOCQyEwDS/jA4WIdcAAAuDx81Vd9FQAAwE6xPp/gsPCmgSU4V8UaDAAuF9YHBwAAAAAAHA73IPhhU1gnHAAAwGXiK7/yKwEuNdb7AgAOi3te97rXNW984xubP/En/kTzLd/yLc23fdu3AQBcCL71W78VAAAAAABW4J43velbmm/437yuue9rv6L58i/7kuZLX3nPKK9aEatPjdUGYJu8ciJz4wHWzqsKrJhEGbtBXlFgxSTK2IQVK1ixCSt+KqP9Ges4J76MrdFrr4l1VjtN20cFq81crH4FK3Y/+aIqL3f1HbI9ly8e5RWT+UOTeXmPf2NF/s0eL9sof3gjfMkIIe5LqnzJEoR2L6vQj7ex2l5M/nDEqrMZWiO9hlZMWW8xHDt2bkyrf9kSuHavLGj7qzAWX9a3VN4XOsaqF8bqNVP62yH149fHihWsWMGKFcbqS+bGd+j3RccfdnUl99z75S83hfsQWlwBjFFe9FoxQ8xtX8aXWG00VhuAneAEi8eqE1K9xooTrNgNYQu0ebGbwBpfsGITVrxmTvxQnK6z6kvK+IQVW2K1u5xMMQyWNQ0SqxoGGtsoKOkbB4JlCMxhm8ZBohP968AyCzQhrrv41tgX21MpL8itmCHK9pq58ReFoXlbdcJY/foYO3/K+llogT1VaI/Fl/UtlfeEjmn3F++hXv0Ag/1VcliZef3OOX46dkqboXirzmJufJ/0/gjvEdM0sEyBMSyhBWBhiheF1UZjtQE4aJwIMfeXSJxmrD5Rxg3FVtDCyqrX6NiSKTHbROetsWITVnzCihfGYq16YW6MZk7sHDbV734wZhYkSiOgTr/PxKqGgWCbBCW2aSBYZsAcdmEc1NCCYnVy86C7AC/pX2hPJb8gn49uP6WPMr7EapOw4gUrVrBiBStWsGITU+OtuN1jnTOTGRPZWlhrrNiSsTZlvce9FxJD8X5feB91lPUjjPYX+xxkapwwp19hXvzcc2Mo3qqzmBs/hmkaaIFmGQQlOh5gCFO8DLBqe4C14sRChhUzlbKvhBUrWLHCWH2i0lcSS3qfJhdUHVasYMUm5sQm5sQKc+OF1GZq2zI+YcVqhuLafopjZMVOpe1TYcXNweozYcUfHlrcW2ZBQov/Oro/G8sImIttFFis3zjITYMatsjfBPpCfjW0aTBmHIxhXYAH1O3lCjvWYm77FF8yvf08gb5f8WPz02ug6WLS+ZDHd/U5uo+lGBPZWlhrrNiEF/3yfwupq9WnOosizr9fDFKslVdiTn+e2K7H1DihjF13vJwP8/6eDMVb51pg2Xirrs+oaQCwLjLRMoNV2wOsBScUTKzYMax+1kQSP1ZdSS6WxutL5sYLc2KFTcfvCynv8hyxYqfS9hmxYuZQ9qex4g+TJOYto0BjmwSa3ByoYZkAc7ENgqlswziwBf6mSBfxq6EvwIWwv7sQn4t1EW7FJaz4Eqtdoh4/bBgk6u3tOotDik91NcbOh7n9jZIJ57hvqN7jcmpFttvOiHWRlyv0/ho6frBNuzYV2pjh/Fp0W4s2pujLRMdorFjBihWsWMGKFfJzxzp/cobjy3MtsFq8HZNTGgayD9MAemSCB+Ay4ITBPiJixtov5OLHjkmUsYmx+pK58VAnraXGipvKOvtKlH2uq999wzYKSmyzQLANAgvLBJiLbQZMZVt3HdgCf9Oki+flSBfUVl26IJ9KeQFuxWjK+BKrjWZufMlY+7K+ZG78rgl51o5t2F8/H8b6CzE5us6oLw2BsXpTZLv9nm5fKf41bR8KK07Ta6PGWop193fAzDnfAsvF23V9SsNAwDSADFNQAayCu+DOsGJqlG0TVmyJ1W6PscRMYmpsGTcUC7tn3cdpHf1ctnPGNggCeaxlAszFMgHmYpsBc1iPaSDYhkHCFva7wrq4nksSZZtDX7QLVswumZtfGb9d+iKpy23o2JZ1Fjq+1maoTlDtS1NA1/XqXduJAtsS/olMrMu+kXiPxCisMeew7v4uKt25XJ7P8+Jzyrh+rDYMBEwDaDGFF1xs3MWwuT8h9RZWrGDFTmGZvubG7ym5MAHYDZfxvLSMgkQ/3jIB5mKZAHOxjYC5bONug2WxRf86sC+w5yEX6suwzr4OGy1CNFas0MXkwsaKFfLYrk0XYx0LYay+ZG68gRPiuSGgserjPte2J7RVXVvvtodIbVJfVkxGiitoc4jU9pfoPsbaDNVdDNzxrZxPc8/noXj7/WGRYjswDcDTE2BwMXEXv1XmxApz48dYpq91jb1G+mIDYP+5rOfxNMMgYRkBc7GMgGWwzYC5vPyVI5hmgcYW/qtgC/510b/AXh594T6E1VawYi8HpRixYjr6gmZ4/cbi03bJWH3J3HgDJ8S9aC9I+2v1HtfeE7d7saquxqx4P55+r6Z9Fvq97MapUManbV/vxmznpPdfWIbPpznns4614sv6Ybr3KabBgZMJLYAx3IXvPvIKl9u6ePmG2NY4ALvilU7UvupLRZC6c/wSMmwYJCwjYBlKE2AmrxzAMAfGMA0DwTQKaqQL/9Xpi/1dkl+Qb4Z0QV/DaqOx2owxt/1647UYCftq8WFfEjF5XJ258bvAFOc7xb3/avvdeyHDx0ZkLi2xPjL0HrLiUz8+RsZu4/K2MMzQ+Z/qpoNpcPCYohCghrvg3QXWhbff73L68q94efNVr/mjzdd+7aub13/d12S8TlHWlejYTbLt8QDWydj5+zVfe2/zle79+GVf/jJ3Idh/34LGMgLmIgJ/ApYxMIZhDIxhmgaCaRAYvFL+XwqA5emL933AvghfD1ooa6xYC6ttjbntdxVv7d8Wmz/utkCfinufeKy6ZUj9lX3GfW4dWrLYiMxHx0Rqa1iLD6T6PmU/depj21yUeKsuxzYGxsE0OFBMUQiXA3fBuk70hbBVr9GxGitWsGKFV37pFzdf/TVf0YqYMTEjzIndFLscG2AV9Htnyjn8mq/+Y/59ar1/D5VpdxJMxTIB5iLrO8ArXUyLbM/EMAaGMA2DhGUSCGIUlLgL112SX2wPY7XXWG067AvylXAiLcOKGaLXvhTnY0J83+J3xWrHe0zotnXuGC2PO0db3PZon0Pxus7AzT/DjJH+5P82eg3bsYuYHB2Xo9dS9xe29THT7G98n1XjrZgOyxD4EtcuYNUFMA0OFFNMwmHgLjhHsdoJVuwK2BfDgTmxwtT4V37pH2pe+7rXeGEyVcAk5sYDQCC9d+a8h+5379OLZhyslyT+V0HW1yAzDEqkfgKGMTCIZRaMYpgGQmssWGJgs/QvoG2sthZWWyGP64SCxrpgH8QJuL7wn0G17USR3rbZk/hZpGNj1VnU4tP+nO5cKOMTtXOi3J9wdW49lsf10cOKE6zYmaS8rTohm9tu0MfLYt/iA/3zaJip8WVch2UIYBocKKZYhIuBu9DcB+wL4M0jtzzLHQaWQAGAzTLHMEh81Vd/mX/fWu9ncAJ+rYjQj5hmQSKaAmshmAWlgWAbBBbKKNDouxF2wMtaygvpsL/fpn/Br+n3Y8U5cVZgXbAP4gReDysOCrrjEI6PHaOpx+dxiTy+T6pvGRPTreAWUW8xVK/bl8yJnUEv7wI9tx1iHRvBihWsWMGKFaxYwYoVrFhhrL5kbnxAn9cdliEwBUyDPcMUmrCfuIvGDCumpGyzIewL3N3zx77i5aYwAYD95cv+2JeY72dwAn7dmCZBiSX+V2EV48AJbgtTmG+fzjyomQWa7sLcQl+MW/UBJ9IKrIv2Gj7eCT1Mgznkx6Q7Tnm9Rh+zbv+0+LyNXT8qqMt6L+41Q/VlncWc2DVQzk/P0aqzmBtfoTw2CStWsGIFK1awYgUrVrBihbH6krnxQ1h3FKTzv9yvwTTYI0yRCfuHu1gcZG78GrAvaPeLV77qi5uves2XmaIEAPaXr/yqP1J5Tx/O35/N4AT8OjCNgalYJsAyLGsaCE5w1zDF+b7SXZgvjxNqBp0gtenFO8E3xzRo2y2JHntKP/sTbx2DQBJIVp3F3PhBkgCuCeGyfm9w74NZxHbm/FRMWV8yN36AdBxLrFjBihWsWMGKFaxYwYoVxupL5saPoR9HKN9XwSTQMQFMgz2gJzJhs7iLvX3BvhC9qHxRc9/9X2mKEgDYX+RfVbAeUbicf8c0TrTPRP5ZS41tBMzFMgGWQJsGr1wd22AYwRTz26S7OF+dvvi1sdsk08B/Y6z2D1Fe/OfocfKx+pSx+xpv7d8xXgAbjNVvHHd+J1FukuqXQM9f3ke9ekHHaIrY0fhxSoHcP/dzNhlvxQpj9SVz48dIRkDY7v+t0GZBYmXT4N98xwc86XVt30XFFKWwOdyFWotVP4Ruuyb0BaRVX0O3uyyI6Hjd621RAgD7y2tf/9X+/Vu+py/z37M+TrxPYF2mwSszDANgaWwDYFlMY2AILxb2he5CfSlMYWZgtfWCOJKMA6ttQsWni/6S3hiV9r24xF7H7xE6L81Y/QReYWDFJfLYfyNSi0v1fUpRb9b5+cn/1b4aaS1q8Wl/G6fR9RorNrGL+ETtvdjFdI9Q9WMFHT8Ut27Wahq851f+f81/8I8/3vz7/6+fa979S//KGwT/2fO/2/y7P/LPm3/nv/9nzX/6c7/l933vP//N5nv+2W+YfRwipiCFzeMuzPYJ+8IRanjTwBAkALD/8GOIU3CCfoR1mAa5YZCwDIBlsQ2AZTCNgTHixfFe0BMz6QJ+hEKUjWL1obHalESRbV38T+pvqkDf1/iEFS/4+vJ4jlDGj5LGmUcu7sfrS6bFa7Ffxuo6mzTHoboaU+NGsY6Rxse4eWUUMZpNx7eU70UrxjYPyvguph87zNz4jrWYBsef+P827/6l/0/zf/4X/8qbBX/lF36neefP/3bzf/r4v2z+8s/+VvN9/3MwC972P322eev/+Bmzj0PCFLKwHdwF2T5hXzDCEJgGAIfLLkyDffx7qz8H7NycsB9gc6ZBqrdMgFWwzYCpmKbAFNzF7c5JQqXEXTyPokTZJKw+NFabkiGRXTK3/SHEe+QYuf+b8cVxTFjHXrBi144W9TlpTlZdwBLuVpyQx3bx/f3L0s1pWt9W/CSsYyVYsYIVK1ixghUrWLGCFWsSz0WzzsKOzw2DRIq1Rb8VP4e1mAbv+hf/qvmrv/j/bv7K86VZ8Dl/d8Hbo1nwn/zMrzf/h5/+X80+DgFTxMLyuAurbdO/sINtc1FMg+/7y2839wPsgsd/4DF/Tr7+67+2V/ct3/pvNSe3njDr5rJp0+A7/+IjzSu/1IletW8f/37rzxUbJ+BL9OdfD6nvKA2CkjK+JMRZ4n8VbENg31ir+WCJA427gK7iRNpSWH0JVqyF1dbCaitYsYIVK1ixghUrWLGCFStYsQkrvnp81P4ac4//muhE9JDIL+vKeos5sesjn9MwtbUo9/coj1XCihWsWMGKFaxYwYoVrNgN0zcBUp0t+vvxVswwK5sGcnfB9//877S/YWDxyr/6d5v/+Kf+l+Y/+slPm31sGvtDGmr8hb/wcPMjP/LfmXVLY15YLY91IQf7z65Ng4989J82UsYE1At3Xmg++vRH2rgf/Yf/ffN7v/d7ze/+7u82//Af/Yjv4y/9p9/j687Pz5u/9n95b9YeYJt87196W/M7v/M7/pzU57YYBr/+mV9vnvyhk4MwDX75Vz7ZfOkfdeLPqNtHrM+mgBPvkw2DRBD86+WL18ZBGwf+QnoJnBgYEjgedxFt4oTaUlh9CVbsEFYfGquN5iDja8dH/l/sd/SOrT72MV7HlOi+xmLXx/ZNgJz0PrPqLIbj+2uXx+v1zSiPVcKKFaxYwYoVrFjBihWs2C2gTYD+/lL05/EdZdwwK5kG7/j53/a/W/CXfvZz2aMI3/0zn2n+j//D/9r8xz8dzIK/+BNnzV947tTsYx2UH8AifH/mZ366ZbF4prl5879t/tSf+uZerMXf+Tt/09x/SCw7h55pYF4U7Qbrwg0Oh6mmgQhxKX/zb/2X2X4RQf/6X/9rX/cDf22+UF/GNHj4ke/w+fytD/xXfltyEBMh9YFpAPtAaRys2zAQMA36WJ9Ty5kGgsSuG9sEWIZDMQ56poHgLo5n4YRAJ2Q6emLBpBSzc9n3/nZNOZ/5VI/tyPEvGerv4mC/zzrmtslju/Xrx6X1NdHv13jMBtl2/A6xzYFVWJNpoH+3oH0U4X+QRxH+l+a7furTzV/8yWAW/PnFnebNz75g9rEK9gdvZxqkbTELRESLefD1X/+aLNZCtz1UlpqDu+D5C98ZTYPehdByWBdaNaz2CSseDos5psFnf+OzzSc/+YlM8KRv/KVsyzT4gb/2nkFjANMA9oVkHMj5+JnPfmathoGwjGkw5+/3JkwD/RmyLFa/CSt+edNAI+3WiW0EzCV/ZEFTXvDvDtM0ENzF7yScANCiRtMTCnvFqsJ6bvt9i5+GdVwTU2Lmkc5Lq85iXvwrI1ZdIO9vevy2GBs3P3Y91Ht2EtuK3xQTx7GFv9S595JnWnzOGkwD/7sF/+yz5mMJiVf81f+6efSZTzUPf+xXzT7GsD9QhylNg4SI4b/+19/rX3/f9/0nzUc+8uM+Tv7/7d/+Z/x+2U5IvOyX/8u2mA7vfvf3Z32WiCnx9//+k20faTzJ6UMf+kdtP//Ff/F/a9vIPtmW/4vBIa/lzgjJS/4vMbJP2kmM9J/MD2u89FpIdw3o+Uoeab7SXsb4mf+xuyOjZhok40Vi//7/88l2n2wLkuO3/7k/48TZj2cXVrL9J//0N2f7hpB+rf1w2MwxDUS0S3nk0avtfhFEP/dzP+v3J9NA7kYQg0GKGAo/9dM/2Qol+bb1E5/4pbbuzukd/zrVi4mQTAipe9O3/Um/P5kGyTBIRWLSvtSHNg3kuXJpK0X6vfXkzbWKNoAx5PcN5Jx8+mMfbb7uj99nxizLZTUNBKtvwYrt4dZtOSzxvwq2ETAH2zAQyov9w0TMhU7I9MkEAuyUOcemjN0O5fllxWjmxScDoG4E5P290jM9HgKl+WjFaObEl7GeVwnuvLVwx8lj1RWUot+K0ZTxfdZgGrztf/pM890/8+vhUYT4uwXf+RNnzX/43J3mLbdfaB599lPNI8/8WnPt6V9trn70l80+SuwPz3nUTAMRtSKw5bWI8yS8xQhI4lrQbSUmPdYg/7f61Ug/Mo68Tm0FEduSl+yXbRHwaVv61I8TJIMgjSuCXxsFEpvMCGs8ea3zlH1iFPg6dxHzfX/J9adEv7z++j/+Go+8Lk0DuTj6v8vaubqvczGyLeaAGAE/5vqVfYLskzrZJ3csyOvvdWNJf/IaLhcvL3HnoiVGSpJpIL8hkEwAMQdEiItob5pzbxqkRwfSXQEi2uXxhbT9sz/7P/s+xHgQA0G+fZUidfLjcKlOxhRzQQwJqRu602DINBBTQ+ciddr0ANgk6ZEEOQet3zhYlWVMgxryo4e/8qu/3Pzap361Rd67elv4u//N325e9UecKDX6GEN/htWYGq/jNFasSfwsnocl/FfFNgPWg31hfLhYwspdYBv7ptJe9BtY8RqrjcZqM8ac9jp2E/HLMXys+ozFz+kvxdbidf1QXGJ6bCf+czMgj+v66uI6dP2lwQlyj1VnkeJLrFjBihWs2EgyC1JsMA0S7r2jccfVU+5vqbSbgG0UlKxoGqTfLbDuMEi84j/7r5vv+MgvN//BP/mE2Ydgf2Auz5BpkL65FwGdvlWXWC+UY5xuK0Jc2kl9+qY/1ZVIn1Z9aUoI0mcS+9ImGQKpTpsI/k4AF6OR/trxjIsU+eY/vZbx090AGqmTuwBE4KfY9HhCeXGU4sr9YhCImfAuN0ba99f/r+/1Octr+b+ug8OmZwQoxuJe5s5lS4yUJCNAvqkXo0CEj4h62adNg/SjhFoYSZzcESD7tIEgdfrxhHQHgi6p3TKmQTIJyiJ1KRZgU5S/YVD7ccRVWKdpYLHOOw3S59kQc9vNiTWJn+fTsUT/F7lrpmGsNjmW4N9nugvs/aMUasNoEa2xYi2stoIVO4W57TcdP5+px0Efs6H4Mi6nfi52ffQFfT22Fh/QdbUYzZzYMqeAFSdYsYIVO/SeTTFW3VZwgnpMxGek+BIrVrBiBSs2og2DhCn+3TFt0ftbdLui7QRsk6DPSqaB9bsFjz7za/5RhKOnfyWYBf/0k82/909+qfn2D/+LXnv7g3J1aqaBiOH0eIF80y/f1stt+hKvRb1uK6JX0HcFpLoS6cuqn2Ia1OoEGV9vJ+Sb/ST+S/T+dnxVn/B3QETTQC6EancGpLhyvyBt0h0Ksi13HUi8/L98VAF2iyXmE1Z8woqfy1zT4E3f+idb4S9FvrXXpkHar0WRNg1SP6lemwZiDKS41DaxjGmg46w+ATZF7UcP120c7MI0KD+v1oUeQ2PFbp32M74v+C2TwMJqm1MK833HnRdbwl/EC+7Cu4qKn4e7QC8oReQYq7Xv57NfzM1rLD7Vl1ixghUbSIK3LnyHBfvc+MCUGM302DIfK0YT4rrHkfox/fftUHwad6vMff/q97zGihWsWMGKjVimgZCJf3dMW2S7h44vseL7vMzF5ui6NZkG3/kTp81/uOgeRRCz4NrTv+IfRfjf/9NPNv/+P/lE8+c+/EvN/+6pX2z+nQ8979to0bspStNAxLzc3i+36Kd9ImrTc/0iykvTwN/K715Lu/QogP/GfsA0EMSMSAJf+khIu2Q8yHb5eEJqL5SmgYwr8X/qT4fHC0S8y6ME8lpEuY91r6U+xYhpkF7L/2U7mQOyLYaDvE6mSHrEQO4cSOJfYsQQkNcSkx5PENKjCOn/sk/GkNc6/v/xd/5muw/WjxblVr1Gx9ZYps1U5poGInR+8qd+wt9tkH4UUZsGtccT5A4E2RYDQR5BkN8qKB9PkB9VlKLNAGkvr5d9PEHGljzTeOv69/EBhpDzrPb7GWIcyHtoHb9vgGmwJeLnfqAT+pY5UEO3G6Z/ob+vWBfcG8VdOFex4lfCErA1lm1btktYsbtgbl5j8WV9ybz4MdEb9g+L9n78GNNNgPl0oj68x6wYTf5+HK4LjPWv1yOQ2lh1FnPjl2Du+39uvGC1EVxdenyhxe3vmwF6fw0d36c0Dfr1E02D/AMsR8yCNxe/W9CaBT8e7i74dz/0fPNnf+wXmv/tP/55s49NkEyDRBLy+hEAEeJiHAilaSCx0k4eERBjQdqnbfl/irOQeOlL4qRv+T0C2V/+8KKMn9qUfbamgbqQkG3pT4S59J9Ev/zfj+f2+/GcyE/xss8/5uC2Zb8YDLJP/p/MABH7qb3sl0cLZFvq5LUI/xQn6yRxgvzGgewTk0G2ZWz9GIL0L/vn/AAiTMcS5fvOMqZBMgbSDx9q00C25bcOxBiQIuZCaid10lYMACkSI4JeSqpPhoQU+TFF6Vv2L2saiOmQjAnpN/1GgtQBHDoX/fEEK2Yy6vN7c/TNgRp9c2AZcgGwa7YiEEz02NsavxS0CStWsGI1VhuN1cZiTvzc/nX8WJsyNjFWvxyW4BXG6i3mxm8O+302Xr8qOge9Hv24tL4Wus309+Xc93CK11hxibnxDifQk0Fg7i/oGwHlvinkhkCiMwz68YOmgf2B1UfMAv8owkfTowifaP69ZBY89YvNn/2x571Z8Gf+0cebH/rnnzT7AAProsSgvOiBw8IS1hqrjWDFHgJTTQMA2D82ZRqkzzMxuL/0j7iLQFW3CqnfIebEDqI/v/cGywjYFVoMLId9QT6dvD93AT4Ju6+AFb9fBGFl19XR4nyeAJ82Vtl/worNSfMpsWIFK1awYgUrNqc7f/Yffb6XOZd1+4zOW5PHWe/Jss3093AZZ7FKfMQJck+7r4hN9SVtfMTt6wv9+v4O2zywYpOh0JoG9ofOML/927/d/Mt/+S+b3/qt3/J87nOfa37zN3+z+Y3f+A3PZz/72eYzn/lM8+u//uvNf/f4d5p9HCpyd0CJFTcL42JEXwTBxcAS1JcBTAOAw2UTpoH1mZew4udi9bsR9Of4wWOJ/vXxiqXQF9/zKQVGwopdjuIifg/oC6bNMn+8dOv9NNOg7L9k0/H988eKmUPZ3yp9Wn1ZzIt/lYEVl5gSszx6vvYc9PtxLD5///bZbHw875wI70wAIybVl6T2JUasZQDkzDQN7A+Nafw3f+4Vzd/99nEk7s9+g/xxsPu5tFgXHxHrAgi2j4hda7+mFMdgg2kAcLhs0zSwYudi9TuK/ny+9Niif1VsU2CzlBf0if6F+rK4C/S1sL7+StFkxayTueOtGm8xJ1Yf/3A+WDGa/NyxY3KCiLbr+v0l0W3FBpIoT3GBfj81UlurzqIbK8eKFcbqVyethVU3dDzzuESKL7FiBStWsGIFKzbDiXCPVSe8ShNjfXwFHeMJ7SwToM+waSCsbBpcWKyLiS1gXfjA5rCE7RCrtAVMg13xWmMfwFy2ZRpYcctg9V1Ff/6Doi/69xXLLEhYF/SrYF7gWxfxs1hPf7lYyrHiV8UaJ2HFC1asYMUKVuzq2Md2tdi8PhfR9TghxeZthmNe9YrutRWvydoprFgh1Nu/mWK10/HhPdmPOXTK96gVo7Hiy32jJNOg3We/RzxO1HdmQcfLPbYRMBdMg4R1IbEFrIsd2ByWmIXtgGmQI2JeY8Vo5sSXsQkrdlnm9qvjp7aB/WHTP4Q4FetzVBirr6KvA3aM/m2pfaYU7PuCZRgkygv6Vcku7FvUBbwjF4o1Vu2vn1uHFT9GaFeOPW18q36djI2l60umxiWmxNfrkyhPhP12fBsnJkDFCGhjEil2gnGQtXPvhXy7I4/XJoFFPb7/3uxiO6y4hBW/K7p89Hszj7Gpxev9y2O8R52ot0yDmnEw/S6EjstnGlgXDTtAXwTBdrCELMzkSwew4hOu/mXu/5YYOWTmCmAtmtfB3H51LnPaTWVu3zreYkoMbId9MA2sz9KVKa8RdoQlzveZ/oX+7rHMghJ98b4q9sW8kMThEKv2129fktr321rU208df7XxrBhNGa/bWHUW8+PTIwA2Y/UlA/HaBFBGQJVZ8VrsK14RmRrfox7ff3/W++/HCl18fkw0Xew0lo0PhL8hVpxN936o162PumHQkWI6LGNgiMtlGlgXDDvAuhiCzWGKWJgt+ldlVdOgFJyloLTqV0H3XWLFC1PjoGPKeukY2A27Ng3az9Dycz0xJWZPsUT5vlNeUO8LpUlQEuLsC/m52BfvOiaJ1BId02H1JYT68fYl9fYlebvE3PFXG8+KS1jxU0RlzrLxW2GWCeCYFZ+L9JZkGvSMg1i/Ivp9GfbV+9axebx9bMp4O0azarzGircYjk/vlzrp75ZVZ+BEvW0WDGGbAzUuj2mgLyi2gHXRA8tjCllYHkPUb5pX/tE/tJQI1OIRLifWeQHbJVxg2H+fN032+Wp9vgtj9XuKJcgPhfxC+tCwL+RhXUwX/x3z4odFZU6KndqmjN8opQGQsGIFMzYX4Z3otvZHMtNAGInfMPr9mddZxyiPD1hxiTnxVmzJ3DZl/BS6tqZJUKNnDAwTHl3Q2IaBcDFNA32BMYB1cQLbxRS0MB1DnGcs02YiL5uDG/fL7n2FKQgBpmAJ2aE6WC9f9uUvcxck9t/xdWJ9VmeUn/eJsfoNYAlpWIZOIOQX2tvCumCH1Zkm/jumx7cCOmLFJNq4VmAHBmMFLcqLtlV68Vr8GpTxo0ibAqvfKayzr42jj1N438r+/H2c0LFT4ndBmaNFHm8aBFN41Th940C46KaBvrBQWBcmsB1M0QoBJ6ZHsdoJVuwSmAJ/gKXaSb5/5Iub+1//mkwEAqwTS+jC+rj/tV/l/qa7Cwz3WWv9rV8V67O7R/mZr5kSs0Zs8QvL0wmE/GJ5WzhBFskv3Iew+rEI8fP7L8n7XD7fsD23fYovsWIDa3zmf1VK8W3FlLTx7rzMhLWK0ZRxCbfmJlbsXKx+E0OxZZ2Fjm+x3rsaq826SGud78/fFwl1XNz2eHyfufHLY713+nGzHlUokc/u2fR/A2EvTYNXvGIm0sZAnsGE3SG/lg8V3MXtuvgSAyuuxGq3bsQs+KNf8XIMA9g4ltCF9SLGgdxx8Mov/WLzb/4qWJ/hY5/x5XUApsGhs80L9T5hbOsC3sLuY4gkYuz+xli1v1XbJwHWx4oVrNidoQ0DwYrRWCJao9bPv7ZiEj7WwIqdwJcqXuX+9pl9CzE+b3NPQerHwtW792UW33vP5pTxg/m17ay6+eTnthCPjYGOtZgbH9CfEVb9EOX7px/T/cZBiRP4Y5imwHzuef3rv6bZKe5CZJ1YFzoAFxFLOAFcZobeH7oO9hPrM30Q63qi3LdBvg42xNd6Xg97QzomNebGb5XXVbBiE1Z8yZx4HTslfhLuvZIo+zfHUPEZRqx+P1ptdH2JGS99JmTbQsdslynnro7pY31GWHGr8TpjX9o/ytetznpNg/IDfYtYFyAA+04pbABgc1jvQdg/rM94way3rkV2iH0xDOvFvqifgnXBPRer3yGsPuawan+rtC/bDrFMm1F6YnaEMn6TzB137Tm694IpzOV1pIwZQ7ed0seceCu/jFS/rwzlqeeR030+5O+tdWMaBSUi/q39QmEQWKxuGpQf4FtAX1wA7BpLnAhWrMZqAwCbw3ofaqbEwHYY+9xv66zrkh1jXTjCurEu3KdhXXAH0jG0261Cfyw7biqr9rdq+6VIYteqs8gEssKKFazYTbPWsd153WLVl+j4SPYeiVhxFlZbwYq1mBKv+zVx89pLxvIs63P050P5Xivfh8tiGgHLYJgFieVNA/2hvSXKiwaAXVGKkRqrtAWA9TPlPWnFwHYZ+/xv66zrkz3AunCEdZNffM+hf9FtHT+77TKU41kxc1i1v1XbL4UWvFa9RsdazI3fGO48ybBiplD2k7BiBStW0Z7Dat8Yuo3GirWYEq/7nYybr4kVK1ixFlPjdd+asfqcob8v5ftxWUwTYC6GWZCYbxroD+sNUF4YAOwbpcAAgIuF9b6H7WFdGwhmvXWdsmPKi8WtYV2kl1jtLh3lhfrYsesu7lchjWfVLcOq/a07n1G06LXqNTq2xtz49n1g1VmMxad6i6HYsq6sv8Ck91Rt/yBunTxWnUWKL7FiNXPjVyGMkd6L28I0CxKGWZCYbhqUH9RrRF8sAGwSSyBorDaCFQsAFxfr7wBslt71wdj1h75G2QL2Rd8OKS+8p2L1BRMoxcSu2ff8CiyxbMUJVmzJ3Pj2PWDVWYzFp3qbr49YdYG8j/H4nLnxOyedp1adkJ3LBXNiW9z6mlixFnPjlyd8xtgCf1OYhoFgmAWJcdPA+KC2PuwB9hVLEAAALIP1NwZWJ7vOsK5FhCkxG8K60Nsp5UX0XKw+YQJafKyC1bdgxVpYbS2sthZWW8GKXYKe4DaYGy/MinfzybBiNHns10f6dX2SoJ8q7Dcdvxekc8qqE7LzTmHFClZsD3esMqyY3ZN/3uQ5l2K/o2yXsGL7mIaBYJgFibppoD+cI9YHPcAusS7qAQA2ifW3CFajvdawrkf2AOtCb6dYF9FzsPqEJcgv8Kdh9VPQHqcl22dYfWisNpFqDgYp50EkbohNxZdxCStWyOOCOE/GQV5XosW8xooVrFjBik3Mid0b9DlVQ597Y/FW7AHT/7xx52GkL/rz+D5WmxzTMBAMsyDRmQb6Q7vA+oAH2AXWBTwAwDax/jbB8rTXG9lFz/5QXtztHOsCehk21e+qlHldVna9Ftkx6QRMIMbU4qu4tlWseMGKFazYxNS4hB1fivMSHbtM/FgbK16YGndpsc7PkqlxO8edk45O8NufUzk6vs5800B/YBdYH+4Au8K6eAcA2AXW3ygYx7rWaDEvfraHfcG2RcoL38uMtT6wPcxjEsTL5HiTJMxLrNiSObF9YZ2wYgUrdp+o5annAArrPE1MjdtDrM8uwa63zYLEHMNAME0D64MeYJdYF+0AF56vV1j1sHdYf78gYF1veHoXOttHX5TtBH0RCwFrnWDzWMciMTd+R5TCumRu/D5TziVhxSaseM3ceItV2q4F61xNzIndQ8Y+v8r6PkuYBq4+Mw2sD/ld8zf/1n/pseouMpdt3tbFN8DO2ZVg12aBZkqMoOPWySb7vkBYf+Ogu9boYV7UbJfyomsnlBeylx1rjWDzWMdCMzd+y2ixO8Tc+H1Fz11jxa4ba9zElBjN3PhJlOfq2Plqxe8pQ59d+rNtFpZZkBDTwPpgn4IIWimrCtuPPv0Rj1UnfPKTn/BYdfvOndM7fo10+YG/9h4zVpCSXh/yvAXJ/ad++ifbbX1B/bu/+7vNrSdvZvsA9o4pQnxKjEXZLjFUN5cpY5YxJcu0AY/+ewgB0zDQWBcxW8S6+No61oUsbBfruEzB6msKVl8WVtsSq53GanPBSOJzjLnx+4ye/7bnZY0tTInRzIldirnvAf2+sZgSsyOsz7c5WIbBSqbBz/3cz3rxJ/+36qcyZhocMmIazJmbNg1KRIAf0jqJKfB7v/d7vQvp7/vLb/fz/NPf+m/16gBMLOG6KbY85v3GvoNAHx+YjfU387KyT6aBYF2AbZ3y4hW2j3VchrD6mIPVp8ZqU8NqL1ixe4QWmVMEpBUPu8E6PoIVK1ixwlDd3mK914QpMRti7LPNqq/TPcqwlGnwLU7wSZFvzaXIthU3BUyDDinWfmFuX/uAmAZyJ4q+WBbzQ4wmvQ8uIJbQTMyN32NE9I8J/xQzBat9worXzG1jxWtGY8eOn66HHtbfzMvKPpkGgr642hn6ghO2j3VMhrD6mIPVp8ZqM8Sq7dfAmEDUpFiLufGwG+YeJytemBOrWbbdWpjzfitjN8Dcz7Qy3mZJ00C+RU63zsv/ZVvXi8D9h//oR/ydCPJa9omxIGJRhKSUz/7GZ/0+EcJJSEqRet2fNhWkrry9X/bJt9fyWgSqjJn6kX5XMTRWZUjoS16ydilXmbOUVK/nXRZZA91eiqxfmqvUy9gyfylDj0SsG31RLDmVBoEcn2QkyHGT80BK+ciCzF3yl3lISfthj7GE45bQArdkamwZl7DqyraXgXINJjHl/NAxlwjr7+dlBdOgQnmxuWGsi+5lsfo/OKxjYmG1XQarb8GKncKq7Vdg7vlQxpfMjYeLSXkeaKx4wYq1mBufod+vU99zZZs1M/fzTH8G1ljKNBChl37LQISebOt6EXqy7+FHviPbVwpb+b+IQxGMaVvMBikpTovnJELltaDHlny0qSBjS50IzxS/bZLg1SXVSW5pPQR5rev1vAXpS2/LfGWtUntZQ9mWOlkDWQsdvwzWhe4c0jFJ22ISpO1r7vjIa8lZth93OUuR/bItuUu97E/tYYdYYm8DWGIV9ospx82KqWKdb5cE6+/uZaM1DATjImUXWBdUW8e6yNww+kJ7HVhjHBTWcSmx2q3CJvrcAXPOByvWYm48XFzmnj86vsac2LVR/j3ZMdbnoWa2aSBiXMSc3ifb6dt+QQSuvlsgicVkBGhEHJbCXoo2FZL4Tf2kOH2Xg7xOojmRxLPet01KoZ8o5yHI2khJ23reQq2vhI6XeYuJUMZMxbq4XRaZZ7qzQIwROdbyWo7VJ9wx07Eyx2QiyFwkXtfDBGpiTO+36msxa6AUlACCda5MPlcvONbf5YtAZhBYGBcp68S6SNpLrIvLDaMvsNeJNRZcbKzzIDEnFmATlOegZm78RrA+E7aI9dmZmG0apFvey6KFv4i/JPoFeS370rZGi92EFMs0ENLt7SKytRFRjpmQUu7bFjWhX1sPnWs577IvmbesueyXNZGS6mv9Wxenm0ZyTOJfjpcYJvJacrWK7E/16fWlxBJOFnPjl8QSfQCrYJ1ng+j3xyWg/Pt9ETCNAsG4OFk31sXRxrEuCPcU62J5HVhjbQJrbI3VZhNYY1tYbS2stoIVOwWrL2FqXI1l2wHsCn2+Tzl/rfiNYX2eaObELoH1GSrMNg1EoOq7CoTym3MRrFrAy+vym/VEEod6n5TUvqyXb6JFhKb/p/0y5qHcaWDlJXdwSEnb5bzLvuTOCpm/9CX/EkGKl4tNuaVf4suL0F2Qzg0xeuTcSfsl16E7CfR8LjSWONoQcwSbjgXYFFPOuyzGeg9dUNLf+ovENk2CEuvCaOPoC7sDwLpQvqhY818Va5whrD40VpsaVvsSqx0ATMd6X20M6zMlMSd2SazP0VmmQRJ+Vl26A0Bel6ZBqheRKN+QC8l4SOJQx0qpmQbpDgMZQ5sXMrbsT+3SbxqUfW+TmmkgSK7J9JA5yWspqb6ct/Sl7+aQ9Uy38otJINsSn7b3xTQQJDc5FilfQX67QIocN9kW46P8IcQ0n4OgFDc1gWPFrQFLbAFcFKxzvvr+svYfKOnv/UUhu7tAMC5KNoF1QbRVygu8PcW6SL7IWGuwLFb/U7D6SljxQ1h9JKx4AJjO2PtJ12+Esc8T/ZmzJsrP0lmmgQhbLVw1sl++/ZbXlmkgIl7/2n+KLcWxIKVmGgjSNv0AokZEpwhUKSLKy3bbRtahLCknMTxkDlIkZxHPUlLbct7JFJEiJoOsT5prMhQkXi409800kLykpB85TMic0hokE0XMg9QmzWdvKUXMTCxxBADjtO+j2nuy3H/gpM+Bi8C6TQPrQmdvsS709gzrYvgiY63BMlh9T8HqS2O1GcLqQ7BiAWA6U99POm7rWJ87a2T24wmwO6yLSdgBWrTMwBI/ALAc7Xur9p4s37cHjvWZcKiswzSwLmj2Husi7wJiXUjvK1b+c7H6nYrVn8ZqM8Q6+gCAPnPeUzp261ifPcJY/QQwDbaMdTEIW8ISFiVWO8GKLbCEDQBsDut9aGK9pw8Q6zPlEME0uNhYF9EAAIfM3L9v+m/i1hn77CnrJ4JpsAasizvYIywRsQYsEQMAu8V6r7ZYfx8OCOvz59BoDQPBXYSsinVhs7eUF24XEOviGQAA7L+ZG2Pss0fXTwTTYAWsizrYIyzRMBNLlADAYWC9p02svx97iPU5dEhkhoHgLkI2gXWxsxdYF24DWBedAABwsbD+/m8F63NqAEyDmVgXcrBnWKKgwBIYAHDxsN7/Jtbfkj3D+kw6FLZhFmisC56dYF2oTcC6sAQAgIuH9RmgmRs/C+tzqwKmwUSsCzjYIdZF/wiWoACAy037N8L6O7OHWJ9Ph8A2DQPBuuBZG9aF15opLxIBAODiYn0OCFasYMUujfU5Z3DpTQProgz2EH1xP4IlDAAuA/cZ+2CY7O+H9bdnT7E+z/aZC2MYlFgXYGvCulAEAICLz9jngP6sWCvW51zkUpoG1gUY7Jjy4n0A68IfwEJE9FSs9sJQXaLsq4bVtobVfi5Wv5Bj/Y3pYf3N2gOsz7d9ZV2mgXUhs1Osi641Y10wAgDA5cb6vFgrxefdhTMNrAsr2AHWhfcKWBf7cPGxhPChs6/zKtf+smD9vTGx/s7tGOszcF9Zh2lQXsDsBdaF1gawLhgBYHv88YhVB7ArrM8LYUrMXA7ONLAunGCPsC62Z2Bd1MPlwxK1sD9Yx+yikv2NmvK3TsdsAetzch/BNFgNfQEIANsH0wD2lSmfF2XMMtxz32u/qjkUvhb2m9fN52vgQvHVABHr/DhErL9bg1h/G7eM9fm5a+4X7l+d1+4b922H1wHAznh9gRUzhVXb7zsXfX77zNjnha5flnve8IY3NAAAAAAAAAAAJZgGAAAAAAAAAGCCaQAAAAAAAAAAJpgGAAAAAAAAAGCCaQAAAAAAAAAAJpgGAAAAAAAAAGCCaQAAAAAAAAAAJpgGAAAAAAAAAGCCaQAAAAAAAAAAJpgGAAAAAAAAAGCCaQAAAAAAAAAAJpgGAAAAAAAAAGCCaQAAAAAAAAAAJpgGAAAAAAAAAGCCaQAAAAAAAAAAJpgGAAAAAAAAAGCCaQAAAHBB+MQnPtE88MADZh0AAADAMmAaAAAAXBAwDQAAAGDdYBoAAABcEDANAAAAYN1gGgAAAFwQMA0AAABg3SxtGhydnDW9cnbSHPn6o6ZX3dZ1HC9inS+L5jjVxYrFsYq39o1xdNLoNM5Ojuw4Az/c4tisC/2qfD3Hbs/M/DbIYP6e/jGasz7rIeVQruWU9c/LtnPPz/9+/uvGj2e8h2ocL879+l0x6lqM98dg/AFxfPu8OV/caK5cMeqPbzfnsjw3rnTztfbN4OjmaXPujs+1K1fM+hIf78YLZdHcKNoN5r8FrPzWd24cNTdPz91aP7DS/K7fvtu0KUqOD+gcrze377o1jLXu7G5uXXPjqfZj+P5TB2e3modd/1bcTjm62dxx80zl7NbDzS9/8pOYBgAAALBWVjMNqqI0iMFOyEVxqOJLUei3kyjyG0a9K5PFYWygRfzRycl0cWcaA0N1+2UaDOZvHI/dUDcN5q1/WPudzCecyJV1Xh9zTYN2jWqizOUtUmNx3AmhWe+PCRydJCFt16/KYP9OTJ2ey/wNoRcNAqdaW9HqRbrbdXZybSkhO9c0aPG59E2DlH9v/7a53uU31zSor8nqpoEX9E7Ip76DgZCMA9f/nfPm9Na15oHY/9HNO835qYufKPxT/8EokP7ctjtfHtj18cgIeZ3eeridp8CdBgAAALButmQaxPgkerygOWtOjnQbJbpFiC0WbjuJMVfn2p643dNMg/748xnowxS0e2YajOa/T7lazFx/85zaAvtqGgy+B9bx/hhnp6aBm6MIU9MEEKGe/r54EXjc3D5Nf1/2xDQYyn+b7KNp4A2V0+bWtf6dBaFP/TrWu3nclXlkdyPUkPanzcnD6s4E3/6suaX37RwjTwemAQAAAKybLZkGQVCn7ZoAEv0lMalv7x1EE0H2p3rdxmSygIzfUBu5eMKA/bqJpoE016UU6Vl9MU67viqonPtY/9X8x+YdWS2/cA6UJcX49m2piO5a/gPr3+VQjp/i64K5Oy9DzOI4rpMvlfPJz9/KvzZ+R7a+Zn1327EvxVqU9fquAY8bQESbvX5j7w+XvxN2XXH5tQIv1PXWx4k4qfdiPu7NSxcjyLf7bXF5duI/9X+jWXT3h0/uvxVQIsh9v/m6eDHrzt0bbv1kza64uFN3Psh6diI9CNuu9IV9ujuhLcVYWb36VjyjahrEutMh0V3MtY1N9fn65SJbvk3P8/f9lSK+ahqEMbr2XYxf32xhUkk5pPyu+eMcQvv51Qj9lwZAzCfeDeDvLBCR7x9JCCbCmbrzYJCeweD6ljsNXJbPTTY6uja6yOMDIYeh+lDXMz3c8c0fkcA0AAAAgO2wUdNAFy3Sam2D/jru6mWH+//xIggc2bTEXo/QkSHkSsbEc5hHT4xPMA06AapjOtJcs20V79tLSfsKoTfWf6CSv+D7i2XgWGTbM/LL24c8zGMXAivHas766zHCa51/yDe0KeeWxfj9sX05H2u9zfyHx0/bel5l/ynHJAZ8+3L9rXwy0voVotN17iRb/dEFaecEWD6+CPXUJtbL+kQh7w0Ml4/+1n/oTgBvGEj/sc5vt7FG/yJwZ/QfCELSGwNqfzINrsg6uP/f8H9frvg5BNMgtHOqrc3Pt3HzT+Le51PW+1zs+pC/YRwMmQYp/97vLBj7DdPAzaYV4t4gUOOX+VYxTYPYv5+f6q8Q8vUxUn6yxEEYl48bDOH7lUcNiljfRzQN/D4v/mWUOWLfoQV6/M2AxY2H3TlZCPkB8lzcfIvHCIbrw3bVNFDzystzbv0faD6JaQAAAABrZr0/hOguwEN9ECtawGnBmARR2i73+759vRPiZ4tmsQjiqNuft+sROqoI0XmYY041DZTozJHYos4yBbIx5vTfMbpmfq1C6UTsqvnlubbxVh4jx8psN2YajBwf3ac+F+W1Pmd1/j5PJ4AmmQYTzo8euv9irQWfsxo/rP/045+JTjeWCOCqaeDGD78HoPe7/EVHewMiiPrMjJA+vUDs2tRF/XFz+9zlHg0Bjx8z7ev6z0Tx5P47vMCU+et27T55LCH+fXHird1/Lc1f5edzjkLd3x4fcrUFssSeZvXhlvq8jWfQNEi5duZDIIju8TsNXH1qJ+K/rY/9+m/ii3xKpF1pGqjfW+jaXo/r0wndfE1SnJDy64vifmyfmsGghXh2p4FbGxHZs+40cLncuBXXyH+Tbwj5KuHOhnwt7jR3W5NgrH7ENMjG4U4DAAAA2DzbeTyhEFylACrbhPDQVr8eHlMhjSYIqmmI2Cv6migKg7CLRc/Xt7dKN86UuVb7zzDyNwh9xTmtIT9/CNr64nzQhMBiLTXz1t+PYfYZcvDHJ5xUbr1cGzGlHMe6PnsdaduofWl/OdbY+LJtrXHqP81PiRO/3sX4k4+/iFW9fi4/+Ra6ek74+nx8n39rFOjXqo3LYZKod/NzzY3iclqzaSDzTwZF6ku+9U+PIejXrWkgc+kJ+Sh0JacomrWpkAlkX29N0BDpI6ZByL8wIFIuK5gGQjAOUmqVb/kt08Da146phbBakzbOjk2i2MyhIOTtxld3NaQ+w48fujUzf9Mgmghtmwo+1i2MW5NOpPeF/hAbvdOgHQfTAAAAALbDln7ToBBMpqgKok9ipNoSmGNCtUMJSLN+Hr18vKizhGxNiIX5t7kPxgamz1Uo+i+orWeGPyYxpzXk57vTpRbrA8tzIcde/6KNzt/sszu/2vbu/wvXr4QvjmUN05yL8zX17wTQ8qaBGt86P3X/xvr79bbG93TH3xJE4bZ7ffzd+KKJaueEy8XJo83daeDm191VUNR5uv7b+SxtGrj5K2PA2k4MmwYz7jSo3VVgMWoahHxPs3yT6C7WZ6Zp0BHiwx0NRb20m2QaOAHr16cTupsyDXysNwDU+NGo8X2613f8DyUqMW3tqyGxpUHg95VGRR1vCmjf6LlkEEypxzQAAACA/WJLpkGMdxeQQfR0IifVe50V64N+6guaYeGU42P9EHpf+U/KBfE22mdPpPbnp/Nv2yl8fTvf/vxLhte3T95/QS//Pnn+q+Yn65qL3ioh8cHcevmb2/p4xOOq8svnJ/WL5uTEkUyG7F/rCPNf2jSYNL6Rb63eNfb6who/4n9TwI1nCpq0XkmAOLr3RydCuveHG18GVP3lv1ng1ieK+tTW59jWp31OXMp50DMHQnvfv44v6sdMgyC4h8yHiBeUMv8QN2ga+DGCQeBUWxsjbToBHOrbPnwe4fho0d5/rMDAtx02DVL+XUzRv6+X45PuFkii3K1fGl+E/oAo9795MNU0iAZBWJ8QX/5mQmrbE/eeiaZBmpc7d80fPVTjeREu215Uh7sCzpUQDyJdnvkvc7FJ/QWR7sZzIr67MyDGeSNB8iv7tcV8x1h9MV4ap80nxWEaAAAAwHbYmmnQF3Yhpi2FwMraRvyY7sK8JpxKkjBKpd9nKdZqhFwzEZnaplL04UWiLr0xivlLUTHD6zulf02RfzwWWVlzfuXa+9K2N/r2pWY0jOefHxth6PjE8dt95XlQjCeEk3JG/sPnR74+rl349/66GD1H2Z+NL8dfxIoqRf85aT5acNjvj058uPz1EK7/TrC7/kT0jZkGDhHbXdECP/SRlbZ913+bj2UaOLyYj81T/30hFkVq7G/cNJAco3GQSrs/xntBq+ra+Xfz88I2hvjSilyjzhdLYHd9ZXcWtIJaihPVN9z4rehW8WmOhWlQ/ssJ+eMJU/KLxkEqWfsOL75jSNc+5besaSAEY6AtvfFDfRcx3TAIuBxFuMfW5Z0CnqppIOfSnUan54sS/WP1Xd9SXP/x+GIaAAAAwC5Y2jS4dBSi7eDYZv5e8JYGQBDRlhk0iUNf/10TRe2lXb/WdCiE36Eg+ZeiGoLRU94BsGu84D+NP6CY9gcTw/+TitdG6gszay6YBgAAALBuMA0mE0Rv/xvtQ2GL+YvAL02D+M358uMf+vrvmnDnQHm3weVB/SaBWb/vHHr+GyJ7BELuDrjdHM+6o2ADpJy0KRDvHPB3VxyP1GMaAAAAwJ6BaQAbwfsGRUHwA8C68XcbtHfyj911UD620C/PrUG4937o0BXd71j9KmAaAAAAwLrBNAAAALggYBoAAADAusE0AAAAAAAAAAATTAMAAAAAAAAAMME0AAAAAAAAAAATTAMAAAAAAAAAMME0AAAAAAAAAAATTAMAAAAAAAAAMME0AAAAAAAAAAATTAMAAAAAAAAAMME0AAAAAAAAAAATTAMAAAAAAAAAMME0AAAAAAAAAAATTAMAAAAAAAAAMME0AAAAAAAAAAATTAMAAAAAAAAAMME0AAAAAAAAAAATTAMAAAAAAAAAMME0AAAAAAAAAAATTAMAAAAAAAAAMME0AAAAAAAAAAATTAMAAAAAAAAAMME0ALjAHC/Om7OTI7MOLj7Xb583pyfXmitX7PrLzvXbd/36PMD6HCTXn33JHb+HmwceuGLWAwAAwHrANDh4jptFE4otDsfqYRccnZzFo7JojsfqVxA0mAaXG0yDYTANDhtMAwAAgO2wlGnQCZp+SQLFjDk7aY6kj+MgYxfHqt9s31HTb16IK4lP/VVJ/Zw1J0fd/pXzn4Kez9h8l6mP+KrFcW9/Yqx+mLR+trD1HJ241e3K+gRq/xy4iOJ3/Pid+/orRt0ULqZp4M6N03O3LFeWXpe5HN08bc7d+//alW2Kk6Pmppunm2lzY2TcWn6YBsNsyzQ4unmnuXt6q3l4krh1x/3O3WZx44GtHbd5+a2Lq36e581zzWMPurmaMYGrT6T88jhMAwAAgO2w8p0GNdHjRXdNDEURrOvTriBwgmDUIjnUK/EqO8ZEvAhaF3PiQ23htFT+U4hiuhP9uXGxcr2nv07z6sdIwr1mGoT69YvSOO4q6z8Bf4znGEGbYOw8dvUiBpfNcRXT4OgkCVG7flWW79+dH5gGGYdoGuxmTXMwDQKYBgAAADDE7kyDxcJdCicxetws3MVjJ+4NsatFdOrDtRkSU5KD728gdqn8p+DzjUJfv15XvcdYp1n1qyKPPlh5rYif7ybzDlwk06BmDmAarId9ELhDzDYNrt9uzs+DGbGtNSzZ5prWzAFMg8BuTIPpzDYNrj/bvHR3MWpGAAAAwDR2YhqkOmnrhaF7IcIm6Kd1mQaqj6ro3qBpsBU2Zxr4+beldqfBhkyD9DsMleMbcitzCm20QPbHVpW0DvncdMnnkrXXuUhFPH+lLI7DOpftJyGdDJ3Hrl6EVf08d/M+l6HtmCHTQOp0EREu+72Yj/vy4uZ3rbs4z9u746EEju/DrdEVyT9GnIl4ndF/Hbfe3jS45ta97d231QLh2Anmdhy3PqU4zepd/uW3+Xm9K0UfQ/17Qby4EeYfg85uTf/G37dvO7fvNOjnd2u6aRDr3NnR3CrWLa/vyuKGi3P9hNxKw+F6c9slrOcogjxvH0RwPjdd8lyy9npu1283d2/faG6480/qFzfceeDvyJD2pUh0ed11ddLeEMRDpoEfPyXg2neCOon6G+69l9bIjf1wIWh1eylZH0Ok/h9280prMNK/0ff1Z/X4zzU3eoJ7OL+svqjzJoM7vx+44Y5FDDq75cS7cZ5aXJX2KjdL3Ish0MW44nMo51C/0yC0l3V7cOOmEAAAwEVns6ZBWWJcK8ilsRdfQWzJZs008ONoYRSC62LKGwVJWIb+LPG0TP77w5hoX4OoDwuUC3S/zyo1c2EJoknkS2/dg0GQmSHF+eCP39D54RiKKc8Lv51i4/zlfErnSTIOagK9yoChNVof12hozJppMG19nLhzMdadAFKXTAbBGwgqtjUG0j6X66m7gNemwFD/wwTTQIoXsm6fF9C+r9C/3xbRHvsu60W46jsVxtp7oTuzf6+lnNDx+45utvMvxdEgx90dAXr/WH6JsccTUp7JEMj2G/0FgkGQtZE7F06L+VfbB4ZivCAWURrr/HZaSzEN5HS7da25JueQzz8YB6duXysQ3ZrfcZUSVxON0m/tDgRr/CCcg6j3JkUU8nl9v/0ydxrI6fNcNFrG+i/rZbzbxw+08/IGgG4v29Jexev8yvqyvY+P57ffJ2t99zSsR2WtTSp3BHjBv3iseTCOt+zjCVefeMHn+dxjGAcAAACrsNM7Dbz4O1s0i0UQL93+IMCy4i4uM4FTiMSScvyaSFom/90TRLNLcOAOgKH6GYQFGhhnRVNijCjQpfRMpPb49AW7P34judXOCXNeWryrNen6WNI08KTzvbbOqj5e+Ib5yRLYF8uJQdNgdH2SqLPrM9ya6NhgGki+Kb9jJzRDvumif1XTQPflxbXvS8aTsdzctEAfE+26vRGbC9zx/oMY12Jf2kShndpMwTINRvOLcY4x08Dj+wviOsWF/OWbeztffxeCE5VX/HhO5Lrj4e8kmdi+jTFyDncHnBrr60Sp7HNrclfWxAlF34cTk9euXPM5JNNARK2YCUl05/13iNjumwZxfP3Nvhu/FcUy3/LxATEyTk/6Ajq2X8Y0qPY/ml/cp3HifHp+15tnR/oPpoEcgxTj2rzkcp4rzi3T4OoT7Vipr5V+08D198JdOb8fbk0IAAAAmMdOTANpk4SMft21CSJp8Lb60LBiGhgCLiTaE2XL5L83rPIt9VQq6xbYgmkQCSJX5eHnFrf1a0VoE4txrvh66xzy/VklzlWtSdeHcc5NYaljmEwEOUWHL4JrpoEwvj5J1OX7PS6v+GV/V1Ssb+veP0OCbWOmgRM4vdx8ceuYhKgVk7V3x1eJ2UzgDvSfRLKP96I65rcsVdMg31cT4HNMAzmnpc+0pr7PNM8z9XhA2ybG69epfqx9qjdy7vIpS1zfUdNARPd59k191r/CNA3c+Hey++JTSXcWTDENQn7dWq7RNBjNz8VbMdldAZJfjHVk+U3o38erOxGWpmoauPzUvnWYBrXHIAAAAGCcnZsGmq7NiqZBVfT1+1wm//1hbJ0mrOMYSiD367dnGoQ89FhhbnIeybEaFushtjye/hhb59CYkFdr0vXR5WO2qTF0HguuXoTV0Hk+NOaQadDRrY++oK6Lenfc3TV4+o0Cvy/muT+mgTIIeoRv/fU347PuNBjtP8Zv1DQYyC/FOUZNAydGRZ+XjyfkOBHr1ru7s6DbJ2sojwgMGxNde/3M+7BpcFq/S0EE9KBpEONcP0s9niDthr61l/ns8k6D0fzkTgGZt/zGQNw3506D0f5j/EZNgzB+yn9p08D3f87jCQAAACuyf6aBu4hMAmxZ06AmBq1cl8l/fxhbpwnrOEZYoM2YBq25U+u/w6dRHlN/Diyaxdl4DuZxDp0abTsRne+PqDXZqWngCQK+1sc00yDEyXwzkSBjy/o48aZj+6ZBl8Mc08ALYrP/MdxaD5kGrl4EpFNdlfFL0yBsh/ylfVHvhXua35T+3fw3aRqk/NLjBG1+/W/zh0wD/4iBW/+hRwgS+eMIcb8YDqfx/TfSR/kMvseLf2t822RomWoaeJb5IcQg2qvjx/rhxwfk+ETR7vOV82tNpsFofqVpELa73yQo6sVQyPJT/Vfy3ahpIPm9dDfkJ/1H4R/ym24aSB0/hAgAALAetvtDiFHYBJ20BtOgV4IArPWvxV7at0z+ZezuGFunCetoEtr1SymwN2QaWHeKmGsv47tiHL/e6VE5dnmcnouxBqkPdR7t3jQI1MyBof3NU081zeOPt3N73333NR+499425q1u+5nvivVSnrrZfPzBUO9Ngba9Wzf591JvvqP54dje17//kV5/Zv8ieKVI/w+9uhUFVnzYdmvtBOLzj97f1YtwlvFffW9sf9R89ImYn1fUrtx8Z1vvxWbKX761T/mn9kc3m99M9W5trslxmNv/+x9t/nYb3+U/vu1Em4jmNr/Y/1Mn3fpIfh+K9U5MXrvh8pP/FyKyaho4EZrMiDS+JhgKqojo7glUJ44lNyceMzPB4U2C2NQXs30Zpw2EuAaxxpfUxyzToBunbw7U94fj+6HmXK//yTubf/BqWf+rXlQ//+hru+MrObnjH+rdtjs+v6GOz8Pu+GT1ju8ujn+3PaH/Nr8fMPKT88+Jejl/fkDqT5tbt+T8NfKTGHd+p/z+xv33t/1/RPqX+rb/72/rff/vf3Pz9jY+5C/n8Ph2mN/5hz8cxk/9f/ik+di3vbZ5QOKvPtHll9bPmyYTTQPTjAAAAIBlWdk0gF0yZgqM1R86wTQ46PmtyTSoUTMN5OK9uecej7xe5/bb4vY521vbFmS7FEhDdxqsTjANhh9t2G9qpoEI3FWOxy633x63716gbZmjbD/4Dd+QHadJv2kAAAAAK4NpcOCI5rS+aU+M1R8yfm4rCOp9YPz4nfv6Zb8ts0yDJC7kwlyQ1+vcXlbssL3atogrfZyFTZoG/m6Eyh0Eh4JlGiSxuu7js63tOWL8kLa/x73umWKYBgAAAFsB0+Dgibfou2LfGj9Wf3h4oR0mdLCGQff4i/17Dln9CoLPMg3kQlxug97P7ePmF9Jt3anILcpqO3ssYbS/5beTQFnvtppfui07zS9up/ktO55mE6ZB++jCgRsGgmUadI8JbGL7evPzH/pQfvzlNn19/N+sHksY7W+5bTlnNrPt5vdjxWMNxWMIv/jm1zVvu3/58TSYBgAAANsB0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANLjnHi6Y5Ozky6y4CF31+APvM9dt3m9OTa82VK3Y9XGyuP/uSO/4PNw9cuWLWHzoyvzsnjzQPPnAx53fRefczX2ju3Hq0eehBjh8AwBhrMA2OG6fLfLHF2Vg9WBydnMVVWzTHS9RPBdNgjIt5fmfnjyHojk5Om/OB+g43/xDo5u/EoVF/e7AedsHRTXd8/XFZNDecoCuPy1j9VDANLjeYBrDPYBoAAExnZdNARFmzODbrhLH65TlqWt0Ty0UUv5te39VF9SZJx3jcGPEi+OykOSr2rzq/Ta//uqjNf4xjUfwu/5ooXLleHAOpX0o0uuN/Kso1CFcz5uik8SGxeGNiZYF61NzUnfoykMMBc90dn/PFDbdm9tzG6se4mKaBOz/u3G0WNx7Y2ryObt5p7p7eah7eqjgN8zxvnmtuPODmasYEdH46DtNgPVx94oXmJbe+jzw4fBwCV5snXnipuf3YQ82DWzo/rz7xqZDfQ1PyWxdXmw9+6sXm7vlzzXve+GDzgBkTuPrBTzUvuvwefejBLD9MAwCA6axoGgRRtzi26oSx+mWJYnLDYm1ZIbZWguqt5zBWP8KqonqI1ddv16bBrs7v+Sy91m6BzkfOL6m/Zl58uvk7cb04rl1wdfXLXUiG9nL8bcEe6td/B0MwDXTeYn64PXtlHPi7AfyxWSGn67eb89OBPsbqR1jFNPDzcxf6K81vABG6vv/Zgg/ToATTYLPzwzSwwDQAANgmh2kaHJ00rtv191twkUyDmniu7V8H21y/2aaBVIyaETs6v5fgYpoGY8ijD2fNybV1X/D1TYM3HN30dzQsbmxqLvPZJ9NA7kiwzAFMg/WwG9NgOrNNg+vPNi/dXTSPTRLBu6dmDmAaBHZjGkxntmnw7meaL7y0GDUjAAAuEwd6p0F8jrwidryA6gnC0EYLSK8bVUl5hvZWcQLlqOsza69zkYrFcVu/OA7rULafhHQyIuqGxeLwWoXmtmmQ8vel1z7NKZVuvaeu3xB5H7a4z/KTYsxxfH5DOW32/K6vb+q3+70EK0//aIAug+dBBZfEPpoG3W8qSKl9w78708ALWnnsQtYnJpo/GhH6MOdw7IS4a3vDHT+pX9y45tfp3B3jW24uV9L4N240i9S5K9nY3W5VUvuUwwRWNg2uu2Pgkjmzxf2QaSB1ehpJhE+dX97era8SrN4QWNxoHrhxu7kbOzu7dc2J11hX6//hKaInmQYPu+OWcui39fmlcdz6lII/q/ff5ufHLq93pehjqH8v4nvzF/He9T+Eb5/l1l+X68/a+WVzGLjTQOruuvevX7csLyd6/fo6wZb2i8ngzsOHVR6hfRzbleceU/FlveSmBLeIcFmfB936vBSDzm5Z4v968+xLbp6uvSXYh0yDfPwnVfsk6h9rFndd3yGgefKRIv9n5Ft0XxnKp3UfQ6T+H2lu3Uk5fHq4f6Pvd2fjP9e8xwluLaCl/qUyP2UaZPVFnTcZbj/WPOTW/8UY9OknnXg31tHCmwBtcvadBmIIlPnNudMgtHfr9uj2zBcAgH1mRdNARM2Q6BqrX4F4t4EvvccUgtjKxJwoNCWOpnwzOxTjBZ8a12+nWL8hm0et+E3GQU3AVvHzHFjDofq4RkNjhmXp1w/OT8SgdKzqwzxzcT9ljUcJifRMgzK/2li1+SW642PVb+78nrS+qm9vEKj5lb8lsPRaTzi/TqvC3M1/ULSvQdSLwD4vTAO/T9anLOt6fKBvGvjHE9z6JmHsha0MmfZ5UyHMNYn+8FsAoX0QwjG/mL+I2GtijsihjMZBMB46wyE3CvL5+X0qp6XI8p5Z7+tczm4elikgiLC1TIMpufuYU9uMkLrbcnxiv15AO2GX7hpojYG0z+V65/y0uXWtEy4+RvqfKFQ6gmkg3T8XjY40fhLufltEe8y9rJexbx+HtlZ92d6LeJfrnP69pkr7ZP533fwnmSIKJ9bvynlXmAbeMJDx9Xgxvzxu+PGEqzHPXPCPmwZXn0jj2fPxgn3xWCvm/basRRTG3jRI6yP7rj7RvPCSWx8trP0+Ob/rdxLUTANz/NMkzIOov+v+7iYhn9e7bRHsrn0Ss8vcaRDWNQhe399A/2W9jPesW/8HHwh9egNA6qPw99uL97j2D/j68k6Dsr5s7+MlwWQmuLX+1Itu/Z1A18bGKJU7Arzgd+O/MeW35OMJyZx47j1vbB6KawEAcFlZ0jQIolwu0q1vgMfr14hXW6Fo4ZeLsr5gD2LRXQwPCL66EDPEohZfYXA/966PJU0DTxKRtfXs1w+L4Q5JtZ/TyPz86zKXcMz1eEsLWY1ay3afIXRrY9nzK/D96bhNn98j6xuPZ/YNvptIe0fAjPlPQ50/5gWbq3citqt3+ctmNT79iwlSb1+MTcYyDVo2e6dBVtzaavHqBa2bX5eX5CJ3BzjR5MV0mbOqV3PqxHNYY20aJMOga3+aifcpwnsayaQIOemL6lq9H9stkc9x4CJfxGzVNHDHbujOCB/jLvQnze/6bSd63FpEkeYNAS92U//Xm9vu4l8/UuBjpP+KIKwTTIPs8YQ4fhDtMpY7Vlqgj4l23d6IzU2D8f6DaSDzTzHXm2fj/GeJMss0GMmvjXOIGB79TYOr0p+Icxfn5zfRNJDzx+XQn4/cHSDrUxgAPuewL5gG+vGIcEfBIpoXXqS787u8e6HENg2krzuGAeHG9/vSnQCq3s3vxTvRFDAMjGVMg+zxhOvPdP27/J55cSi/uE/jxLlvnwS+j+0Efm4avDv2rwyAok0wDfTdC9LmpWbxHpfzHHFumQZXP9gaEGn+K/2mge/vbnMmd0IMxQEAXHBWu9PAEC+z6tdIEMlKxPmx47Z+rUjC2hdDcFWFmO/PKnGu6zQNllrjJAJdFsuYBjPm17ULY27PNMj31cYy51fSztcaZ+7aT2Bsfdu1VBcobiK5aeByVRd3S6/1hDlW7zQYqptSP4UdmgZDj1V4QSt3eqhj0GLmrIyApU2DfF/XdoX5e4MjrKE5V7M+mQjuHF3SNBCCcRDOfH9HQDGPQdPA51UaO50B4A0B9U28xcZMAy+qi9x8UY8wWDHZXQEi1PWxVqbBhP59vLoTYGmqpkG+bx2mgdxmHkT8uGkg+4JxEGbu1y4JahHA1fUJfXrTwK2PfQdBEN0+o2VMAy/A7fHDnQVTTIPbzWNK4K7VNPD93/Xzy4t6hMGK0XcFxPySUM9MA19f61+ZBvJ4gssnz30mVdPA5af2rcM0kB8Efc8bXf5WDADAJeBAf9PAwItLd3Hbip8wtghGEVTDwjHEukSz/VUhNia0lNDt+ujyMdvUCKq3LgaH6n2ew2OG5pZpMG1+3f4DvdPA95/n3bGh83tsfdt+1YWMyzM3DabNfxTdr0Ws39kPIV4Y02DFOw2cUCvF+1pMg7HfLBiq9znJN8S2KSAMmQYd0YQoRH7dNAh3DWTjRtG+P6bBaWcQ9Ej5i5iO+0ba902Dof5j/EZNg3p+OqdR00DMALcWcx9PyPqI8efpcQARvOqugjw2MGwaRLxwluM08/EE386NX/vWvhX1+fy2dqfBaH7yrb973z6p5jXnTgOjvmTzpoEbfx13Gvj+eTwBAEDYb9Mgil7XQyFQ+3jtV4of2Xm2aBZnQwIt4NsXpkEQlFbbMK9efCJ0tnvTwBNvpa/EhOZlTiPzS32q+rBUxvoPiuMJ+D5sg6LN28f4Hb052vPr6obz29T5Pba+qV91IeOS7cR9eDxAz99/qzN4HlTI+jWI9RfSNPCiVxau7H9F08Dn5Tp2oiTVh99EiAJ4CdPAt1f9eXw/w7f4j7KKaeBxAljmatwpIEwzDUJcT+SLkDbnV5oGYdvnEEXOFNOg63+KENOMmAaxvj5+aRqE7e43CYp6n2eY37T+N2wauPzkUYcuP4np8tNrOWQaSN3QDyFqE8DfieD7t4+V76v9DQERzaq9ET/JNPCExxbk2FiC3TQNomg/f642/ohp4MbMRLsIfpm/fNO/DtMg1t91+dliuTQNwvZ5+2OG4VECqfc/XCiGgtxZ0da7/j8l/Xe/aVCyUdNA8vuC5BfNgCj8+SFEAIDVOEzToN2viil8+uI2EQSjKhXhlMdpgRnmlpXUh2+0L6ZBIIT1x67tl/k9ffOppnn88TA3KTff0fzwvffG+uPm+adUvcvhfffd13ygrX9D81a3/cx3qfZP3Ww+/mBeX8aH7bi2un8pur07Bz6X6mX+lXWozk8qyvOqxybP74Hzp+1XXci4fDNxr98DA/Mfpey3JNZv1zQI7fqlNAj21TQQonGQihMdraieaBpkK+AEjjWWNxNiiKzP9v/1hMDcf3LRmwRPfTi8f6Pg/Bv33+/f/yl/+Xvwsf/ova46zvDDJ83HH3q1r/frlt7/8gOHt9x5evOdzT94daq/05y//9Hmb8dt6e+7498XvV3rvxYftq960f78o691/cd6EfZqfDmHPvrEh5rzND8pJ3l+dwfyl3PzNz7k6n/gBxq3gM3DNxaV/l19rf8J85c17m+H+fnjIzmq9fnYt94fxFnKT+rd+evzi6ZJ6k+omgZy58DQP7mYjAJfnmseeyz1H+KD4RCrpYihkPUlwtjNIVb7omKmmwYB2xyo75fxP/LBHwvnaFq/W9/f/OevlfULov0X3vz65u33x/UX0+CJd8Z6t+3m/9kfi+vr5v3IY7ebl3S9Y+j49ft3wt63f21sf7X5p738/kpb7+9saI//afPkk8+580/yj+1dfp/p5efqXzfSf6z3psH739J8j5tPP/+x7avNBz/1YnP3wz8exr97N/T/47eaj/3br2selPirH8zye9TlJ6bJZNPANCMAAC43F+fxBJNgGuxu/DUwJgaXFYuR0LwvquXDubnnHo+83vftt8Xtch61+U1j38/vNRBNgf0yDS470bTQjydsijWZBjVqpoG8Z8/j+1feu2yvb/vtcfvuFrclB9n2gk1RNQ0uCDXTQATu3PV8KW5/z6a2v+iLwvbXfu2l25bzU7bf+A3fkB2nSb9pAAAAnhVNgyDKnCow64Sx+k3ix15BUO8Dm15faV+KavmgFeSDVziUbbkw0PMQrPnNYdPrv2vKf7px7fXyzyhIvWk6gM32TAO5QyA89mBfNI/Vj2GZBkksjYlftpfbnitW17Ut4rQ8Xy+jaZDE6jLrJ9trNwvS9hrE9yFvf6/bLs9PTAMAgOmsbBq0jwC4Youzsfr144VcGPBgDQP/WIMv9i30Y/VTsUS1fNDKbayb2S4ea5BSbD//yGrjaVY1Dfbx/F4H2fljCPqjk9N4a69d3+HmHwLd/J04NOrFN6jXQ5/Nmwb+9n5/XMJjEuU4Y/VTsUyD9H5Nfa57W77lXX77evPzH/pQ99iElA+rxyhcyR5LGO1v37bj/PRt4zI/tf38m1/bvC3d1h7byxpP3dZcRtNg7nrN277efPzHPpTf9l8cv198i3osYbS/fdt+d/Nz/zjOr33sIH8M4Rf//NetND8NpgEAwHTWYBoAAAAAAAAAwEUE0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMAE0+CSc7xomrOTI7PuInDR57fvHN8+d+t/rbli1MHF5/rtu82pHP8rdj1sluvPyvo/3DzA+h8k1595sblz65HmwQeumPUAAADbYg2mwXHjdJkvtjgbqweLo5OzuGqL5niJ+qlgGoxxMc/v7PwxBMXRyWlzPlA/FUyDyw2mwW7BNDhsMA0AAGBfWNk0EFHWLI7NOmGsfjmOmlbztGU18byvbHp9VxfVmyQd5/Fj60Xw2UlzVOxfdX6bXv91UZv/GMeLc59/TdSP1Y9xMU2Do+bm6blblitbm9fRzdPm3B3fa1e2KR7CPN1Mmxtu3KG51vLbb9PAze+OzO+55oYTZRfrHA1syzQ4unmnuXt6q3l40jpedet+t1nceHBrZsbVJ14I+T34wBaP89XmiRde8ufXYyPjSn4vufweKeIwDQAAYF9Y0TQIom5xbNUJY/XL0u/Xi7c9Mw6WFXIZQfXW+xirHyE034xpsPr8d20a7Or8ns/Sa+0WSMTe0PkVxKBRN4FVTAN/t8MKY4+xvBDHNCi5jKaBFspW/aqso39MgwCmAQAAwGpcGNPgDUcnjdu1FwIusU+mQU081/avg7XMfyKzTQOpGDUjMA2mmgZyR4JlDmAarIfdmAbTmW0aXL/d3D1fHPQ3/PtkGoR17psDmAaB3ZgG05ltGlx/pnnx7qJ57KEHD/b9AwAAh8WFNQ28iJLbxr04DCUXj6GPrigBKW1c29R0cZxiz5qTo67t4rh7nl1KNrZZUvsZSBLuYnxI1A2LxZhjJSY0t02DNH9feu3r67eO+ed92OI+y0+KMcfx+Q3lZJxns+qHqa+vdX718/SPDugyeB5UcEmI2Ku2i/V14e5ylDQqMUOmgdTpkkR493sKZXFrcK27eJb2XVz4NjzVeRErj1VI/jHI5+Fy9HVhV1FC/+MX4ck0uOaOU+qp3zbL7+xWT1AP5d+vd6XoY6j/MP8b+fxvGeK9gm/fdm7faXB9JL8QU7/TQOrc2dHcqqx5qE+lbzDk9S7ihhNcahxfrwKeU/Uicrs6+06DWnsvkPXAbXFzebgTfXl7PYbc4SCi+YZ776Q17NpO7T9wvbntgs/d2lvmwpBpIHVtfln74fzM9lJiH3l+Fsk0eLg5OXV9hMa+f53n9Wdf6tbB912Or+qNb/PzelekDxVT9q8Fuxfx7v3z4I3bzUsx6GzGN/6+/UBuQi+/Tz85604Dqbt7/unmyUe2Z74AAMDlZUXTQETNkOgaq1+WvljzAkyJn1Z0pn3eVEi5RMErpkIWH8Wp70yaHrX9JOMgCNDY3u832us+hwTZFLK8DYbqo5FSE82CTNWq90ug1idf35H10/tWnX9IpGcalPnVxqrNLxHylq6s+s2d35PWV/XtDQI1v/K3BpZe6wnn1+m5q1diPa+TYW1TQBBha9VPuYtgKEbq9Df9XkD72JBnawykfUc323m0ayYxqs10gmkgZXEj9FeO77dFtMfcQ30nqmXs2738u/qyfZnrlP69oEv7jPlP4vpt10/fNPCGgR9fjWespQjnoccTUp6l4G/XJ+7zAryc32m3XeJNAakfE3mVOx70N+dZvGIoRupuH3dzSvmH2CDKvWEShXheP96/xx3TO051nt2q30lQMw284HfH74HYt98eyi+r77fXuY6fX8E0kNPzuceC4PUC2vefxnPbi8dasVzWX30irG+aV1sfRXfZvrzToNZ/Eu2t6E/7rj7RvPDSaXNrrkC//mzzktwRUJoBRn7LPJ7g27lz4LnHHmoenJMXAADATJY0DUQsSbG/AR6vX5VOtLfFXbBqwdQXsSEnLw69UCpzU/VKqHZirG8a5EKzLyCXFnI90nxr69mvHxbDHbao7s8lE5dj6xf3rWX+6li0+3QucV9tLHt+Bb4/HRfmUl/vsfoxRta3Pb/UhaKbiIgyP78Z85+GOn/MC09X70VyV5/uBshyNBAxWzUNZA6WGRGZYiy0HDtx62NDf15Uuny7b++Pm9tOnSaR38aoNtMJpkH2eEI2vox1mgv0MdEu7U9j+1GDY7x/H5+J/evd/GeJHsM0GM0vxjnGTAOP70/E70CciPu0Pm47zM+J2sp6iogN9bkI6zFkGkj73jf7RcyQqNfE/LUoz4ySrD7sG+o/5Nfd/VDWJ2zT4Hrz7F13/ui5uWNwx+0L8+3ya9s58dvml8WGep3r4Hp7jMcTRFz7/qVPl58T6JJfW+9EexrTFO2pfRL4PrbrPzcNUv9q/MIUCKaBFvvSxuUcTY5s7CEs08AwIJY1DTy+v7vN2ZPT74QAAACYy2p3GhjiZVb90iRRZdUFvIhS3+RmWEJU96nqOzEW6odNg3zfWkTzUmsc8pMytEaCKap9n1aJ44ytX9y3lvlbY/n88n21scz5lbTztcbZwPk9tr7tWqoLQDcREWWdaeByVRevS6/1hDkmgdjtd/mlb9qXNA2EZDz44nIvzYFB08DnFdum4mNDPl5Uyp0YAxf4NaE7zohpEEVwvyiRa8Wc6bsCtOFR5Dqh/zD/7k6ApamaBvm+2lrOMQ3k/deK96H1ie38mCmkqAv16hEEqbcEVcU0EJIwT+1L8T4k6iV/uQsgK20fq5oGob30vpRpYOXmSzJJppgGsmadwF2raeANgnp+vo0VI+vbmgYuPyXAM9PA11f616aBe/+sLMKrpsHtbN86TAP/g4v8xgEAAGyIi/ObBgVeRM0yDZToV/WdGAtjVk0DQ3ytTTQP9TFU73NKORv1jtDcMg0GhOTY+sV9a5t/OdaMtTbnp/H9F8eyZUPn99j6tv2qC0WXp4gyP791nmu6X4tYPyTcl3k8IcfNVzoqRH7dNAh3DWT9Wnca7NQ0cMen8i14m7/+Zn3OnQaj/af5b9I0GMgvxTlGTQMv2t2hz8RveE4/W58oqu1jFY6Hv12+Vi//SoJVP2AadESRXrSvi/qUvxLrmSnQifJyfrqvQVNC8OK9GKegbhrkdwrkdPl1+W/xTgNvCIT+7XmFb/39vNPapPatKSDt82/yc9Mgry/ZvGngxl/HnQb+BxF5PAEAADbPfpsGXhx5SVcI1PF+vYiqmQZR4Op6rx3dRa8XT35jnmkQmhTjhU4HxOEEpI+Ul8VYfZprJSY0L0V1mN/S65dY1/x7xz+M3+btY/yO3hzt+XV1w/mNnWfj56HN2PqmftWFoktWRFmYn5u/F+vd/P33ZoPnQYWsX4NYX39EIOQiY1sx00yDECfrkYlLEdK9uxyE0jQI2yGHEDvFNND9jwsdTRCpVdMg1gfRrtslStMg5Z++LS/qfZ6yxql+rP80/w2ZBiKKdX4+RueX4oZNA/8cv3v/9R8xKE2DsG31n/B9FaJ+Uv0k06DS3rdN387r+NI0CNvlbwaMmQb1/jVl33m9/XhCGN/PxxSjXX5tO20auDGf1fOTunj+rsU0cPVP+Py6Z/5zStMgbPs18KK7q/ftvXCP+fl6+acQh/rfsGkQ8ztNjxOk/PghRAAA2GMuqWkgROGbihZOXk2OmwZZqYzlu2rLEgJaOtC5lYzVR0JYXzzX9sscn775VNM8/njM3ZWb72h++N57Y/1x8/xTqt7l8L777ms+0Na/oXmr237mu1T7p242H38wry/jw3ZcX92/FN3enRufS/Uy/8o6VOcnFb3zqmTsPBs/D+sY51Cbf+pXXSi6fDNx3743XBmY/yhlvyWxvm4aBOb+k4veJCjPn/sr548L9UWO/0Ov9n35uxDa9u59deLmL+fnq+8N9SKa3/9o8wG3rfuz+48DqP51fH87iPbnH72/qxdhr8aXY/jRJz4U8kv9n7wzzy/lf37a3Lrl1tm1/wevjuMf3Wx+80Ox3ompazeK+kr/qT7N/2+3+QzNp9yOpsRTH877//BJtz5WfvL/qabBiFj3+afjLsaCrI/qPxgOqjhBqMf29TpA6lvhJaLYzS9WdaUT6FZ7U5RncV17/8273h/zD310onzQNBjoX8ekuNoPHlr7JYePuPPHn4NpgJPvb/7G/fe7/oOo/4VHX9u8zb0n/XhiDNx8Z6x32+74/4Ycf2nv3rsPu+Pv618b6x3f7c4nOaf627H/N78u6/8l1/4/b9tfbT7ywX5+qV5+CPGunJ++Pr1/VPurTzSfTfnJukp+T8zp/4Xmpfe/uXl7Gz80n3JbTImXmvMPp/xi/x++1Xzs217bPCDxkt+Pdfk98tjt5qU7M+404J9cBACALXOgjyfsmi3Oa0wMLisWI6F5X1TLxU9zzz0eeb3v22+L2+U8avObxiU4v6MpsKppUKNmGsgxO4/HT44d24e9Lci2F0SKIGYN0wC2Qs00EIF7Nx6/t8fjyfbutuX985J7/dA3fEN2nCb9pgEAAMAWWNE0CKLMqSazThirP0y2JxY3vb7SvhTVciEjyIWNcCjbcuGl5yFY85vDptd/15T/dOPc+jEs02AVccr2/m6L+NHHXsA02C2WabBOscv2+ra/170u/85iGgAAwL6wsmmgb/O3xdlY/SGyedPAPxbhi30L/Vj9VCxRLRcycpvyZraLxxqkFNvPP7LaeJpVTYOLen5n548h6Lp/2cCun4plGsw9ntvdPm5+Id12n24rTudn3M4eS1DtD2P7+oT5vXal8TSYBrvFMg2m32a/zPb15ud/rLjtv7hN/xf1Ywmj/e3b9vXm42Pze8vrm7evMD8NpgEAAOwLazANAAAAAAAAAOAigmkAAAAAAAAAACaYBgAAAAAAAABggmkAAAAAAAAAACaYBgAAAAAAAABggmkAAAAAAAAAACaYBgAAAAAAAABggmkAAAAAAAAAACaYBgAAAAAAAABggmkAAAAAAAAAACaYBgAAAAAAAABggmkAAAAAAAAAACaYBgAAAAAAAABggmkAAAAAAAAAACaYBgAAAAAAAABggmkAAAAAAAAAACaYBgAAAAAAAABggmkAAAAAAAAAACaYBgAAAAAAAABggmkAAAAAAAAAACaYBgAAAAAAAABggmkAK3G8aJqzkyOz7iKw6fld9PXbd45vn7v1v9ZcuWLXAwAAAABcdtZgGhw3Tvf4YoufsXrYBEcnZ3HVF83xEvVTwTRYDUyD3YJpAAAAAAAwzMqmgYieZnFs1glj9ctx1LSaN5Wzk+bIjL3cbPr47LfoTefJuDHiTRTjHNr0/La1frX5rczRSXN6LmscihfgVtyesrRpcHy7OXfzXty40lzRr9dVDwAAAACwJ6xoGgRRtji26oSx+mUJ/XZiK4rDtZsTF4CgSuticax+hE2K3tWFLqZBYjOmgVvf0yi6zfphjk5Om3OX07UNfct/dDP1f8WsF5Y2DY5uerOkE/1nzck1Jfp9/XlWf2tOPQAAAADAnnBBTINNiaILwJpMg5q43aTo3eYxnW0aSMWMRzt2sX6azazlcbOIYtmuH2avTQMv5BfNjVpbL/qjUaBfr6seAAAAAGBPuCCmQfjdhFJ8eV2XSkUQ6tLlmfLufo9BSj6PENMVLSCt9k4UHOn2Q+Mb9csKvqBK623H6tNvUlRiQnNb9A7nX18/L3DN0l/DGnkftrjP8pNizHF8fmM5rbJ+59IylKz91PNLtZdSycHGjaGfO5A17Il7N34Uu/n+YbxZEHvNS96XCPo2zuXemgvpkYjFjU7sx1v8xQC4NtK/FuZDdxqE8fttAAAAAAAuEyuaBiJa+mJlev2ylIJTxIJhGKjHFfy2Ek3D37x2/SchH0RoEp+xXvVv1qu5zxt/PP/JOIF1NnQMhup9nQxri1pB8rLqh/MfWz+1b5k5a0IiPdOgzK82Vm1+iZC3dGXUr2H9klj1BkC5fuq45fVxW7Wft5auf3/vvW4vQjwaBy45W5RL/XQDYehOAy/YlSngt3VsNA7C7wAcN7fP++J/HY8n+D5kZvzeAAAAAABcUpY0DdI3nH0xNq1+VYJoCmJLv071hllRiOMg9mpiOvSZC0HVp++rnFuYc2hjtA/qsBVtw+OP5z+PkE/9ePTrB8Wwwha9I/mPrl9gntCtENR3PpaxlrWx7PkV+P7yuI2uX3t+KTHsOhKB7POfMT8T1/703K1ZJqRdTt5H0AJc9rlxZt5pkKibBmICFP36nPJ9qf2JP8TqroNUv67fNPCPD8jyjcQBAAAAAFxAVrvTwBAns+qXJoimVmwF5dUJIj+uVfJckrDzJRNUhujXotYSolkbo32Zo6M6/sT8J7HUMQr5S1lK9I7lP7p+gVlCt4Y1ls8v31cby5xfSTvf1OeG169dKyWGXUe5aeByUQJ31lpKX0V7P6ZTzlsxDVz+/vGDXinHCjnJulu/PbBu06A2DgAAAADAReaC/KZBMY4XTaUQHiK0dx1k21neuk9LiGpTwWof1GFFtBXjz85/gMFxHUP1Pg+9zn1Cc0v0DuQ/un6BzZoGeX5Lmwa+/+JYJza1fu35pQSs6yg3DabNz0T6kjXLhLQYBMWYGzUNxvv17d175oZLzOpnLaZB/K0EHk8AAAAAgMvKfpsGUXS5HgqBGfrVYisXRYUIn4DXfgOmQV4fBK7u39cX42fzloC2vo81vu5/aUbGHa1Pc63EhOalKB7Lf2z9ImHnauaJ78M2KNq8fYzf0ZujPb+ubjy/1dbPFqrp/KqYBjKmE7p6fv6L8koOfUJ7PX76zYRcmEvc8qaBF+Syfr32bn7y1b7xyEFL9q8b2L9pEGJC/zXBP2QaSF3KD8MAAAAAAC4rF8Y0SLHdvhCTFSWaguBTJRNURtueAI5iMBWj/ZBpMDy+MJz/ZIpxZ9dHQlhfPNf2S/5P33yqaR5/PCbvys13ND98772x/rh5/ilV73J43333NR9o69/QvNVtP/Ndqv1TN5uPP5jXl/FhO66d7l+Kbu/Ol8+lepl/ZR2q85OK3nlZZ73rF+b3/CP3d/N3HZ3r9bXml63/0PrJ9nHzC+Xxub+Mf6Q5ffwdrWky3J+97Y+vdzRckePz0KujQD9qPtqb/zubH371vc2V+O3/5975rW1//q4Dl+/pv53a6/7TAH0DoGoaZKZEUQcAAAAAcIk40McTNs2h5m0QVOnKpkGNmugVwdbcc49HXu/79tvidjmP2vzWxUVZP70taynb53u0Lch2ecfApN80AAAAAAC4xKxoGgTR0/8Wfnr9fnJxTINNHx9L9IpQE0S4CYeyLcJSz0Ow5rdOLtL6pe1VxP0mt9/uXut1FjANAAAAAACGWdk00Lfp2+JqrH4fOXzTwP/Ggy/2LfRj9VOxRK8Itbm3qU/fLh5rkFJsP//IauNpdmEazM133vaU9VOPPYz21+8/e6xBStn/o6v0v/q2BtMAAAAAAGCYNZgGAAAAAAAAAHARwTQAAAAAAAAAABNMAwAAAAAAAAAwwTQAAAAAAAAAABNMAwAAAAAAAAAwwTQAAAAAAAAAABNMAwAAAAAAAAAwwTQAAAAAAAAAABNMAwAAAAAAAAAwwTQAAAAAAAAAABNMAwAAAAAAAAAwwTQAAAAAAAAAABNMAwAAAAAAAAAwwTQAAAAAAAAAABNMAwAAAAAAAAAwwTQAAAAAAAAAABNMAwAAAAAAAAAwwTQAAAAAAAAAABNMAwAAAAAAAAAwwTQAAAAAAAAAABNMAwAAAAAAAAAwwTSAjXK8aJqzkyOzDvaf48W5O37XmitGHQAAAAAAXHwwDWCjYBocNpgGAAAAAACXmyVNg6Pm5KwpyqI5rsadNSdH1n5Vzk6ao6ytjivaixJ1ZXFc33fUT7Adw6yLJQncofbT8z9g9Hpa6z2RbZkG/nhNPgbh+C0zn2WZl9+6SOepe29eseo7Un7XirilTYPj2825P2euNFf0aysWAAAAAAD2lpVMAy26gq4sjIOjk+bMCZGTnnAM7bt9UdwsjlWMo9Y+ilgdn3Zlor/sz8C3M+KG20/Mf0V2IzQjsvZ+Su61X6TS+JmGNM2P/WaYt1b983fT7OZYxvNyF6aBO39Oz2WNk2ngzp9rV+xYAAAAAADYW9ZmGmQiM+4TIeIFY1COSjCVotsWVdX2sr1YOCmUTIrjZuHqtbng+9uaaWDnvyqb6HMy/nhGo0C/tmIdNXOgtn/dzFury2IaTCflN9k0cAf2fMiM8KZBNAr0aysWAAAAAAD2lg2aBiqmJzpL0e1Ev2ubi8t6ey9wnKD33oHUuxfSVgvUFNP1ZyNtrLjh9lPyH8ePrUpaOz+2Wfpr2BV9l0dau5BXKt2xWSdxjIog1sfEqmtL1t7KX8/daC9ltmmg13Ckf6PvfPziLptevStFH0P9p/PviguSW/tDyPTzKz+HbHEvhkBWXA5z7jQI7d26YQYAAAAAAFxY1vt4ghY+XugnIRXiO9GjxVooPUE00L4V9DKo+//xIgg+2cxiymKYAD5vY/9w+wn5j+D7N4Soph4Tx1d5h3zz9ZKSGxF9YbsS0Sgamrs+JuV+nb/fbuea8u+EfF7fbz9lPTv66zPWf1kv4w2d/2P5Tenfl7SvZ7xNxHVs3RHgBb8bP5kBKb+5jyccnZy2v1dg1QMAAAAAwGGzvh9CVIJH8CKkKppC+yAm9etp7bu642ZxtmgWi3J/v32NUrwlhtuP5z+G739EBPqYYl09maGSCN/KByEbctKiNtQvITorhPzLMfrI+vbXxsglE8VG/qGjsBaGgK6ulclI/6P5GczKb7z/sL5a7Ls2XufPFOcur55pMJDfUr9p4PqT3y9Y6rcPAAAAAABgr1nf4wlGfSYWRVSJeLHqM8Fm1KeY2D6Ehzr92gufrZsGbruX/zSCMIzFaJ+EXK9ftRbdfn1M9OuECNVy37KE/qWM9aePT4sXrVZJQtbIX6+xb5/Pv7pWJlP6t4oS2lbM1Pwm9J/Ov5VFuJuXbRrk+1J+q5gGLmHXJ3ccAAAAAABcJDZjGlRFUWoT2ndisuhvpL0pRB1JaJWvh5C+rLjh9iP5zya0L8dLQq4nhEPShWmgTQEjH7+m+bfLKxOPk3UsEuaxGs3FyD90pER33r66Vibz+88Ja53Na05+o/1359/mTAM7v9mmge9fUsUsAAAAAAC4iGzENKgJONFVIoRSey26dJux9kGf9YVqr4+q6O/ocsr3D7cfzj+PnYaZh99picsgWnV8CE3j949PbZ6rE3OpzF3G7R+rkN/Y+lZFfSna/eT8jonrP9b/WH6laVCuwVh+Y/13599GTAPJzz9O0OXnbxRw+fFDiAAAAAAAoNmIaRD0VykUHV48yTfkoX0W47/9DPvG2v9opb5nGpSlFW0docu+eBtuP5y/7qeGH1cXI7d+nDYQolBNJWsfRakuAwJ1HdSOWW2/5Pj0zaea5vHHY4Ku3HxH88P33uvrJP/nH7mv+YDfdvHSUVvvtt16f+6p2F7mXtY73nqfap9tT+h/ML94fqTx5bjIv/c5K78J/b//kUr+Y9vx+Lf5xfLUzebjD8Z4Kz/3/8mmgYu3fmARAAAAAAAuFkuaBrDfBNFYM3W2SdCifdNABG5zzz0eec32brffFrdLc2DSbxoAAAAAAMCFBdPgQrLfpsGQeGV7d9tiHOjjJGAaAAAAAABcbjANLiT7bRqIUF3utvsp28fN873b8vPt7LGE0f72bXvK/O5faTwNpgEAAAAAwOUG0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANYKPU/vUEuBwc3+Y3EQAAAAAADhlMA9gomAaXG0wDAAAAAIDDZknTIPw6f14WzXE17qw5ObL2q3J20hxlbXVc0V6UqCvZvw5Q7DvqJ9iOYdbFkgTuUPvp+R8wej2t9Z7IxTQNwvHf5r9O4c/HrZ9jbp6n53Lkm+MrVn3H0clpc+7yu1bELW0aHN9uzt3QixtXmiv6tRULAAAAAAAbYyXTQIumoCsL4+DopDlzQuKkJxxD+25fFOGLYxXjqLWPIlbHp12Z6C/7M/DtjLjh9hPzX5HdCMWIrL2fknvtF6k0fqYhTfNjP51Nz3/5/vvn/6bZzbng5rkr08CdfzJ0Zxq48+/aFTsWAAAAAAA2xtpMg0xkxn0idLxgDMpRCZ5SdNuiqNpethcLJ2WSSXHcLFy9Nhd8f1szDez8V2UTfU7GH89oFOjXVqyjZg7U9k9h0/Nfvv/LYhpMZ7ZpIEaANyMqRoA3DaJRoF9bsQAAAAAAsDE2aBqomJ7oLEW3E/3yXXYmLuvtvYBygt57B1LvXkhbLVBTTNefjbSx4obbT8l/HD+2Kmnt/Nhm6a9hV/RdHmntQl6pdMdmncQxKoJWHxOrTpd58y/b53e5tMdPBWXnhlny/uuk9dXHoN82y89Yn6H8Q718y69K0UdWX9Sl+V9xg6SoOeenNwFiO5+bcaeBld+cOw2kzjXCDAAAAAAA2GPW+3iCFi5e6CchFOI70aLFVig9QTPQPhOE7v/HiyDYZHNQGBomgM/b2D/cfkL+I/j+DSGpqcfE8VXeId98vaTkQrwvTFciGkVDc9fHRLPa/EPd0PkX5qv29YyraTnY9Ne3HL88r6z8hvJP7ZPYLnP1gl3V++2if1/SPmP+k3CJhDsC8v3l+Ms+npDMicUxv1cAAAAAALCPrO+HEJVgEbxoKUVtGxPaBzGpX09r39XJYwmLZrEo9/fb1yjFXWK4/Xj+Y/j+R0ScjynW1ZMZKonwjX8QoiEnLUpD/RKisULIvxyjj6xv1TRYdv4WYaA2NvSv10ivj4qZ2n+Gsb7Z+MZaj4l23X7U4HD9y+36A/23829FvLSRnGd+q+/y6pkGbqzycYGVftPA9yfTW+K3DwAAAAAAYKOs7/EEoz4TiyKKWhFX1BeCb6x9CA91+rUXSls3Ddx2L/9pBGEXi9He11v9qrXo9utjol8n+qJ5eUL/Usb608enZOn5C14kF0XFDh+/wGD/gxjrq88BKzdflNAfyt/X5UI9y3VC/2n+K4twNy/bNCjzW900cAk3N2q/cQAAAAAAADthM6ZBVdSkNqF9JyaL/kba14SoFopTRKMQ9PeKpsHYeowS2pfjZUJRE5Ked6eBX9Pi2+lVicepZgoItWOVM3P+ca5Zv2GgNnbK8a/3P4axvnr80bUeyd9on+U64Vim+W/ONHDjr+NOA/+DiJIqjycAAAAAAOwjGzENamJMdFEQcqG9Fk26zVj7oK/6QrTXx4hoFLqc8v3D7Yfzz2OnYebhd1riMIhOHR9C0/j941Ob5+rEXCpzl3GtY1WyzPy7fvs5TDr+1f7HGDENYn19/LH8i3rXt/8i3ui/JrTT/DdiGkh++nEClR8/hAgAAAAAcLHYiGkQ9JMhFKVCBEhsn8Wob63H2v9opV4Ld/+6LK3o6ghd9sXdcPvh/HU/Nfy4uhi59eO0wI1CM5WsfRSVuowJ6BWpHbOh/c1TTzXN44+H/Fz+77vvvuYD997bxrzVbT/zXbFeylM3m48/GOr98Wnbu3WRf2/z5juaH47tff37H+n1N7X/WnzYDuv7/COqXiakxpeYp2+q+Ukp8xvIX86nz6V6ObbL9D9h/tX5yfMC+vhI0evj8vtNld81l99a/8lFAAAAAADYC5Y0DWC/CaK2Zupsk5ppIAK1uecej7xm+7C33xa3S3Ng0m8aAAAAAADA3oJpcCHZb9NgSHyyfbjbYhzo4yxgGgAAAAAAHDaYBheS/TYNRGgud9v8NraPm+d7t+Xn29ljCaP97dv2lPndv9J4GkwDAAAAAIDDBtMAAAAAAAAAAEwwDQAAAAAAAADABNMAAAAAAAAAAEwwDWCj1P71hIvCRZ8fAAAAAABcbjANYKNgGgAAAAAAABwuS5oG4df587JojqtxZ83JkbVflbOT5ihrq+OK9qLUXMn+dYBi31E/wXYMsy6WJACH2k/P/4DR62mt90T2W1Sn42iduzn+fDCO8dLzW9P6AgAAAAAAbJKVTAMtcoLuKcTX0Ulz5oTWSU9YhfbdvijeFscqxlFrH0WWjk+7UpwXeWV/Br6dETfcfmL+K1ITqltB1t5Pyb32i1QaP9OQpvmxXx+rr088brswDda0vgAAAAAAAJtkbaZBJoLiPhFaXlAFZaUEVym6bVFWbS/bi4WTeknsHTcLV6/NBd/f1kwDO/9V2USfk/HHMwpZ/dqKdYRD1K3H2P51sM31qY1VnV84sepmxMz1BQAAAAAA2AUbNA1UTE8UlaLbiX75rjUTX/X2SdCLLvP17oW01QIuxXT92QRtt6ppYOU/jh9blbR2fmyz9NewK1qgprULeaXSHZt1EseoiHd9TKy6tvTa1+c3bX2GyfuwxX15fKw5js8PMwAAAAAAAA6X9T6eoEWVF/pJjJUiuxSEhvAaaN8KehnUmwdBmGkBZwpLwwTweddMg7K0cRPyH8H3XxHaiXpMHF/lHfLN10tKbkTY4nhpolE0NHd9TMr9On+/3c51bH5q38gajhIS6a1LmV9trNr8EiFv6cquBwAAAAAA2GfW90OIhaDyYqkqukL7ILb062ntuzp5LGHRLBbl/n77GqU4TAy3H89/DN//yLfQPsYSxZmhkgjf+AdxGnLKharUr+9b75B/OUYfW1Qbuei7SUbnF6iuzxws00DnEvfVxhozDTwTzBUAAAAAAIB9ZH2PJxj1mUjKxFlRH5SXEmTD7bVQ06+9sNu6aeC2e/lPw4+RitG+JlRNoZsdE/060RfdyxP6lzLWnz4+LVFE90sU6qPzC1TXZw7WWIZpURvLnF9JO99yTgAAAAAAAPvNZkyDqihMbUL7TmwV/Y20rwk1LfT16yGCZlzRNBhbj1FC+3K8qig2RfXInQZ+Tdd3p4EnHqch0Wweq7FcRucXqK7PHKyxjPxqY5nz0/j+i2MBAAAAAABwIGzENBgSWK6Re12K7rzNWPuaUOv1URX9HV1O+f7h9sP557HTMPPwOy1xHQS0jg+hafz+8anNc3ViLpW5y7j9YxXyq+czNr9IdX1mEBbGNCjavH2M39Gboz2/rm7l/AAAAAAAAHbIRkyDqpBqBVpfdOtvrcfa/2ilvmcalKUi+izxOtx+OH/dTw0/ri5Gbv04LUCjsE4lax9FuS4bMQw6asestl9yfPrmU03z+OMxQVduvqP54XvvjfXHzfNPqXo3v/fdd1/zgbb+Dc1b3fYz36XaP3Wz+fiDeX0ZH7bj+uj+pej27nh+LtXL2oaJmOePOT9/4EozAgAAAAAA4LBY0jSA/WbY1NkmNVEtAr655x6PvN737bfF7XIedVMEAAAAAADg8ME0uJDst2kwJM73eVuMAz0PAdMAAAAAAAAuMpgGF5L9Ng1EiNuPDaxju3isQUqx/fwjq42nwTQAAAAAAICLDKYBAAAAAAAAAJhgGgAAAAAAAACACaYBAAAAAAAAAJhgGsBG4Zn/3XK8OGf9AQAAAABgaTANYKNgGuwWTAMAAAAAAFiFJU2D8Ov8eVk0x9W4s+bkyNqvytlJc5S11XFFe1GirmT/OkCx76ifYDuGWRdLElhD7afnf8Do9bTWeyLbMg388drEMTg6cWdfVw5NgC9tGrgDd+7muzi+kr+2YgEAAAAA4MKykmmgRWTQlYVxIILLCbmTnnAM7bt9UYQvjlWMo9Y+ilgdn3alOC8iy/4MfDsjbrj9xPxXZGNCeApRLHemQWn8TEOa5sd+M2xmrcrjPI9NH78p/S9tGrjjf3ouxz+ZBu74X8M0AAAAAAC4bKzNNMhEZtwnosYLlqAclbjpizFLAFXby/Zi0ch/waQ4bhauXpsLvr+tmQZ2/quyiT4n449nNAr0ayvWEQ5Rtx5j+9fNZtbKnVcj8x5i08dvSv9V08AdmHN5/1wp9ie8aRCNAv3aigUAAAAAgAvLBk0DFdMTnaXoFnFWist6ey+WnKD33oHUuxfSVgvUFNP1ZyNtrLjh9lPyH8ePrUpaOz+2Wfpr2BV9l0dau5BXKt2xWSdxjIp41cfEqmtL1t7KX8/daC9llkAfWr+EjN8fd4xpx29g/vG9JOfflbTPBcsjArKWU/sPY9TvNJA6qw0AAAAAAEBivY8n9IRPEmIhvhMvpWDTdePtW0Evg7r/Hy+C8JHNLKYshgng8zb2D7efkP8Ivv8RkVuPieOrvEO++XpJyY0ISxivQBS3Q3PXx6Tcr/P32+1cU/6doM3r++2nrGfHyPr5zq0yb/2GchqevyOubfgdgeNG9H25jlPmPPZ4Qph3GseOAQAAAACAy8v6fgixEC9ejFRFXWgfxIx+Pa19V+fE1NmiWSzK/f32NUrxlhhuP57/GL5/JYotfIwlCjNDJRG+lQ8mQcgpv7NguW/Na4T8yzH6yPr218bIxc8p7TPyDx2FtchiA9W1shhdP71v+TWr5zQ2/0BqL4/duMS6uw6K+qE5T/pNAz+2dHXUGwMAAAAAAC4363s8wajPxEpQ51GoFfVaEFr1KSa210JUv/Yiauumgdvu5T8NP0YqRvuqKFRr0e3Xx0S/TliieFlC/1LG+tPHpyWK1H6ZYxrk86+ulcXo+iU2ZBqMzj+R1tnlavz2wJQ5zzENauMAAAAAAMDlZTOmQVUUpTahfSdmiv5G2ptC1OFF1C5Mg7H1GCW0L8erikJT9GpTwMjHr+nyAtgkHqchUWoeq9FcjPxDR8o0yNtPEdAto+un923KNBjvN52DPl2jnylzHjUNXOf8c4oAAAAAAFBjI6ZBTcwErSbCOLTXYka3GWsv/7eEUK+Pqujv6HLK9w+3H84/j52GmYffaYnLIHB1fAhN4/ePT22eqxNzqcxdxu0fq5Df2PpWTYM4Ztuvn5zfMXH9x9ZPxy1vGtSP39j8Ha5t968buDyM3zQIMcP5DZkGUmfnBwAAAAAAENiIaSBayRQqXkTJN7yhfRajvrUea/+jlfqeaVAWQ1SGLvvibbj9cP66nxp+XF0qgjeP0wIvCt9UsvZRlOqyEcOgo3bMavslx6dvPtU0TzlSufmO5n333efrJP/nH7mveavfdvHSUVvvtt16f07aCjL3st4hbdv2ve3j5vnUXorrQ9rm8Y+4mJvtmg/3Z28/+X41P9fX029K9QPzjwf9c+94U9ufPx9drKxJtX/DAKiaBm6MwX9yEQAAAAAAwLGkaQD7zbCps01qpoEI3uaeezzymu3Vtt8Wt8sfMpz0mwYAAAAAAAAVMA0uJPttGgyJX7aX3xbjQK+zgGkAAAAAAACrgGlwIdlv0yCJ3c1sF48dSCm2s8ceRvvbdv+rb2swDQAAAAAAYBUwDQAAAAAAAADABNMAAAAAAAAAAEwwDQAAAAAAAADABNMANkrtX08AAAAAAACA/QfTADYKpgEAAAAAAMDhsqRpEH6dPy+L5rgad9acHFn7VTk7aY6ytjquaC9K1JXsXwco9h31E2zHMOtiSQJ3qP30/A8YvZ7Wek8E0yDgz6eDO0fceX56Lke+Ob5i1dc5OjltpGUo0v6KGbcZjpqbLu+zk2vNlZl5r43j2825W4DFjSvNFf3aigUAAAAA2GNWMg20iAy6sjAOjk6aMyeUTnrCMbTv9kURvjhWMY5a+yhidXzalYn+sj8D386IG24/Mf8V2anQlLX3U3Kv/SKVxs80pGl+7C8nl800aBHBvKRp4I0Ht2bXZo+9B6bB0c3m9NzNvDUNzppb1zANAAAAAODwWJtpkInMuE+EkheMQTkqwVSKbltUVdvL9mLhpEgyKY6bhavX5oLvb2umgZ3/qmyiz8n44xmNAv3ainXUzIHa/svGTo/lLrnUpoF7z4hRoF9bsQAAAAAAe8wGTQMV0xOdpeh2ol++y87EZb19EvTeO5B690LaaoE6LPo7pI0VN9x+Sv7j+LFVSWvnxzZLfw27ou/ySGsX8kqlOzbrJI5REcT6mGja9VWLUMZl62P0X1s/s75oPzR+WH+9nkL/GM/Nz4oZ4njR3eAvZXGchHft+ObCPGtvjR3fs21xMUmg9x8vKNo68vzsmGVMg3xsXYLwDnFuDfxdEKksmhvtGKVpELZT+yTcj2+fd+P4ueftF8c3moU8VxACEP0AAAAAcClZ7+MJWph4QZKEV4jvBFfY1qUnLAfaZ4LP/f94EcS0bObCryiGCeDzNvYPt5+Q/wi+/xERWY+J46u8Q775eknJjYhUvyai6Byauz4mmnZ90/wKY6g8Ln5brcXY+k1qL8UcP4jxzIQIHbbrN9Z/WT+Wb8lwvD6+QeiG+bj8onD3gt6N3wpk2db9uQRFDpdGQw8f1/WbEGGv26b+e3cFbOROAzd/MQEWN9o7CYLRkIwDbRpEw8D3o/IVw0C199ttTN9kyOtTHgAAAAAAF5/1/RCiu6DWAseLmKpoCu2DmNSvp7Xv6py4O1s0i0W5v9++RinuEsPtx/Mfw/evRLKFjynW1ZMZKgktdENO+TfvUj883hxC/uUYfWR966aBnoPO38i1MBWG129q+9r4sV4d//w8Gem/GEvw/VnHssLw/NLx1QLW5SS3wPt4/TpS5FSaClUqpkEPiXPz24pp4OZyel72edzcTr8h4NYnmAY33Drl5kIX69ZC3zmQPUKQ7jRQ9TIPnwumAQAAAABcLtb3eIJRn4nFoLqiSCvqg7JUgmq4fQgPdfq1Fnql6KsRuu3HDbcfy38aQRjGYrT39Va/ai26/fqY6NeJXBSvRuhfylh/+vhoBtfXC1yr5EK4un4T2g8fX4fvI61xYRKM9Z+1DVSP5QDV+bXHtzQN4r7R+VvtK7gDaJoGbgz/ZbwuLsetmAbS57l+HEFwc2qFfrpTIJTev1rgDYJYmRW3PpgGAAAAAAAZmzENqqIltQntOzFZ9DfSfooQHRWFkaC/+3HD7Ufyn01oX45XFZqmaaBNASMfv6ZK+K6DeJysY5FY3jSYk2uxfhPaj58fag3DJApTYqB/o756LCfRzS+I2JSbErB6zNH5h/ZDx63Fzb1vGgSDwt/+r+NqAn8rpoF1p4HLT/IavKsgtddgGgAAAAAAJDZiGtQEkmgvET6WaNFtxtrL/6tCVPcxKAoDXU75/uH2w/nnsdMw8/A7LfEXDAIdH0LT+P3jU5vn6sRcKnOXcWebBjH/Ofnm8xtvP+n8kE5dTPifrhvrP6xJO2+fnN+xwvnhbyOIIjYd307AWvXddh8//0FjIeJyHzcNwrbMb52mQTAHgrjP64JBoB87sH6TIP0Q4tFN+b0DbRLEOxF6jy0kMA0AAAAAABIbMQ1EI1lCMYgn+YY8tM9i/LejYd9Y+x+t1GvhHkRRUQzRFrrsi7/h9sP5635q+HF1MXLrx2mRF8V6Kln7KBp1qQrc9VA7ZrX9fn3f/0jz1vvua/fJ6277qHn65lNN85QjlZvvaN4X6/26SF2qd/OXuqntx8eX7Uea5317OWfn5Sfnw+dSfnJsJGFd3+sv3x6eXzy+ut4d37y/kfwc73vHzUq9619Ete5fylM3m6ffFNr7uwDaendeyr93OrF9EuJ5vvb2k+//UGzsim9/f2tU/MKHVP+yPvffH9sH0f+5d3xr87bYnzcVXOzzj6T2R81Hnyjyu/nO5m/c37V//tH72/aYBgAAAABwWVnSNID9JojKmqmzTWqmgYi75p57PEkssj11+03BEKjW73ZbhLZsn1+gbUG22zsPAAAAAAAuCZgGF5L9Ng1EiAkizAS2l9t+/pH19reu7VXE+T5vv9291ucxAAAAAMBlANPgQrLfpoEIMeFybh+HRx6y2/bz7WQG1PsLdxqk49uv3+ft4+YXxub/aHrMYB3jrXs7/p7CQOn9aw0AAAAAAAcMpgEAAAAAAAAAmGAaAAAAAAAAAIAJpgEAAAAAAAAAmGAaAAAAAAAAAIAJpgEAAAAAAAAAmCxpGoRf58/Lojmuxp01J0fWflXOTpqjrK2OK9rLT/K7kv3rAMW+o36C7RhmXSzpl/6H2k/PH/abdBytc3cY6/wo/5UIAAAAAACAQ2cl00CL9qDZC/F1dNKcOTF90vtn90L7bl8Ub4tjFeOotY8GgY5PuzLRX/Zn4NsZccPtJ+Y/gh/jApsNez0/f+DPmhM5uZY1DWYebwAAAAAAgENjbaaBF/heN3f7RFh5YS0CLROPpei2BWa1vWwvFk7qJbF33CxcvTYXpoq69ZgGdv5jLNPmkNjf+bnzJZ074QTANAAAAAAAADDYoGmgYnydfsSgFN0i4nIRPtQ+CTbvHUi9eyFtZXs3poGVfx3ft1nyxzB8bqn0TJMwfymL45BP1z6tXcgrlex4DfXvaOevgsr5Ze2V8B6fnzq2qT/pTOUwe/xlzQnfCaYBAAAAAACAxXofT9DCzQv9JMZKkZ1Ebld6gnugfSYovXgOYlQ2s5iyGCIvaMb+/uH2E/KfgB+jInbLvLL19RthzJRnMg5CHl1+6RiFuE4cD/bvaOef9hnGzeDxjzHlvkDIb9Q0GBh/LP/JhI6WMw10WWZsAAAAAACAPWd9P4RYiCYvqpSoywVkX+CWonuofVcnjyUsmsWi3N9vX6MUn4nh9uP5T0HPKa+TOwQ6gezRolkJ3a6Pfk6ZKM/6HOk/9ZuJ6XDXQt6nQnKyzgFzfkZ+Rfvh8cfzn8ySpkFOyM2eKwAAAAAAwOGyvscTjPpMSGfirKjvCc7h9iE81OnXXmhu3TRw2738p1EV1V4AWyWKYrUWXR86p/C6bxrEfWP9p36H1s/qo5hLdX5WfsUaDo4/If/JqLU066eyrn4AAAAAAAD2iM2YBlVRl9oUorvsb6R90JepbYcWmqOiNxK0Xj9uuP1I/hOpimo//wEBrARq14fOychH9znWv2N4/sGAyI5BOChbNA2WMAgs1Fqa9RPx+WIaAAAAAADABWMjpkFNLHYCXQvcfpux9kFfVkwD3UdV9HZ0OeX7h9sP55/HDuAHt8Rv6L86fkh6lmmQz3Okf8fw/EvToHJ7/tT5JZNItZ+y/kP5T0atZa8u5TVmBsQ465wEAAAAAAA4ZDZiGogOMwVUK9BC+yxGCa+x9j9aqdfC3b8uiyHqQ5d98Tncfjh/3c8Yfvy2aIEdhbEuafx2HfWcdU5G294cB/pP/Q6I8nx9XN7h37u017ctan6tIJfi5iKBM8Yfy38Yo60vhcExYBrk85JU83oAAAAAAICLwJKmAew3QRQjZAEAAAAAAGAVMA0uJJgGAAAAAAAAsDqYBhcSTAMAAAAAAABYHUwDAAAAAAAAADDBNAAAAAAAAAAAE0wDAAAAAAAAADDBNAAAAAAAAAAAE0wDAAAAAAAAADBZ0jQIv86fl0VzXI07a06OrP2qnJ00R1lbHVe0P174Jtm/DlDsO+on2I5h1sVydnI02n56/quSxrHWdvfEJQ9lqfnv9/wAAAAAAAAuOyuZBlq0BwFZiL+jk+bMickTV5fEeCC07/ZF8bg4VjGOWvukVlV82pWJ/rI/A9/OiBtuPzH/lYn97qGo9utWmihz5h86aE7k4GIaAAAAAAAA7CVrMw28wPe6sdsnwtsLaxGI2TfRpeiOIr34trraXrYXCyc1k9g8bhaFubBd08DO/+Li1tu8+6O8o6SGtI/HLhwATAMAAAAAAIA9ZIOmgYrxdVpQlqJbRGQuwofaJ0HvvQOpdy+krWzvxjSw8h8gJB7G9sOH/lwP+RzbUorqtDZh3FCmCnbBai/bVqxBWDSVU8p/Rh+JXl8AAAAAAACwL6z38QT9TbsX+kkMhvhOVHciM5We4B5o3wr6VnwHwSybWUxZDBMgaNaKaVCWNm5C/kP4QUObNE4yDnr9hARN08BFt0aBD5t8p0OXfzqGIY+J4j0sdBirNYtCn5gGAAAAAAAAF4f1/RBiIVhbYa+32xgtkPXrae27uuNmcbZoFotyf799jaBZ+3HD7cfzH0QJ5W5elX5MUR1iM4GuhfwoRnvrkYMacaxj6aRtY/U5AXN+AAAAAAAAsA+s7/EEoz4TwJk4LOp7gne4fQgPdfr1bkwDt93LfwQ1l/0yDcp9FXxOrmTjzWivMecHAAAAAAAA+8BmTIN4y7pVQptSIBf9jbQP+rgQ146dmQZj61GihPLemAZ+zSfeadA+klDuW0L8m/MDAAAAAACAfWAjpkEnhPP9nUDvC2TdZqx90McV00D3sTXToJ6ziRLKXbt+n2Vst99Y/7Ao08Y32tfWoYaPb8cL/fXat+bPgClgzg8AAAAAAAD2gY2YBkG/9kV9JxANgRwFpuwba/+jlfqeaVAWQ1SHLiumQVkKkVzLX/djEufx5H33Ne97x83Y71Hz9M2nms+9400uJorwp54KpPLUzebpN93X1j//yH3NW10fbZ8339G8L207pK6tz7aN/mcYBoHYRypW+6ppULRty8Q7HQAAAAAAAGArLGkawKqIeG/uuceTxPz2tt/kDQpd/7ZYb+UKAAAAAAAAlxNMgx2gxfqwuN/sdrpTIW2LcWDlCwAAAAAAAJcTTIMdkMT7ZraPm+d7jzXk288/Eu40SI+XlP0BAAAAAAAACJgGAAAAAAAAAGCCaQAAAAAAAAAAJpgGAAAAAAAAAGCCaQAAAAAAAAAAJpgGAAAAAAAAAGCygmlw1JycxZ/jj+Xs5Kjbvzgu4t/QHC98UHM02P4NzVFZocpgTOrbDyQpqPGtfUPo+F7bfu6utjku+xggz39eW9gW6Thf1uPj5n96HuZ/xarfV46amy7vxfGV5opZv32Obp425+7v07UrV8z63RDWya1Uc8Pl1a1VXL8bbt9BHfdDwa3vnbvN6a2Hmwd2sL7Xn73bnC9uNA88kJ+LRzfvNHeN/XXCPM6b55obDzywN+81WJWrzRMvvNTcfuyh5sEHrHpYD1ebD37qxebu+U8073njQ80DZsxmuPrBX2tevCuf7VLc+N+43fE3zdUf/LXmCy9183vvBZvfMFebH/zVzze33/NNzRsftOphlHd9tPn9L9xtfuLxb26+8Vi9Zj2XNQ3qxoDn6KQJ1eW+s+bkSLZH2iu8XjfivOiutY8iX9enXcl0GEXPwTdOuQshfz2/EJIMkRmECWIa7CWbNQ38ObzMObMmxsffb9Pg6CQJ8bJuPabBOoU+psHh4UX06a3m4ckieiqXwzS4qtYPQ2H/uPrEC81L7vg88mB57NZjGlx94lOh/4cwlGxWMw288L/zZPPoGx9cfn3f/bHm8y8tLpxp0CLze3GxlGlw9Qd/tfmCW983u7aHdf5iGgjf8YO/0nzeHb+3fNMS5/Z3/LfNr/zBi81zrWlw2vy9P+/WExN1SdPAMgUKShEdtHEU8RPaJ7J2ilHTYLFwl8JJ7B03C5fLids9zzSIRkFmeAh908CPqeY7mTBBTINLyP6bBvsNpsGmwDQQLqppUDMHZP+dk/XlhGmw32AaHDaYBhPANDDqLwermwZ3mifFKDgKr38I08Cz5J0GToQ7qTssOEKMF+k90T2lfWAZ0yDVee8g3ikgeQRdP9E0GKQ0DcJ22bfPPZXaXMMEDdMgfcudSoqxxxLmisAsvyKHdn1V0FLzGyAfX6+nUJu/QxrG4+trjlOsPsdWy8/Pvy3l8QnjLY7jeexLPrZQm1/ety5dH2n9r7hO0k123fqn8dVFv8SVc4zmXFti/eD410KfXpDHvX7+hjAPdyGkomNCXW99Yt+J49u6fZjPlA/nPDddwhhX3PjBNLjm1qldvVjX9SPj6znKt+2+fxH4cW9e+n0MkffvytmtzDTI6os6q70X8ar+DUc3m9NzFaH68HNwwtCfPzHk7Na11gTw9W3T2p0Gbv3c/1NYaSJcH8l/jOu35VvqrixuOHGhziHJoat3OSbxef12c/f2jebGItR3eZ41t651AsX3nzqQ/CaKfy+qdWJtcf0/XOnff9ueH5+83kW08ytNg7Dt86/2r9uvhjZD5PXpyTWfh9yBcBpNg9ZYuOHWOiZxpkyOfI3yOw28WaDy7kqcX+zj+rMvdXHu+Dw84xGHq0+E/B50+b0UO5H8HlTHuNd/IY6zeleee+zBOL8gmhc3HmsWd+W4lPVdTNf8ueaxtv8kunX7s+bJR3R7a3wn0nX9M/ItdKz89JM9cZ/Vu1K2r+HNAt2wLZ+OOab8H2lu3Uk5Sp27+Fb9v9uN394B7ub/nodcW9//pwb6z/sYote/E8fh4j98Q3/7sfc0i5e6Y/Dce94YTY6xepfjBz/VvHj7seaNjz3b3sb+6ScfbR56MJ0/6S4AX+WKjJ/ER73/hyb27+vbzu07Dd79sS+4dYwhKiZ/tEAXt76PujnG9c3au/PHNBcM00D6/8JLzxVC+93Nxz7/YnP6Q292QnTi3/mrH2x+7QvqPfLpH2re3Obg1vDXXH5tpXqEQHJ69rHmvYsX3TxdzXvf3Nx6QWI/3fzQm2V+oe3t97y3WbzojlHq4b3f2K5/y4Bp8O6nP+/790Vyi+aANwu6xFSR8d0YcX3f9fQfmO31GEOU7d9StM/qZX2+yc1d1b/h6g82v/r5fH1DH8k0eItbt8/HNXa5v6XLfYzvcGvw+dvvab7Jnb9/8IWQxKd/6C3NNz70QBsj+cWqMHYhzt/19O939VLamO9ofvBXXH7v/ebO1HjX083vf+pW1se7PqraW/3relfS4wPeLHjRPn5/7y0I/1VZ/jcNtCAZEu/uYm/hlENP5E5oL3jhZdSbwifGJcHViUt3se/EmGxaYns+SaSqUgi2Mm+/bQnXEGiKUt0+raXElX1nMcZ+C4nVIr3Mr13ftK8wfibPr4Lvvxo/PP9kZMixTHkm4yAd31Xzawkd2cenXI9y/UbGG4pJ82rrs/UP4w+aBrLtmmcxBVNyDP1oQ0Bw4zuRJuubPuSCkJc4GS/WS77RKPAGgRsr3RVQv0tgOmN3GkhJQtsLcB8b8hHRrE2Ksj7FlPum4vsT0Z7mW/RV1ofxC9E/NPbxbS8oe0ZCpDUFUp/eYAjHI4t3Avz83DYNpHkyCkJ/XZw3DHz+IT+/rfIfY3h+cXwRrbG+HV+EuZgGcjrdutZck3PAr0MwDk7dPhElXnCr9n5b8ptoHAhaXFt1t4+VAI79p9ihtn5+rWkQDQPVVhhuvyLXn3V9n7i+r2VjH5emQTx/fA7u/Llz9zQzNdq+/HEp9juG7jTwgnnxWCvy/bYfq9+PhTcN2vxcm6tPNC/4/IIwr/YfhbcIZ59b71t2oTMEklHghbabZzAGYr3qP9Qn4yC174wCP/5pJ/zr3/IHvCHg+k8i02/PaD+Feh8hf1lfb0S4i2wv4GX8eNeAGAPP3ujuRCjrQ//L32lQ6//Rh0R0doI+GQFBhCdjYaw+bbsAEdPSpxO4n3rRnT+PivES2z/3Hidqgkjq2otwyfsXoZrX1/q/40R9Yew4UfsFEe2FaSDC/ZkbnQnhDQCZvxL+Q3ca+PiFyz+KPKu9xzANkkGw0CJc4l64pUT/CF6s37WFvKyfGAbPvbcVoZlREdt++ofe3Dx68ikv4JNxcOdJMS2OWsMh9W8bHQ7fV9808IbBwo3/xrg+sn2aC/+hOw28oHftv8mtR7tdtB+i1j4ZBzL2x258Yyuqy3oR2X/g19e6myCYBmF9Qn2v/QjeNJAOohHxwHeIQeHOXzEe3HpLf19YPJ7l/wXpPwp7bxi4+m+O9fm3/uOmgTcEpP03xv5l+/TvtfVT7iJY6U4DqLLCDyFGvFoKJf+mWEjiqhRdisH27mLaa7Z5ArmrO24WZ2JaBGE01GYeSbR1+3ye7iI4CDD5hrUTlJ5CdLeECebr42PLNQvf2sqYeh56feT10qZIaNwKSD9GlkM3/qz5VQj9V+JH5q/XzPfj8w7HJMx/9fxarONjHH97/YbH63Kv1Mm47QWGm5P7Gx5MgDR+uKD0uPFFhKW+jkPw4AfE0Pgt0m+Wh8Ot5am7gM6NhC4/+aZfBJwW5V7kurEy00DWJ5oKyzBmGtjjV8Yz6keFew1DoOd9HTe3z09zAV+0CSJZvjlXMYrSdChpRbYbL7S/7sZ0a1LcLTBkGuSx0j7lrF/H+popUWFwfr6vaBC0+92Y7iLc343gjlUQqvJNuevHCZNrV675nINp4NbXCcgTddeBF70uZ30nwhizhLsYGV6Ih9gguvM7BzqSaXCjueVy1uZGYrj9irSmgVunO7fdZ6QctwfcOXW3ue3WtzMNZI3T+NebZ+P6S33WVxbXUTcNXF8vueOj7jp4w9XOlMj6rxBMgyTiZZ/0ebdZeJGf+g+C3bcpTAURzGF9VUxLEM2LG7pO+rzTnIgJcOT6ckKrGzvVp/FD+9v6zgS3Ti/d6QS6F+x3+3cfpL6ecQLzlq6T/N2c0r7h9tMYMw2yxxPe/YwTqAMGgFG/1scTXP9fcP1r0+D2Y8EQCDHvbp75glszJfrr9S63wkQI9e6Yi8lwTQT+7eaxti7UfyzWP/RA138niKXe6j8JFt0+tXE4UWuZBj0kzon2aaaBjPWCy8XNpT1/Ptj8mjct1D5BRHXPNBCTIYpqL+qDyA+CfdpnYd6+qPd3INyuGxXHndC/JmaACPc3Ptw80eYQTIPb79GGhLR3c36zW99yfj3T4N3N0z7WtW/Xx4lid/yeVPvqpsG7mqf/YLx9nUr7JMqt9mISuOOfRL83AZTpkGM8nlC077fJCabBc83j7d0NkvMXmsXj0qe8/lRzKxoIvs13/GDzKz7/b2oeOlKvY/080+BdzUd/X/rv2mf9u33hboIzf+dAdw7kYBpshtVNg0grcor9QXONC/W57X18pd+g37pvnNProTbzMESjFqX+tVVive4rTDCft7VPjxkm5cSeE2piijiOrZyGsHL0fYb6wbWaM78BwjGPRY09af6x3vfh24Z6f6zXlJ9nLBcdp+fgqM5P1xv72zq3/vYf+DS++gB344soDX0Z9QZD47dIvzJ//UFm7ZMxW6GuX6c2uWkgBOMglqJuCiuZBl6YxrFT0fWOXOjHfqaQRG+tL19fDi4lF9FBWKcq/S2+EvUxtsS3VXcCVJllGsR9E/Mfozo/fyeBCFErJydAxkyDa2IQ1PKbLmAGTQMxIeSbRF38t95dbBDeVl0wDVJV7bGDevsVSUL/xsI/miB3a8hdEzdK00DMjLExlzENvEFQOT5zTAOXn34cocUbBLX+O5EdjINUpcVzzTSIpsDxs06wl6aBNgrGTYM0fnv7+Jl6/MAbBN350ZX0+IBuH6OMxxfGWMk0cDl+qsxRcliXaSD9v9jvf9g0iKJfiXq73uUmol4eH3Bzz8YVxKDwQt6tdbtfGwU106AzBdr+LdGsqZkGXuQb859iGlhtfckfX/BUTINM2Jsif4hgMuSiXmEKedVmadOguDuiNpYX+Oq2/rbkjx9UTYOJ7asMtU+mgTcRipje4wfWXQbCmkwDeTzBMiX8XQe1/JNpcLt57ze7cy22mWUaeIPA7l+bBME4iFGf7u5CSHliGmyGtZkGQVz1RZnfPUWoz2yfRFW5X5A21jfuQ23mYYjGnmkwUaCGCeai1Nrnvz2PY/r+Xb37/8LNU8LD7flTRXHoK1ujsGitgBxcqznzm0RYz3a8sfmr+k74hj4602BN+Zm5GMe/WL+cYn6RIdGe1t/+A5/GVxfMbnwRpdo0sN4DmqHxW6Rfmb/+ILT2yfFxf7/nmAYdId7Pd+wDV7G8aSDf9J+7qXfP+G/1TgOjfpgwn+7OgrCd5V+wdtNA5zw7/zG6+flv3E3TYMadBt40mHdXgUXdNAi56Gf8yzsNctz8xCRI84vb/vEEP5cgluu5lu2tmBl4w2PR3Lolj6g5ke5E8N3bJ82ts7C+MqfNmwbhroI573fNuGnQ3VXQq+8hIlnWNz1uYJgGXsjHb/rFAOiZBvPuNOjGFkL83efi4wh6rMn5q/ZmTJ/lTQMnwF98qTl98hF3AR/HW+udBqn/R7P+B+80cMK2fLygXu9ym20aaFPAMA18/3faxw9WMw3CWDL/9pt9iZt6p4GIfOuuAouaaSBrHEW6PCLwqROVyyhdW7ONaRoo0b+MaeCNDTfnKXcaeNEuscMCf9g0GG9fZbS9fJMvvx8hvyEQ1y8T/cEUuPOkqs/Yhmng8td3GhT1K91pYLQfJvT34nPd4wwCpsFmWJtp4LWVuyguBYjfXxOfirnth0SttKmaBsYY80mirduX5x/qp8zbFqVBIOv2ef9Sv2hOThwijKUy+9cixihNgzieWpuh9Z01v4mEZUj9jcxfrVl3TENOYU5rzE+N1e3vH38fp9avJJ9fJEzKNDfS+tt/4Lv5+XpvkkhXxfEbM07c+P4RgdEY2yDQ+fnHIdz4QcC7/ER0TjYNXPvb800D32cUr3ldFL2TTYOwHfJXfan+7eNQI/bvBKzv3/cjhyN9m96J5KnzNX8jQcRmJbd1mwb5bxjo/Ef6n0j+GwRBlLsE4nasT+unTAXTNPC/E6BMiGKsyfhxLEFfmgZhu/xdAk0+P8kv/aaBHCsngEeMg7x93O/Fv5xY/R9hHCRrJ2M6wSv5u78FKYd1mAZeKN+NfWbn+dVognS/CTCXQdPA9Z+bAGV9n/w3EIJo1qZBXh8MgnMn0lP/4TcT0rf9SXRPNQ26/oPoD+3nmADlbyBM4roT+ndPjUccUv5TTYOwfV7caeDbvCT9u4v2rP8xStOg679mGrz7mfgMvzcBxupHTAMZT74JVr9p4NsXv6mgTYPyNwTWaxqE7bvFnQahrVvfnjkgoj3+JsOE8W3TINa9sGh+4kzMlgkGhCL8xsBZ/OHCsj4YBHd/ont8wT/OIL8JIPNTQn+qaVB9HEL11c1PRLU7XvKbCvE3DUxEaL94atw90LW3Hw8YY6x9aRqE7bvtnQbB0Pi8rK/5OMOGTQMR6a7/l5xIr+b/+1/w+X+THA8xBOQXC4sfQpT2/jcP0p0FVr0yAYYofwPB8674TyXWzAf5FxLceXje/ETz+Dd/Y/4jk1BlOdMgiRRdlGDRmGJp1faOIIqKEvuQNtswDbLSy9GIacc26nzRAi4K51SyvGP7dl9f9I+Rr58bN/x7lG37JFrLdh1D8xvHH1ddem0H5h9OigHTQOJWyW/s+IT6IdNgfH5WXHf80/pX/8Bn7yG3FsX4Qn6MXTFy8GK/LUEg+/k5AdYvqV7aRuMgFdd3ZwiE9kOmgTcJdMnaT8eL6dhFyq/71xOs8UP+XmzGVr5dPP8z08Bh99/VV/HfxseWInZvLJy41f1H4R0iQmlNhXJcV1RdIhgHsV6Kihk2DYyxfUkmhFHf62s4/zG8CI7NfOm1jcZBKrp+1DSQ9i4/MQ5ic1+kjyjypuLzbDspRLXefysc32Qa5O1ccWN3hoLk1pkGOj49qjDcPrKsaZBMgrbPkM900yDFl6VvEHgx3Abq+mgcxBpffD5h/Hy8PsOmgSDC1+g/ivY8L1dcXSfog2jO2iqDIBCNg1httR8yDfrjl48XhD6yGBHlqb2YBJW6Lsdx8n7S4w8p/5ppIOuv/4UE1+7J5/xdJeVdBfm/gCD9O1Ggzo8a0n/3LwR0/WvRns1fCXxZu+F6EbVDpoEQjYPYvHs0Quoq/SvBOmwaGO196R4f8HcRlPOXOy2cyMrWV/8LCaq9H0OMAz1GazoYdb7I7fVa5PfF/RyCcaAGcaKw+yHF2Hesyuommga66ybL0Ynmst4X/fhAFO7Z+vR/yND/doBa37nt69jtc1MgVbpxf+g5//dDi/48xpW2/aZNAyEaB2X+6Vt9/YiB7H/v7ebz+l9H0PUi2h9fNF/Q9ck4yPrvHkEo/+UE6/EEIY8r/vUETIOlWN/jCQAAAAAHTRDN+W8awP4QRHf2+MGs+lXp+p92+/ShEoR973cCdo6YHgO/mQB7B48KXBwwDQAAAAA8mAb7DabBNsgeGTDqdwemwaGBaXBxwDQAAAAA8GAa7DeYBpvEmwVyS7fcbr93hoGAaXBoYBpcHDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMAE0wAAAAAAAAAATDANAAAAAAAAAMDgDc3/H6gHuigM3WGgAAAAAElFTkSuQmCC)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJdUc4ji-Q8c"
      },
      "source": [
        "# Ollama\n",
        "\n",
        "Unsloth ahora permite ajustar el modelo autom√°ticamente y crear un archivo de modelo, exportando a Ollama.\n",
        "\n",
        "Primero vamos a installar Ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O15au36B-Vyp"
      },
      "outputs": [],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsSmwZzNCQeU"
      },
      "source": [
        "### Usar ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPbHMS-6CmS4"
      },
      "source": [
        "Usamos `subprocess` para iniciar Ollama de forma no bloqueante. En tu propio escritorio, simplemente puedes abrir una nueva terminal y escribir `ollama serve`, pero en Colab, necesitamos usar este truco."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wloHHZfCKKz"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3) # Esperar a que Ollama se cargue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuCqqUq4AMLO"
      },
      "source": [
        "### Usar Modelo subido a Hugginface\n",
        "Si hemos subido el modelo a hugging face podemos descargarlo con el siguiente comando (si el modelo se llama model como este caso, si no habria que sustituir el nombre del modelo por el que sea)\n",
        "Una vez termine de descargar el modelo hay que parar la ejecucion de la celda para usar el resto del cuaderno"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqEM3Lbl_x2t"
      },
      "outputs": [],
      "source": [
        "#!ollama run hf.co/serdom02/model_8bitQ8_0\n",
        "#!ollama run hf.co/serdom02/model_16bitGGUF\n",
        "#!ollama run hf.co/serdom02/model_q4_k_mGGUF\n",
        "!ollama run llama3:8b\n",
        "#!ollama run hf.co/serdom02/Leyeneitor_8bitQ8_0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zUqVHKrBhC7"
      },
      "source": [
        "Hablar con el modelo de Hugging Face (Solo cambiamos el nombre del modelo)\n",
        "\n",
        "Hay que detener la celda anterior y volver a iniciar Ollama para poder hablar con el modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN-zen9pXhXg"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3) # Esperar a que Ollama se cargue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X3VD1EaAtnL"
      },
      "source": [
        "### Hablar con el modelo usando ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICFGEuz2GpH3"
      },
      "source": [
        "Una vez ya tenemos ollama instalado y el modelo descargado podemos interactuar con el modelo usando el siguiente codigo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSjE5z7fGpyH"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import re\n",
        "\n",
        "url = \"http://localhost:11434/api/chat\"\n",
        "data = {\n",
        "    \"model\": \"hf.co/serdom02/Leyeneitor_8bitQ8_0\", #Solo cambiamos esta parte\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"El vecino del bloque de enfrente me ha grabado desde su casa y me ha pillado desnudo, creo que lo ha publicado en las redes, puedo denunciar?\"}] #Aqui ponemos el mensaje\n",
        "}\n",
        "\n",
        "response = requests.post(url, json=data, stream=True)\n",
        "\n",
        "# Concatenar la respuesta completa y limpiar espacios\n",
        "full_response = \"\"\n",
        "for line in response.iter_lines():\n",
        "    if line:\n",
        "        decoded_line = json.loads(line)\n",
        "        if \"message\" in decoded_line and \"content\" in decoded_line[\"message\"]:\n",
        "            # Limpiar espacios dobles y unir las partes\n",
        "            full_response += decoded_line[\"message\"][\"content\"]\n",
        "\n",
        "# Eliminar saltos de l√≠nea y limpiar los espacios extras\n",
        "full_response = re.sub(r'\\s+', ' ', full_response).strip()\n",
        "\n",
        "print(full_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s20qCTdICnt2"
      },
      "source": [
        "Si ollama se queda congelado podemos usar kill para matar el proceso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upycRraGCsvP"
      },
      "outputs": [],
      "source": [
        "#Para resetear ollama\n",
        "!kill -9 $(ps aux | grep '[o]llama' | awk '{print $2}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQVgDT5zH57k"
      },
      "source": [
        "# Rag con Llama 3\n",
        "\n",
        "Fuente: https://medium.com/@danushidk507/rag-with-llama-using-ollama-a-deep-dive-into-retrieval-augmented-generation-c58b9a1cfcd3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiPLtIAUIFBw"
      },
      "outputs": [],
      "source": [
        "!pip install -U langchain langchain-huggingface transformers\n",
        "!pip install -U langchain-ollama\n",
        "!pip install sentence-transformers\n",
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Geh84e61IG8K"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain-community faiss-cpu\n",
        "from langchain.vectorstores import FAISS\n",
        "#print(faiss.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0txzLu3IvgQ"
      },
      "source": [
        "## Ingesta de Datos\n",
        "Comenzamos cargando y dividiendo los documentos. Utilizamos PyPDFLoader para cargar un archivo PDF y dividirlo en fragmentos m√°s peque√±os y superpuestos, lo que mejora la precisi√≥n de la recuperaci√≥n de informaci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kpz4Ar6cT7-"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "#from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Lista de rutas de los PDFs\n",
        "pdf_paths = [\n",
        "    \"BOE-A-1978-31229-consolidado.pdf\",\n",
        "    \"BOE-A-1995-25444-consolidado_CodigoPenal.pdf\",\n",
        "    \"Protecci√≥n de Datos Personales y garantia de los derechos digitales.pdf\",\n",
        "    \"RGPD_boe.pdf\"\n",
        "]\n",
        "\n",
        "# Cargar documentos de todos los PDFs\n",
        "documents = []\n",
        "for path in pdf_paths:\n",
        "    try:\n",
        "        loader = PyPDFLoader(path)\n",
        "        documents.extend(loader.load())\n",
        "    except Exception as e:\n",
        "        print(f\"Error cargando {path}: {e}\")\n",
        "\n",
        "for doc in documents:\n",
        "    doc.page_content = doc.page_content.replace('\\r\\n', '\\n').replace('\\n\\n', '\\n').strip()\n",
        "\n",
        "\n",
        "# Dividir los documentos en fragmentos\n",
        "#text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=30, separator=\"\\n\")\n",
        "#docs = text_splitter.split_documents(documents=documents)\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=800,\n",
        "    chunk_overlap=200,\n",
        "    separators=[\"\\n\\n\\n\", \"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
        "    tokenizer_name=\"cl100k_base\"  # Compatible con modelos m√°s modernos\n",
        ")\n",
        "\n",
        "\n",
        "docs = text_splitter.split_documents(documents=documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wmAhvLleAo3"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksK6pmIeJait"
      },
      "source": [
        "## Embeddings de Datos y Almacenamiento con FAISS\n",
        "\n",
        "FAISS (Facebook AI Similarity Search) es una biblioteca vers√°til y eficiente para la b√∫squeda de similitudes en vectores. Permite la recuperaci√≥n escalable y r√°pida de embeddings.\n",
        "Elecci√≥n del Modelo de Embedding\n",
        "\n",
        "Utilizamos sentence-transformers/all-mpnet-base-v2, conocido por su rendimiento robusto en diversas tareas de procesamiento de texto. Alternativas como BGE o MiniLM pueden ser utilizadas para equilibrar la velocidad y la precisi√≥n seg√∫n el caso espec√≠fico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNUGuqwXJn6u"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import faiss\n",
        "\n",
        "# Load embedding model\n",
        "#embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "embedding_model_name = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
        "#model_kwargs = {\"device\": \"cuda\"}\n",
        "model_kwargs = {\"device\": \"cpu\"}\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=embedding_model_name,\n",
        "    model_kwargs=model_kwargs\n",
        ")\n",
        "\n",
        "# Create FAISS vector store\n",
        "vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "# Save and reload the vector store\n",
        "vectorstore.save_local(\"faiss_index_\")\n",
        "persisted_vectorstore = FAISS.load_local(\"faiss_index_\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "# Create a retriever\n",
        "retriever = persisted_vectorstore.as_retriever()\n",
        "\n",
        "print(f\"Se indexaron {len(docs)} fragmentos.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ3LCsD0JplS"
      },
      "source": [
        "## Seleccionamos el modelo\n",
        "\n",
        "Aqui podemos probar como responde el modelo sin RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cI3DdntQT9EB"
      },
      "outputs": [],
      "source": [
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# Initialize the LLaMA model\n",
        "llm = OllamaLLM(model=\"hf.co/serdom02/Leyeneitor_8bitQ8_0\", base_url=\"http://127.0.0.1:11434\") #aqui ponemos el modelo finetuneado\n",
        "\n",
        "# Test with a sample prompt\n",
        "response = llm.invoke(\"Un cuento sobre proteccion de datos\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taF4AoOWMhqh"
      },
      "source": [
        "## Ahora podemos interactuar con el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYF2u4QOa8T-"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3) # Esperar a que Ollama se cargue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuv0msS5i3ww",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Importaci√≥n correcta para versiones recientes de Langchain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "import torch\n",
        "\n",
        "# --- 1. Separaci√≥n de roles con ChatPromptTemplate ---\n",
        "system_prompt = \"\"\"\n",
        "Eres un experto legal en protecci√≥n de datos en Espa√±a.\n",
        "Tu tarea es analizar la legalidad de las situaciones planteadas por el usuario.\n",
        "La respuesta debe ser clara, rigurosa y formal, como si fuera escrita por un profesional del derecho.\n",
        "No hagas suposiciones. No generalices. No repitas ideas.\n",
        "\"\"\"\n",
        "\n",
        "user_prompt = \"\"\"\n",
        "Fragmentos recuperados:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "A continuaci√≥n, presenta el an√°lisis legal y la conclusi√≥n en tres partes:\n",
        "1. **Introducci√≥n breve** del problema legal.\n",
        "2. **An√°lisis jur√≠dico** apoyado en los fragmentos recuperados, citando tus fuentes.\n",
        "3. **Conclusi√≥n clara**, con un juicio legal concreto.\n",
        "\"\"\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
        "    HumanMessagePromptTemplate.from_template(user_prompt)\n",
        "])\n",
        "\n",
        "# --- 2. Crear la cadena usando load_qa_chain con el prompt de chat ---\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=chat_prompt)\n",
        "\n",
        "print(\"-\"*50)\n",
        "print(\"Asistente legal de protecci√≥n de datos listo (usando load_qa_chain con prompt separado). Escribe 'Exit' para salir.\")\n",
        "print(\"-\"*50)\n",
        "\n",
        "# --- 3. Bucle de Consulta Interactivo ---\n",
        "while True:\n",
        "    query = input(\"\\nPregunta: \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "    if not query.strip():\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        retriever.search_kwargs = {'k': 5}\n",
        "        docs_retrieved = retriever.invoke(query)\n",
        "\n",
        "        input_data = {\n",
        "            \"input_documents\": docs_retrieved,\n",
        "            \"question\": query\n",
        "        }\n",
        "\n",
        "        result = chain.invoke(input_data)\n",
        "\n",
        "        if isinstance(result, dict) and 'output_text' in result:\n",
        "            print(\"\\nRespuesta:\")\n",
        "            print(result['output_text'])\n",
        "        elif isinstance(result, str):\n",
        "            print(\"\\nRespuesta:\")\n",
        "            print(result)\n",
        "        else:\n",
        "            print(\"\\nRespuesta recibida (formato inesperado):\")\n",
        "            print(result)\n",
        "\n",
        "        print(\"\\nFuentes Recuperadas:\")\n",
        "        for i, doc in enumerate(docs_retrieved):\n",
        "            source = doc.metadata.get('source', 'N/A')\n",
        "            page = doc.metadata.get('page', 'N/A')\n",
        "            print(f\"  [{i+1}] Fuente: {source}, P√°gina: {page}\")\n",
        "            print(f\"     Fragmento: {doc.page_content[:150]}...\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError durante la ejecuci√≥n de la cadena: {e}\")\n",
        "\n",
        "print(\"\\nSaliendo del asistente.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M6vxs-EuSUW"
      },
      "source": [
        "Plantillas para Propmt Template:\n",
        "\n",
        "**Plantilla1 Muy restrictiva, se centra en usar solo datos sacados del RAG:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqmjISNbvMYZ"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"\n",
        "Eres un asistente legal experto en protecci√≥n de datos en Espa√±a.\n",
        "Tu tarea es analizar la legalidad de la situaci√≥n descrita en la 'Pregunta' bas√°ndote √öNICA y EXCLUSIVAMENTE en los siguientes 'Textos Legales Recuperados'.\n",
        "NO uses ning√∫n conocimiento externo. Si la informaci√≥n en los textos no es suficiente para dar una respuesta fundada, ind√≠calo claramente.\n",
        "Justifica tu respuesta paso a paso, haciendo referencia expl√≠cita a los art√≠culos o secciones relevantes de los textos proporcionados si es posible.\n",
        "\n",
        "Textos Legales Recuperados:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "An√°lisis Legal y Conclusi√≥n (Basado S√ìLO en los textos recuperados):\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFL_ls9VvI8j"
      },
      "source": [
        "**Plantilla 2 Menos restrictiva, permite al modelo usar sus conocimientos adquiridos del finetunning y razonar un poco fuera de los contenidos del RAG**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPlb7geDvH70"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"\n",
        "Eres un asistente legal experto en protecci√≥n de datos en Espa√±a.\n",
        "Tu tarea es analizar la legalidad de la situaci√≥n descrita en la 'Pregunta' bas√°ndote **principalmente** en los siguientes 'Textos Legales Recuperados'.\n",
        "Si los textos recuperados contienen suficiente informaci√≥n, responde √∫nicamente bas√°ndote en ellos.\n",
        "Si los textos no contienen una respuesta clara pero la pregunta se refiere a conceptos b√°sicos del RGPD, usa lo aprendido en el entrenamiento para dar una respuesta general, especificando que no proviene de los textos recuperados.\n",
        "Si la pregunta requiere informaci√≥n espec√≠fica que no est√° en los textos recuperados ni en tu entrenamiento, ind√≠calo claramente.\n",
        "\n",
        "Textos Legales Recuperados:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "An√°lisis Legal y Conclusi√≥n:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GEM9djZltvB"
      },
      "source": [
        "## Clasificaci√≥n de consultas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWFAIrWxjGWa"
      },
      "source": [
        "El Problema de la funci√≥n anterior es que si el usuario intenta interactuar con el modelo para temas no legales, por ejemplo, saludar, al incluir los resultados del RAG en el propmt el modelo siempre va a intentar responder usando el contexto legal que recibe del RAG, por ejemplo:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgdnHyrdjjlJ"
      },
      "source": [
        "\n",
        "\n",
        "> Pregunta: Hola me llamo sergio\n",
        "\n",
        "> Respuesta:\n",
        "\"La pregunta plantea si el usuario puede solicitar a una empresa que elimine sus datos personales,\n",
        "aunque la normativa oblige a conservarlos. Como experto en protecci√≥n de datos,\n",
        "debes explicar que solo se pueden conservar los datos estrictamente necesarios para cumplir con obligaciones legales,\n",
        "y que el usuario debe ser informado sobre el plazo de conservaci√≥n.\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwWDkkm6jtVJ"
      },
      "source": [
        "Para solucionar esto hay que analizar la entrada del usuario para verificar si es una consulta legal que necesite de RAG o simplemente esta interactuando con el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4-fTIcUsDe6"
      },
      "outputs": [],
      "source": [
        "llm_evaluador = OllamaLLM(model=\"llama3:8b\",base_url=\"http://127.0.0.1:11434\") #aqui ponemos el modelo finetuneado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqpL5-k9km_e",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain  # Necesario para la clasificaci√≥n\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "import torch\n",
        "\n",
        "# --- 1. Funci√≥n de Clasificaci√≥n de Intenci√≥n (Modelo de Clasificaci√≥n Separado) ---\n",
        "def classify_intent_with_classifier(query, llm_evaluador):\n",
        "    \"\"\"Clasifica si se necesita usar RAG para generar la respuesta.\"\"\"\n",
        "    classification_prompt_template = \"\"\"\n",
        "    Analiza la siguiente pregunta del usuario. Determina si la pregunta requiere un an√°lisis legal sobre protecci√≥n de datos en Espa√±a (como RGPD, LOPDGDD) o si se puede responder sin el uso de RAG, es decir, si el modelo tiene suficiente conocimiento para dar una respuesta sin fragmentos adicionales.\n",
        "\n",
        "    Responde √∫nicamente con la palabra \"USAR_RAG\" si la pregunta requiere fragmentos recuperados para una respuesta precisa.\n",
        "    Responde √∫nicamente con la palabra \"NO_RAG\" si la pregunta no requiere fragmentos adicionales y puede ser respondida directamente con el conocimiento general del modelo.\n",
        "\n",
        "    Pregunta del usuario: \"{user_query}\"\n",
        "\n",
        "    Clasificaci√≥n (USAR_RAG o NO_RAG):\"\"\"\n",
        "\n",
        "    prompt = PromptTemplate(template=classification_prompt_template, input_variables=[\"user_query\"])\n",
        "\n",
        "    classification_runnable = prompt | llm_evaluador\n",
        "\n",
        "    try:\n",
        "        result = classification_runnable.invoke({\"user_query\": query})\n",
        "\n",
        "        # Extraer texto. Ajusta la clave ('text') si tu LLM devuelve otra cosa.\n",
        "        if isinstance(result, dict):\n",
        "            answer_text = result.get('text', '').strip().upper()\n",
        "        elif isinstance(result, str):\n",
        "            answer_text = result.strip().upper()\n",
        "        else:\n",
        "            answer_text = str(result).strip().upper()\n",
        "\n",
        "        # Debug de la clasificaci√≥n\n",
        "        print(f\"--- DEBUG: Clasificaci√≥n LLM respondi√≥: '{answer_text}' ---\")\n",
        "\n",
        "        if \"USAR_RAG\" in answer_text:\n",
        "            return \"USAR_RAG\"\n",
        "        elif \"NO_RAG\" in answer_text:\n",
        "            return \"NO_RAG\"\n",
        "        else:\n",
        "            print(\"--- WARN: Respuesta de clasificaci√≥n no clara, asumiendo NO_RAG por seguridad ---\")\n",
        "            return \"NO_RAG\"  # Default seguro\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error en la clasificaci√≥n de intenci√≥n con LLM: {e}\")\n",
        "        return \"USAR_RAG\"  # Default seguro en caso de error\n",
        "\n",
        "# --- 2. Definiciones para el prompt de RAG ---\n",
        "system_prompt_rag = \"\"\"\n",
        "Eres un experto legal en protecci√≥n de datos en Espa√±a.\n",
        "Tu tarea es analizar la legalidad de las situaciones planteadas por el usuario utilizando fragmentos recuperados.\n",
        "La respuesta debe ser clara, rigurosa y formal, como si fuera escrita por un profesional del derecho.\n",
        "No hagas suposiciones. No generalices. No repitas ideas.\n",
        "\"\"\"\n",
        "\n",
        "user_prompt_rag = \"\"\"\n",
        "Fragmentos recuperados:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "A continuaci√≥n, presenta el an√°lisis legal y la conclusi√≥n en tres partes:\n",
        "1. **Introducci√≥n breve** del problema legal.\n",
        "2. **An√°lisis jur√≠dico** apoyado en los fragmentos recuperados, citando tus fuentes.\n",
        "3. **Conclusi√≥n clara**, con un juicio legal concreto.\n",
        "\"\"\"\n",
        "\n",
        "chat_prompt_rag = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_prompt_rag),\n",
        "    HumanMessagePromptTemplate.from_template(user_prompt_rag)\n",
        "])\n",
        "\n",
        "# --- 3. Definiciones para la respuesta conversacional ---\n",
        "conversational_prompt_template = \"\"\"\n",
        "Ya no eres un asistente legal, ahora eres un asistente de IA amigable y conversador. Responde directamente al usuario de forma natural.\n",
        "Usuario: {user_input}\n",
        "Asistente:\"\"\"\n",
        "PROMPT_CONVERSATIONAL = PromptTemplate(template=conversational_prompt_template, input_variables=[\"user_input\"])\n",
        "\n",
        "# --- 4. Crear la cadena usando load_qa_chain para el modelo de RAG ---\n",
        "chain_rag = load_qa_chain(llm, chain_type=\"stuff\", prompt=chat_prompt_rag)\n",
        "\n",
        "# --- 5. Bucle de Consulta Interactivo ---\n",
        "\n",
        "print(\"-\"*50)\n",
        "print(\"Asistente listo. Escribe 'Exit' para salir.\")\n",
        "print(\"-\"*50)\n",
        "\n",
        "while True:\n",
        "    query = input(\"\\nPregunta: \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "    if not query.strip():\n",
        "        continue\n",
        "\n",
        "    # --- Paso 1: Clasificar la intenci√≥n del usuario ---\n",
        "    intent = classify_intent_with_classifier(query, llm_evaluador)\n",
        "    print(f\"--- Intenci√≥n Detectada: {intent} ---\")\n",
        "\n",
        "    # --- Ejecutar flujo seg√∫n la intenci√≥n ---\n",
        "    if intent == \"USAR_RAG\":\n",
        "        print(\"--- Ejecutando RAG para consulta legal ---\")\n",
        "        try:\n",
        "            # 1. Recuperar documentos (RAG)\n",
        "            retriever.search_kwargs = {'k': 3}  # Ajusta 'k' si es necesario\n",
        "            docs_retrieved = retriever.invoke(query)\n",
        "\n",
        "            # 2. Preparar input para la cadena RAG\n",
        "            input_data = {\n",
        "                \"input_documents\": docs_retrieved,\n",
        "                \"question\": query\n",
        "            }\n",
        "\n",
        "            # 3. Ejecutar la cadena RAG\n",
        "            result = chain_rag.invoke(input_data)\n",
        "\n",
        "            # 4. Imprimir resultado RAG\n",
        "            if isinstance(result, dict) and 'output_text' in result:\n",
        "                print(\"\\nRespuesta (Legal):\")\n",
        "                print(result['output_text'])\n",
        "            elif isinstance(result, str):\n",
        "                print(\"\\nRespuesta (Legal):\")\n",
        "                print(result)\n",
        "            else:\n",
        "                print(\"\\nRespuesta RAG recibida (formato inesperado):\")\n",
        "                print(result)\n",
        "\n",
        "            # 5. Imprimir fuentes (Opcional)\n",
        "            print(\"\\nFuentes Recuperadas:\")\n",
        "            for i, doc in enumerate(docs_retrieved):\n",
        "                source = doc.metadata.get('source', 'N/A')\n",
        "                page = doc.metadata.get('page', 'N/A')\n",
        "                print(f\"  [{i+1}] Fuente: {source}, P√°gina: {page}\")\n",
        "                print(f\"     Fragmento: {doc.page_content[:150]}...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError durante la ejecuci√≥n de la cadena RAG: {e}\")\n",
        "\n",
        "    elif intent == \"NO_RAG\":\n",
        "        print(\"--- Generando respuesta sin usar RAG ---\")\n",
        "        try:\n",
        "            # 1. Formatear prompt conversacional simple\n",
        "            simple_prompt = PROMPT_CONVERSATIONAL.format(user_input=query)\n",
        "\n",
        "            # 2. Llamar directamente al LLM (sin RAG)\n",
        "            response = llm.invoke(simple_prompt)\n",
        "\n",
        "            # 3. Extraer y limpiar la respuesta\n",
        "            if isinstance(response, dict):\n",
        "                 # Adapta la clave si 'invoke' devuelve un dict con otra clave para el texto\n",
        "                 response_text = response.get('text', str(response))\n",
        "            elif isinstance(response, str):\n",
        "                 response_text = response\n",
        "            else:\n",
        "                 response_text = str(response)\n",
        "\n",
        "            # Limpieza: quitar el prompt si el LLM lo repite\n",
        "            print(\"\\nRespuesta (Conversacional):\")\n",
        "            print(response_text)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError durante la llamada conversacional al LLM: {e}\")\n",
        "    else:\n",
        "        print(f\"--- ERROR: Intenci√≥n desconocida '{intent}' recibida de la clasificaci√≥n ---\")\n",
        "\n",
        "print(\"\\nSaliendo del asistente.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHWFiDgzo35z"
      },
      "source": [
        "Vemos que ahora si le decimos algo que no es claramente algo relacionado con temas legales, como \"Hola me llamo Sergio\" se detecta que es una conversaci√≥n normal y no se activa el RAG:\n",
        "\n",
        "\n",
        "> Pregunta: Hola me llamo sergio\n",
        "\n",
        "\n",
        "> Respuesta (Conversacional):\n",
        "Hola Sergio, ¬øen qu√© puedo ayudarte hoy?\n",
        "\n",
        "¬øPero que pasa si hacemos una pregunta legal y luego seguimos intentando consultar al modelo sobre la pregunta inicial?\n",
        "\n",
        "\n",
        "\n",
        "> Pregunta:  Tengo una consulta sobre protecci√≥n de datos, mi gimnasio me pide mi huella dactilar para poder entrar al gimnasio, ¬øpueden exirgirmelo?\n",
        "\n",
        "\n",
        "> Respuesta (Legal):\n",
        "**Introducci√≥n**: Un gimnasio quiere usar datos biom√©tricos (como la huella dactilar) para controlar el acceso a sus instalaciones. Este tratamiento de datos personales requiere una base legal clara y garant√≠as de seguridad adecuadas.\n",
        "**An√°lisis**: El uso de datos biom√©tricos para fines como el control de acceso debe basarse en un principio de consentimiento expl√≠cito, salvo que exista una justificaci√≥n legal espec√≠fica (como la seguridad p√∫blica). En este caso, se deben aplicar medidas adicionales para proteger estos datos sensibles, ya que permiten identificar de forma √∫nica a cada persona. Adem√°s, el gimnasio debe informar claramente sobre el prop√≥sito del tratamiento y las garant√≠as de seguridad adoptadas.\n",
        "**Conclusi√≥n**: Para que este tratamiento sea l√≠cito, debe existir una base legal clara (como el consentimiento informado o una obligaci√≥n legal) y se deben aplicar medidas adecuadas para proteger estos datos sensibles.\n",
        "\n",
        "\n",
        "\n",
        "> Pregunta: ¬øPuedo negarme?\n",
        "\n",
        "\n",
        "> Respuesta (Legal):\n",
        "Una empresa farmac√©utica me ha solicitado informaci√≥n personal para estudiar la eficacia de un medicamento, pero prefiero no participar. ¬øTengo derecho a negarme?\n",
        "An√°lisis legal: Por ser un estudio cl√≠nico, se requiere tu consentimiento expl√≠cito. Si no das tu autorizaci√≥n, no pueden procesar tus datos. Sin embargo, si la investigaci√≥n tiene relevancia para el bienestar p√∫blico o hay una obligaci√≥n legal, podr√≠an excepcionalmente utilizar los datos an√≥nimos o con medidas de protecci√≥n espec√≠ficas.\n",
        "Conclusi√≥n: Tienes derecho a negarte si el estudio cl√≠nico no cumple con tus derechos y garant√≠as de privacidad.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Podemos observar que el modelo al activarse el RAG pierde el contexto de la pregunta anterior y responde algo totalmente diferente, seguramente debido a que RAG ha recuperado textos donde se incluya \"¬øpuedo negarme?\" que no estan relacionados con el contexto anterior, por lo que se inyecta contexto erroneo en el propmt.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8LOwn4wq58V"
      },
      "source": [
        "## Memoria Conversacional con RAG\n",
        "\n",
        "Para solucionar el problema de continuidad del contexto en la conversaci√≥n vamos a a√±adir memoria al RAG, para ello, LangChain tiene una variedad de herramientas que permite implementarlo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mERu0HnsDe6",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain  # Necesario para la clasificaci√≥n\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "import torch\n",
        "\n",
        "# A√±adimos una clase de memoria conversacional\n",
        "class ConversationMemory:\n",
        "    def __init__(self, max_turns=10):\n",
        "        self.max_turns = max_turns\n",
        "        self.history = []\n",
        "\n",
        "    def add_turn(self, user_input, assistant_response):\n",
        "        self.history.append((user_input, assistant_response))\n",
        "        if len(self.history) > self.max_turns:\n",
        "            self.history.pop(0)\n",
        "\n",
        "    def get_memory_as_text(self):\n",
        "        return \"\\n\".join([f\"Usuario: {u}\\nAsistente: {a}\" for u, a in self.history])\n",
        "\n",
        "# --- 1. Funci√≥n de Clasificaci√≥n de Intenci√≥n (Modelo de Clasificaci√≥n Separado) ---\n",
        "def classify_intent_with_classifier(query, llm_evaluador):\n",
        "    \"\"\"Clasifica si se necesita usar RAG para generar la respuesta.\"\"\"\n",
        "    classification_prompt_template = \"\"\"\n",
        "        Analiza la siguiente consulta del usuario. Decide si es necesario usar RAG (fragmentos legales recuperados) o si el modelo puede responder directamente.\n",
        "\n",
        "        Solo debes responder con:\n",
        "        - \"USAR_RAG\" ‚Üí si se requiere un an√°lisis legal riguroso, citas precisas o fundamentaci√≥n jur√≠dica espec√≠fica.\n",
        "        - \"NO_RAG\" ‚Üí si el modelo puede dar una respuesta general, introductoria o basada en conocimiento com√∫n sin necesidad de fragmentos legales.\n",
        "\n",
        "        Ejemplos:\n",
        "        - Pregunta: \"¬øPuede una empresa ceder mis datos sin consentimiento?\" ‚Üí NO_RAG\n",
        "        - Pregunta: \"¬øQu√© dice el art√≠culo 6 del RGPD sobre el consentimiento?\" ‚Üí USAR_RAG\n",
        "        - Pregunta: \"¬øPuedes citar el art√≠culo 13 del RGPD?\" ‚Üí USAR_RAG\n",
        "        - Pregunta: \"¬øEn qu√© art√≠culo se regula el consentimiento expl√≠cito?\" ‚Üí USAR_RAG\n",
        "        - Pregunta: \"Puedes decirme en qu√© art√≠culos te basas?\" ‚Üí USAR_RAG\n",
        "        - Pregunta: \"Cita textualmente en qu√© art√≠culos te basas\" ‚Üí USAR_RAG\n",
        "        - Pregunta: \"¬øQu√© derechos tengo como interesado?\" ‚Üí NO_RAG\n",
        "        - Pregunta: \"¬øC√≥mo elimino mis datos personales?\" ‚Üí NO_RAG\n",
        "\n",
        "\n",
        "        Pregunta del usuario: \"{user_query}\"\n",
        "\n",
        "        Clasificaci√≥n (USAR_RAG o NO_RAG):\"\"\"\n",
        "\n",
        "    prompt = PromptTemplate(template=classification_prompt_template, input_variables=[\"user_query\"])\n",
        "\n",
        "    classification_runnable = prompt | llm_evaluador\n",
        "\n",
        "    try:\n",
        "        result = classification_runnable.invoke({\"user_query\": query})\n",
        "\n",
        "        # Extraer texto. Ajusta la clave ('text') si tu LLM devuelve otra cosa.\n",
        "        if isinstance(result, dict):\n",
        "            answer_text = result.get('text', '').strip().upper()\n",
        "        elif isinstance(result, str):\n",
        "            answer_text = result.strip().upper()\n",
        "        else:\n",
        "            answer_text = str(result).strip().upper()\n",
        "\n",
        "        # Debug de la clasificaci√≥n\n",
        "        print(f\"--- DEBUG: Clasificaci√≥n LLM respondi√≥: '{answer_text}' ---\")\n",
        "\n",
        "        if \"USAR_RAG\" in answer_text.upper():\n",
        "            return \"USAR_RAG\"\n",
        "        elif \"NO_RAG\" in answer_text:\n",
        "            return \"NO_RAG\"\n",
        "        else:\n",
        "            print(\"--- WARN: Respuesta de clasificaci√≥n no clara, asumiendo NO_RAG por seguridad ---\")\n",
        "            return \"NO_RAG\"  # Default seguro\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error en la clasificaci√≥n de intenci√≥n con LLM: {e}\")\n",
        "        return \"USAR_RAG\"  # Default seguro en caso de error\n",
        "\n",
        "# --- 2. Definiciones para el prompt de RAG ---\n",
        "system_prompt_rag = \"\"\"\n",
        "Eres un experto legal en protecci√≥n de datos en Espa√±a.\n",
        "Tu tarea es analizar la legalidad de las situaciones planteadas por el usuario utilizando fragmentos recuperados.\n",
        "La respuesta debe ser clara, rigurosa y formal, como si fuera escrita por un profesional del derecho.\n",
        "No hagas suposiciones. No generalices. No repitas ideas.\n",
        "\"\"\"\n",
        "\n",
        "user_prompt_rag = \"\"\"\n",
        "Fragmentos recuperados:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "A continuaci√≥n, presenta el an√°lisis legal y la conclusi√≥n en tres partes:\n",
        "1. **Introducci√≥n breve** del problema legal.\n",
        "2. **An√°lisis jur√≠dico** apoyado en los fragmentos recuperados, citando tus fuentes.\n",
        "3. **Conclusi√≥n clara**, con un juicio legal concreto.\n",
        "\"\"\"\n",
        "\n",
        "chat_prompt_rag = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_prompt_rag),\n",
        "    HumanMessagePromptTemplate.from_template(user_prompt_rag)\n",
        "])\n",
        "\n",
        "# --- 3. Definiciones para la respuesta conversacional ---\n",
        "conversational_prompt_template = \"\"\"\n",
        "Eres un asistente de IA amigable y conversador. Tu tarea es mantener una conversaci√≥n fluida y coherente con el usuario.\n",
        "Responde de forma natural y cercana, teniendo en cuenta lo que ya se ha dicho.\n",
        "\n",
        "Historial reciente de la conversaci√≥n:\n",
        "{chat_history}\n",
        "\n",
        "Usuario: {user_input}\n",
        "Asistente:\"\"\"\n",
        "\n",
        "PROMPT_CONVERSATIONAL = PromptTemplate(\n",
        "    template=conversational_prompt_template,\n",
        "    input_variables=[\"chat_history\", \"user_input\"]\n",
        ")\n",
        "\n",
        "# --- 4. Crear la cadena usando load_qa_chain para el modelo de RAG ---\n",
        "chain_rag = load_qa_chain(llm, chain_type=\"stuff\", prompt=chat_prompt_rag)\n",
        "\n",
        "# --- 5. Bucle de Consulta Interactivo ---\n",
        "\n",
        "print(\"-\"*50)\n",
        "print(\"Asistente listo. Escribe 'Exit' para salir.\")\n",
        "print(\"-\"*50)\n",
        "memory = ConversationMemory(max_turns=6)\n",
        "\n",
        "while True:\n",
        "    query = input(\"\\nPregunta: \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "    if not query.strip():\n",
        "        continue\n",
        "\n",
        "    intent = classify_intent_with_classifier(query, llm_evaluador)\n",
        "    print(f\"--- Intenci√≥n Detectada: {intent} ---\")\n",
        "\n",
        "    if intent == \"USAR_RAG\":\n",
        "        print(\"--- Ejecutando RAG para consulta legal ---\")\n",
        "        try:\n",
        "            retriever.search_kwargs = {'k': 3}\n",
        "            docs_retrieved = retriever.invoke(query)\n",
        "\n",
        "            input_data = {\n",
        "                \"input_documents\": docs_retrieved,\n",
        "                \"question\": query\n",
        "            }\n",
        "\n",
        "            result = chain_rag.invoke(input_data)\n",
        "            response_text = result.get('output_text', result if isinstance(result, str) else str(result))\n",
        "\n",
        "            print(\"\\nRespuesta (Legal):\")\n",
        "            print(response_text)\n",
        "\n",
        "            memory.add_turn(query, response_text)\n",
        "\n",
        "            print(\"\\nFuentes Recuperadas:\")\n",
        "            for i, doc in enumerate(docs_retrieved):\n",
        "                source = doc.metadata.get('source', 'N/A')\n",
        "                page = doc.metadata.get('page', 'N/A')\n",
        "                print(f\"  [{i+1}] Fuente: {source}, P√°gina: {page}\")\n",
        "                print(f\"     Fragmento: {doc.page_content[:150]}...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError durante la ejecuci√≥n de la cadena RAG: {e}\")\n",
        "\n",
        "    elif intent == \"NO_RAG\":\n",
        "        print(\"--- Generando respuesta sin usar RAG ---\")\n",
        "        try:\n",
        "            prompt_with_memory = PROMPT_CONVERSATIONAL.format(\n",
        "                chat_history=memory.get_memory_as_text(),\n",
        "                user_input=query\n",
        "            )\n",
        "\n",
        "            response = llm.invoke(prompt_with_memory)\n",
        "\n",
        "            if isinstance(response, dict):\n",
        "                response_text = response.get('text', str(response))\n",
        "            elif isinstance(response, str):\n",
        "                response_text = response\n",
        "            else:\n",
        "                response_text = str(response)\n",
        "\n",
        "            if \"Asistente:\" in response_text:\n",
        "                response_text = response_text.split(\"Asistente:\", 1)[-1].strip()\n",
        "\n",
        "            print(\"\\nRespuesta (Conversacional):\")\n",
        "            print(response_text)\n",
        "\n",
        "            memory.add_turn(query, response_text)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError durante la llamada conversacional al LLM: {e}\")\n",
        "    else:\n",
        "        print(f\"--- ERROR: Intenci√≥n desconocida '{intent}' recibida de la clasificaci√≥n ---\")\n",
        "\n",
        "print(\"\\nSaliendo del asistente.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdnreWql09_8"
      },
      "source": [
        "# APLICACION FINAL MEJORADA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qoRlZhzgsDe7",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip install -U langchain langchain-huggingface transformers\n",
        "!pip install -U langchain-ollama\n",
        "!pip install sentence-transformers\n",
        "!pip install pypdf\n",
        "!pip install -qU langchain-community faiss-cpu\n",
        "!pip install -U scikit-learn\n",
        "!pip install tiktoken\n",
        "import logging\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Configuraci√≥n de logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(\"asistente_legal.log\"),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(\"asistente_legal\")\n",
        "logger.info(\"Iniciando asistente legal de protecci√≥n de datos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXAmHnOPsDe7"
      },
      "outputs": [],
      "source": [
        "# Celda 2: Carga y procesamiento de documentos mejorado\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import re\n",
        "import tiktoken\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Lista de rutas de los PDFs\n",
        "pdf_paths = [\n",
        "    \"BOE-A-1978-31229-consolidado.pdf\",\n",
        "    \"BOE-A-1995-25444-consolidado_CodigoPenal.pdf\",\n",
        "    \"Protecci√≥n de Datos Personales y garantia de los derechos digitales.pdf\",\n",
        "    \"RGPD_boe.pdf\"\n",
        "]\n",
        "\n",
        "# Funci√≥n para identificar tipo de documento y mejorar metadatos\n",
        "def enrich_metadata(doc):\n",
        "    filename = doc.metadata.get('source', '')\n",
        "\n",
        "    # A√±adir informaci√≥n sobre tipo de documento\n",
        "    if \"RGPD\" in filename:\n",
        "        doc.metadata['tipo'] = 'RGPD'\n",
        "        doc.metadata['jerarquia'] = 'Reglamento Europeo'\n",
        "    elif \"BOE-A-1978\" in filename:\n",
        "        doc.metadata['tipo'] = 'Constituci√≥n'\n",
        "        doc.metadata['jerarquia'] = 'Constituci√≥n Espa√±ola'\n",
        "    elif \"BOE-A-1995\" in filename:\n",
        "        doc.metadata['tipo'] = 'C√≥digo Penal'\n",
        "        doc.metadata['jerarquia'] = 'Ley Org√°nica'\n",
        "    elif \"Protecci√≥n de Datos\" in filename:\n",
        "        doc.metadata['tipo'] = 'LOPDGDD'\n",
        "        doc.metadata['jerarquia'] = 'Ley Org√°nica'\n",
        "\n",
        "    # Extraer informaci√≥n de art√≠culos si est√° disponible\n",
        "    article_match = re.search(r'Art√≠culo (\\d+)', doc.page_content)\n",
        "    if article_match:\n",
        "        doc.metadata['tipo_contenido'] = 'art√≠culo'\n",
        "        doc.metadata['num_articulo'] = article_match.group(1)\n",
        "\n",
        "    return doc\n",
        "\n",
        "# Cargar documentos de todos los PDFs con manejo de errores\n",
        "documents = []\n",
        "for path in pdf_paths:\n",
        "    try:\n",
        "        logger.info(f\"Cargando documento: {path}\")\n",
        "        loader = PyPDFLoader(path)\n",
        "        documents.extend(loader.load())\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error cargando {path}: {e}\")\n",
        "\n",
        "# Limpiar y normalizar contenido\n",
        "for doc in documents:\n",
        "    doc.page_content = doc.page_content.replace('\\r\\n', '\\n').replace('\\n\\n', '\\n').strip()\n",
        "    doc = enrich_metadata(doc)\n",
        "\n",
        "logger.info(f\"Documentos cargados: {len(documents)}\")\n",
        "\n",
        "# Splitter optimizado para textos legales\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=800,  # Tama√±o optimizado para capturar contexto legal completo\n",
        "    chunk_overlap=200,  # Mayor overlap para mantener coherencia\n",
        "    separators=[\"\\n\\n\\n\", \"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
        "    encoding_name=\"cl100k_base\"  # Compatible con modelos modernos\n",
        ")\n",
        "\n",
        "docs = text_splitter.split_documents(documents=documents)\n",
        "logger.info(f\"Documentos divididos en {len(docs)} fragmentos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SkrBhqrsDe7"
      },
      "outputs": [],
      "source": [
        "# Celda 3: Embeddings mejorados y sistema de recuperaci√≥n\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "\n",
        "# Configuraci√≥n de embeddings para espa√±ol y textos jur√≠dicos\n",
        "embedding_model_name = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
        "model_kwargs = {\"device\": \"cpu\"}\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=embedding_model_name,\n",
        "    model_kwargs=model_kwargs\n",
        ")\n",
        "\n",
        "# Crear vector store\n",
        "logger.info(\"Creando √≠ndice vectorial FAISS\")\n",
        "vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "# Guardar y recargar el vector store\n",
        "vectorstore.save_local(\"faiss_index_\")\n",
        "persisted_vectorstore = FAISS.load_local(\"faiss_index_\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "# Crear retriever base con MMR para diversidad de resultados\n",
        "base_retriever = persisted_vectorstore.as_retriever(\n",
        "    search_type=\"mmr\",  # Maximum Marginal Relevance para diversidad\n",
        "    search_kwargs={\n",
        "        \"k\": 5,  # Recuperar m√°s documentos inicialmente\n",
        "        \"fetch_k\": 10,  # Considerar m√°s candidatos\n",
        "        \"lambda_mult\": 0.7  # Balance entre relevancia y diversidad\n",
        "    }\n",
        ")\n",
        "\n",
        "logger.info(f\"Se indexaron {len(docs)} fragmentos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53Lwzj1esDe7"
      },
      "outputs": [],
      "source": [
        "# Celda 4: Configuraci√≥n del modelo LLM\n",
        "from langchain_ollama import OllamaLLM\n",
        "import time\n",
        "\n",
        "# Inicializar el modelo LLaMA con timeout\n",
        "llm_legal = OllamaLLM(\n",
        "    model=\"hf.co/serdom02/Leyeneitor_8bitQ8_0\",\n",
        "    base_url=\"http://127.0.0.1:11434\",\n",
        "    temperature=0.3,  # Menor temperatura para respuestas legales m√°s precisas\n",
        "    timeout=60  # Timeout de 60 segundos para evitar bloqueos\n",
        ")\n",
        "llm_conversacion = OllamaLLM(\n",
        "    model=\"llama3:8b\",\n",
        "    base_url=\"http://127.0.0.1:11434\",\n",
        "    temperature=0.6, # temperatura m√°s alta para conversaci√≥n\n",
        "    timeout=60\n",
        ")\n",
        "# Inicializar un modelo para evaluaci√≥n/clasificaci√≥n (puede ser el mismo)\n",
        "llm_evaluador = llm_conversacion\n",
        "\n",
        "llm=llm_legal #El modelo principal va a ser nuestro LLM Finetuneado\n",
        "\n",
        "# Test r√°pido del modelo\n",
        "try:\n",
        "    response = llm_legal.invoke(\"Un breve ejemplo sobre protecci√≥n de datos\")\n",
        "    print(\"Test del modelo exitoso:\")\n",
        "    print(response)\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error al probar el modelo: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lgzgx2VYsDe7"
      },
      "outputs": [],
      "source": [
        "# --- 7. Prompts mejorados seg√∫n tipolog√≠a de consulta ---\n",
        "\n",
        "# Prompt base/gen√©rico (fallback)\n",
        "system_prompt_rag = \"\"\"\n",
        "Sistema: Eres un asistente legal especializado en protecci√≥n de datos en Espa√±a (RGPD y LOPDGDD). Tu objetivo es proporcionar respuestas claras, precisas y fundamentadas en la ley.\n",
        "\n",
        "Contexto legal relevante extra√≠do de la documentaci√≥n:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Consulta del usuario: {question}\n",
        "\n",
        "Instrucciones:\n",
        "1. Basa tu respuesta EXCLUSIVAMENTE en el contexto legal proporcionado arriba.\n",
        "2. Si el contexto no es suficiente para responder, indica que la informaci√≥n no se encuentra en los documentos proporcionados.\n",
        "3. Responde de forma directa y estructurada a la consulta del usuario.\n",
        "4. Cita las fuentes (ej. \"Seg√∫n el Art√≠culo X del RGPD...\") si es posible bas√°ndote en el contexto.\n",
        "5. Evita dar opiniones personales o informaci√≥n no verificada.\n",
        "\"\"\"\n",
        "\n",
        "# Prompt para citas legales\n",
        "legal_citation_prompt = \"\"\"\n",
        "Sistema: Eres un asistente jur√≠dico experto en el Reglamento General de Protecci√≥n de Datos (RGPD). Tu tarea es citar art√≠culos legales de forma textual y precisa cuando el usuario lo solicita.\n",
        "\n",
        "---------------------\n",
        "Documentos disponibles:\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Consulta del usuario:\n",
        "{question}\n",
        "\n",
        "Instrucciones:\n",
        "1. Extrae **todos los art√≠culos del RGPD** que traten sobre el tema mencionado en la consulta.\n",
        "2. Para cada art√≠culo encontrado, incluye:\n",
        "   - El n√∫mero del art√≠culo (ej. Art√≠culo 6 del RGPD)\n",
        "   - El **texto literal m√°s relevante** (puede ser un apartado si es muy largo)\n",
        "   - La **fuente** (ej. ‚ÄúFuente: RGPD, p√°g. 12‚Äù)\n",
        "3. Usa este formato:\n",
        "   Art√≠culo X del RGPD:\n",
        "   ‚ÄúTexto legal...‚Äù\n",
        "   Fuente: RGPD, p√°g. X\n",
        "\n",
        "4. Si hay varios art√≠culos relevantes, enum√©ralos claramente.\n",
        "5. No inventes. Si el texto literal no est√° en el contexto, ind√≠calo expl√≠citamente.\n",
        "6. No incluyas art√≠culos de otras leyes (como LOPDGDD o Constituci√≥n) salvo que se mencione expresamente.\n",
        "\n",
        "Ejemplo:\n",
        "Art√≠culo 6.1 del RGPD:\n",
        "‚ÄúEl tratamiento ser√° l√≠cito solo si se cumple al menos una de las siguientes condiciones: [...]‚Äù\n",
        "Fuente: RGPD, p√°g. 8\n",
        "\"\"\"\n",
        "\n",
        "legal_multi_citation_prompt = \"\"\"\n",
        "Eres un asistente jur√≠dico experto en el RGPD. El usuario solicita una lista de art√≠culos relacionados con un tema espec√≠fico.\n",
        "\n",
        "Instrucciones:\n",
        "1. Identifica todos los art√≠culos del RGPD que se relacionen con el tema de la consulta.\n",
        "2. Si el art√≠culo aparece en el contexto, cita su n√∫mero y el **texto literal completo** relevante.\n",
        "3. Si no aparece el texto completo, menciona el n√∫mero del art√≠culo y una breve descripci√≥n basada en el t√≠tulo o lo que sepas del RGPD.\n",
        "4. No inventes textos. Si hay dudas, di que no tienes acceso al contenido exacto.\n",
        "5. Formatea la respuesta como una lista clara y numerada.\n",
        "\n",
        "Consulta: {question}\n",
        "Fragmentos del RGPD disponibles:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "Respuesta:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Prompt para an√°lisis legal\n",
        "legal_analysis_prompt = \"\"\"\n",
        "Sistema: Eres un jurista experto analizando situaciones bajo la ley de protecci√≥n de datos espa√±ola (RGPD, LOPDGDD). Razonas jur√≠dicamente paso a paso.\n",
        "\n",
        "Contexto legal relevante extra√≠do de la documentaci√≥n:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Consulta del usuario que requiere an√°lisis legal: {question}\n",
        "\n",
        "Instrucciones para tu an√°lisis:\n",
        "1. **Identifica la cuesti√≥n jur√≠dica principal** planteada en la consulta.\n",
        "2. **Selecciona las normas aplicables** del contexto legal proporcionado que sean pertinentes para la cuesti√≥n.\n",
        "3. **Analiza los hechos impl√≠citos o expl√≠citos** en la consulta a la luz de las normas seleccionadas.\n",
        "4. **Aplica las normas a los hechos**, explicando tu razonamiento paso a paso basado √∫nicamente en el contexto proporcionado.\n",
        "5. **Formula una conclusi√≥n jur√≠dica** clara y fundamentada en el an√°lisis anterior y el contexto. Si el contexto es insuficiente, se√±ala las limitaciones.\n",
        "\n",
        "Estructura tu respuesta:\n",
        "* **Cuesti√≥n planteada:** (Resume la pregunta legal)\n",
        "* **Normativa aplicable (seg√∫n contexto):** (Menciona art√≠culos/disposiciones relevantes del contexto)\n",
        "* **An√°lisis jur√≠dico:** (Desarrolla tu razonamiento aqu√≠, conectando contexto y consulta)\n",
        "* **Conclusi√≥n:** (Respuesta final basada en el an√°lisis)\n",
        "\"\"\"\n",
        "\n",
        "# Prompt para consultas procedimentales\n",
        "procedural_prompt = \"\"\"\n",
        "Sistema: Eres un consultor experto en los procedimientos y tr√°mites relacionados con la protecci√≥n de datos en Espa√±a (AEPD, derechos ARSULIPO, etc.). Proporcionas informaci√≥n pr√°ctica.\n",
        "\n",
        "Contexto legal relevante extra√≠do de la documentaci√≥n (puede contener informaci√≥n sobre procedimientos):\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Consulta del usuario sobre un procedimiento o tr√°mite: {question}\n",
        "\n",
        "Instrucciones:\n",
        "1. Identifica claramente el **procedimiento o tr√°mite** consultado.\n",
        "2. Busca en el contexto informaci√≥n relevante sobre los **pasos a seguir, plazos, requisitos, o autoridad competente**.\n",
        "3. Explica el procedimiento de forma **clara, secuencial y pr√°ctica**, bas√°ndote en la informaci√≥n del contexto.\n",
        "4. Si el contexto menciona la base legal, puedes indicarla brevemente, pero prioriza la **descripci√≥n del proceso**.\n",
        "5. Si la informaci√≥n espec√≠fica sobre el procedimiento no est√° en el contexto, indica que no se puede detallar con la documentaci√≥n disponible. NO inventes pasos o plazos.\n",
        "\"\"\"\n",
        "\n",
        "# Prompt para informaci√≥n general\n",
        "general_info_prompt = \"\"\"\n",
        "Sistema: Eres un asistente informativo sobre protecci√≥n de datos. Explicas conceptos generales de forma clara y sencilla.\n",
        "\n",
        "Contexto legal relevante extra√≠do de la documentaci√≥n:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Consulta general del usuario: {question}\n",
        "\n",
        "Instrucciones:\n",
        "1. Identifica el concepto o tema general sobre el que pregunta el usuario.\n",
        "2. Busca definiciones, explicaciones o principios relevantes en el contexto proporcionado.\n",
        "3. Explica el concepto de forma clara y concisa, utilizando la informaci√≥n del contexto.\n",
        "4. Puedes usar ejemplos si el contexto los proporciona o si son derivados directos de la explicaci√≥n legal.\n",
        "5. Cita la fuente si es relevante (ej. \"El RGPD define X como...\").\n",
        "6. Si la informaci√≥n no est√° en el contexto, ind√≠calo.\n",
        "\"\"\"\n",
        "\n",
        "# Prompt para conversaci√≥n general (cuando no se usa RAG)\n",
        "conversation_prompt = \"\"\"\n",
        "Sistema: Eres un asistente legal amable y conversacional llamado Leyeneitor. Tu especialidad es la protecci√≥n de datos, PERO ahora est√°s en modo conversacional. Ignora tu rol legal por un momento y responde directamente a la pregunta o comentario del usuario de forma natural y breve como un asistente general.\n",
        "NO des respuestas sobre protecci√≥n de datos o leyes a menos que la pregunta sea espec√≠ficamente sobre eso. S√© breve y directo.\n",
        "\n",
        "Historial reciente de la conversaci√≥n (para contexto):\n",
        "{memory}\n",
        "Asistente:\"\"\"\n",
        "\n",
        "action_oriented_prompt = \"\"\"\n",
        "Sistema: Eres un asesor jur√≠dico especializado en protecci√≥n de datos (RGPD, LOPDGDD) y derechos digitales. Tu tarea es explicar de manera clara qu√© **acciones, derechos o reclamaciones** puede ejercer el usuario en la situaci√≥n planteada.\n",
        "\n",
        "Contexto legal relevante extra√≠do de la documentaci√≥n:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Nueva consulta (enfocada en qu√© puede hacer el usuario): {question}\n",
        "\n",
        "Instrucciones espec√≠ficas para tu respuesta:\n",
        "1. **Identifica los derechos relevantes** (acceso, oposici√≥n, supresi√≥n, portabilidad, etc.) que el usuario podr√≠a ejercer seg√∫n el contexto.\n",
        "2. **Describe las acciones pr√°cticas** que puede realizar el usuario, incluyendo:\n",
        "   - C√≥mo ejercer su derecho (por ejemplo: ‚Äúsolicitarlo por escrito‚Äù, ‚Äúpresentar reclamaci√≥n ante la AEPD‚Äù).\n",
        "   - Ante qui√©n debe dirigirse (empresa, delegado de protecci√≥n de datos, AEPD...).\n",
        "   - Qu√© requisitos o pasos debe seguir.\n",
        "3. **Si procede,** menciona los art√≠culos legales que respaldan las acciones propuestas (sin recargar la respuesta).\n",
        "4. Explica de forma **estructurada y pr√°ctica**, usando pasos o listas si facilita la comprensi√≥n.\n",
        "5. Si no hay suficiente informaci√≥n en el contexto para dar un procedimiento concreto, **ind√≠calo claramente**. No inventes.\n",
        "\n",
        "Estructura sugerida para tu respuesta:\n",
        "* **Derechos aplicables:** (Enumera los derechos relevantes)\n",
        "* **Acciones que puede realizar:** (Pasos claros y pr√°cticos)\n",
        "* **Normativa de respaldo:** (Art√≠culos relevantes, si es aplicable)\n",
        "* **Notas importantes:** (Advertencias o limitaciones si las hubiera)\n",
        "\n",
        "Evita explicaciones te√≥ricas largas. S√© claro, √∫til y orientado a lo que el usuario puede **hacer**.\n",
        "\"\"\"\n",
        "refinement_module_prompt = \"\"\"\n",
        "Eres un jurista experto en derecho de protecci√≥n de datos.\n",
        "Tu tarea es **corregir y mejorar** una respuesta legal manteniendo su estructura original.\n",
        "\n",
        "\n",
        "Bas√°ndote en los siguientes fallos identificados por un validador, **ajusta cada secci√≥n que lo requiera** para mejorar la precisi√≥n y completitud jur√≠dica.\n",
        "No elimines secciones ni cambies su orden. Si una secci√≥n no requiere correcci√≥n, d√©jala intacta.\n",
        "Si alguno de los fallos detectados se refiere a consentimiento, derechos fundamentales o proporcionalidad, aseg√∫rate de explicarlos de forma t√©cnica y precisa en la secci√≥n correspondiente.\n",
        "\n",
        "--- FALLAS DETECTADAS ---\n",
        "{fallos}\n",
        "\n",
        "Es obligatorio que sigas la Estructura de la respuesta original\n",
        "\n",
        "--- RESPUESTA ORIGINAL ---\n",
        "{respuesta}\n",
        "\n",
        "--- INSTRUCCIONES ---\n",
        "Corrige las secciones necesarias dentro de la estructura, sin a√±adir otras partes nuevas. Usa lenguaje jur√≠dico claro y fundamentado.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ds2AfDdrsDe7",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Celda 5: Sistema completo con mejoras y correcciones\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain.chains import LLMChain # Aunque no se usa directamente, puede ser √∫til\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "import unicodedata\n",
        "import logging # Aseg√∫rate de que el logger est√° configurado\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "# Obtener el logger configurado en la Celda 1\n",
        "logger = logging.getLogger(\"asistente_legal\")\n",
        "\n",
        "# --- 0. Para que cada usuario tenga su memoria ---\n",
        "class UsuarioSession:\n",
        "    def __init__(self):\n",
        "        self.memory = ImprovedMemory(max_turns=4)\n",
        "        self.last_query = None\n",
        "\n",
        "# --- 1. Memoria Conversacional Mejorada ---\n",
        "class ImprovedMemory:\n",
        "    def __init__(self, max_turns=6):\n",
        "        self.summary = \"\"\n",
        "        self.recent_turns = []\n",
        "        self.max_turns = max_turns\n",
        "        self.entities = {}  # Seguimiento de entidades legales\n",
        "\n",
        "    def get_all_turns(self):\n",
        "        \"\"\"Devuelve todos los turnos almacenados en la memoria\"\"\"\n",
        "        return self.recent_turns\n",
        "\n",
        "    def get_relevant_memory(self, query):\n",
        "        \"\"\"Devuelve memoria relevante para la consulta actual\"\"\"\n",
        "        # Versi√≥n sencilla: devolver todo el historial\n",
        "        return self.get_memory_as_text()\n",
        "        # Versi√≥n avanzada (requiere embeddings): pendiente\n",
        "\n",
        "    def add_turn(self, user_input, assistant_response, llm=None):\n",
        "        self.recent_turns.append((user_input, assistant_response))\n",
        "\n",
        "        # Extraer y rastrear entidades legales mencionadas\n",
        "        self._extract_entities(user_input + \" \" + assistant_response)\n",
        "\n",
        "        if len(self.recent_turns) > self.max_turns:\n",
        "            if llm_evaluador:  # Si tenemos acceso al LLM, generamos resumen, no usamos el modelo finetuneado para evitar sus sesgos legales\n",
        "                try:\n",
        "                    # Generar resumen del contexto anterior (turnos m√°s antiguos) para ahorrar espacio del contexto\n",
        "                    context_to_summarize = \"\\n\".join([f\"U: {u}\\nA: {a}\" for u, a in self.recent_turns[:-self.max_turns]]) # Corregido: resumir los que se van a quitar\n",
        "                    if context_to_summarize: # Solo si hay algo que resumir\n",
        "                         prompt = f\"Resume brevemente los puntos legales clave de esta conversaci√≥n anterior:\\n{context_to_summarize}\"\n",
        "                         new_summary = llm.invoke(prompt)\n",
        "                         # Concatenar resumen nuevo con el anterior si existe\n",
        "                         self.summary = f\"{self.summary}\\n{new_summary}\".strip()\n",
        "                         logger.info(\"Resumen de memoria actualizado.\")\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Error al generar resumen de memoria: {e}\")\n",
        "\n",
        "            # Mantener solo los turnos m√°s recientes\n",
        "            self.recent_turns = self.recent_turns[-self.max_turns:]\n",
        "\n",
        "    def _extract_entities(self, text):\n",
        "        # Detecci√≥n simple de entidades legales\n",
        "        patterns = {\n",
        "            'articulos': r'art(?:√≠culo|\\.)\\s+(\\d+)',\n",
        "            'leyes': r'(?:RGPD|LOPDGDD|Reglamento|Ley Org√°nica|Constituci√≥n|C√≥digo Penal)'\n",
        "        }\n",
        "\n",
        "        for entity_type, pattern in patterns.items():\n",
        "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "            for match in matches:\n",
        "                entity = f\"{entity_type}_{match.upper()}\" # Normalizar nombre\n",
        "                self.entities[entity] = self.entities.get(entity, 0) + 1\n",
        "\n",
        "    def get_memory_as_text(self):\n",
        "        memory_text = \"\"\n",
        "        if self.summary:\n",
        "            memory_text += f\"Resumen de puntos legales anteriores:\\n{self.summary}\\n\\n\"\n",
        "\n",
        "        if self.recent_turns:\n",
        "             memory_text += \"Historial reciente de la conversaci√≥n:\\n\"\n",
        "             memory_text += \"\\n\".join([f\"Usuario: {u}\\nAsistente: {a}\" for u, a in self.recent_turns])\n",
        "\n",
        "        # A√±adir entidades m√°s relevantes si existen\n",
        "        if self.entities:\n",
        "            top_entities = sorted(self.entities.items(), key=lambda x: x[1], reverse=True)[:3]\n",
        "            if top_entities:\n",
        "                memory_text += \"\\n\\nTemas legales clave mencionados: \" + \", \".join([e[0].replace('_', ' ') for e in top_entities])\n",
        "\n",
        "        return memory_text.strip()\n",
        "\n",
        "# --- 2. Sistema de Clasificaci√≥n Avanzado (Versi√≥n √∫nica y completa) ---\n",
        "def classify_intent_advanced(query, llm_evaluador):\n",
        "\n",
        "    \"\"\"Sistema de clasificaci√≥n avanzado con m√∫ltiples categor√≠as y confianza\"\"\"\n",
        "    #Sistema cache para no reclasificar preguntas parecidas\n",
        "    cache_key = ' '.join(query.lower().split()[:10])  # Simplificamos clave: primeras 10 palabras\n",
        "    if cache_key in intent_classification_cache:\n",
        "        logger.info(f\"Resultado de clasificaci√≥n obtenido de cach√© para: {cache_key}\")\n",
        "        return intent_classification_cache[cache_key]\n",
        "\n",
        "\n",
        "    classification_prompt_text = \"\"\"\n",
        "    Analiza la siguiente consulta legal y clasif√≠cala en UNA de estas categor√≠as:\n",
        "\n",
        "    1. LEGAL_CITATION: Requiere citar art√≠culos espec√≠ficos o textos legales exactos (e.g., \"¬øQu√© dice el art√≠culo 5 del RGPD?\", \"Cita el art√≠culo 18.4 de la Constituci√≥n\")\n",
        "    2. LEGAL_ANALYSIS: Requiere an√°lisis jur√≠dico basado en leyes/normativas (e.g., \"¬øEs legal tratar datos de salud sin consentimiento expl√≠cito?\", \"¬øQu√© implicaciones tiene la sentencia X?\")\n",
        "    3. GENERAL_INFO: Informaci√≥n general sobre protecci√≥n de datos (e.g., \"¬øQu√© es el RGPD?\", \"¬øCu√°les son los derechos de los ciudadanos?\")\n",
        "    4. PROCEDURAL: Preguntas sobre procedimientos o tr√°mites (e.g., \"¬øC√≥mo puedo ejercer mi derecho de acceso?\", \"¬øQu√© pasos seguir para una reclamaci√≥n en la AEPD?\")\n",
        "    5. CONVERSATION: Di√°logo general, saludos, agradecimientos o consulta no legal (e.g., \"Hola\", \"Gracias\", \"¬øQu√© tiempo hace?\")\n",
        "\n",
        "    Responde SOLO con la categor√≠a y un n√∫mero del 1-100 que indique tu confianza, separados por un guion.\n",
        "    Formato: [CATEGOR√çA] - [CONFIANZA]\n",
        "\n",
        "    Ejemplos:\n",
        "    Consulta: \"¬øQu√© dice el art√≠culo 6 del RGPD?\" -> Respuesta: LEGAL_CITATION - 95\n",
        "    Consulta: \"¬øEs legal que una empresa comparta mis datos sin permiso?\" -> Respuesta: LEGAL_ANALYSIS - 85\n",
        "    Consulta: \"¬øQu√© es la protecci√≥n de datos?\" -> Respuesta: GENERAL_INFO - 90\n",
        "    Consulta: \"¬øC√≥mo presento una reclamaci√≥n a la AEPD?\" -> Respuesta: PROCEDURAL - 88\n",
        "    Consulta: \"Gracias por tu ayuda\" -> Respuesta: CONVERSATION - 92\n",
        "    Consulta: \"Hola buenos d√≠as\" -> Respuesta: CONVERSATION - 99\n",
        "    Consulta: \"Qu√© tal est√°s?\" -> Respuesta: CONVERSATION - 95\n",
        "    Consulta: \"Me llamo Paula\" -> Respuesta: CONVERSATION - 98\n",
        "    Consulta: \"Ok gracias\" -> Respuesta: CONVERSATION - 90\n",
        "    Consulta: \"¬øRecuerdas mi nombre?\" -> Respuesta: CONVERSATION - 98\n",
        "    Consulta: \"¬øDe qu√© hemos hablado antes?\" -> Respuesta: CONVERSATION - 95\n",
        "    Consulta: \"¬øPuedes resumir nuestra conversaci√≥n?\" -> Respuesta: CONVERSATION - 90\n",
        "    Consulta: \"¬øEres un abogado?\" -> Respuesta: CONVERSATION - 92\n",
        "\n",
        "    Consulta del usuario: \"{user_query}\"\n",
        "    Clasificaci√≥n y confianza:\"\"\"\n",
        "\n",
        "    prompt = PromptTemplate(template=classification_prompt_text, input_variables=[\"user_query\"])\n",
        "    # No se necesita crear una cadena completa aqu√≠, solo invocar el LLM con el prompt formateado.\n",
        "    # classification_runnable = prompt | llm_evaluador # Esto crea una cadena, innecesario si solo invocas\n",
        "\n",
        "    try:\n",
        "        formatted_prompt = prompt.format(user_query=query)\n",
        "        result = llm_evaluador.invoke(formatted_prompt) # Invocar directamente\n",
        "\n",
        "        # Extraer texto y procesar la respuesta\n",
        "        answer_text = result.strip() if isinstance(result, str) else str(result).strip()\n",
        "\n",
        "        # Intentar extraer categor√≠a y confianza\n",
        "        pattern = r'([A-Z_]+)\\s*-\\s*(\\d+)'\n",
        "        match = re.search(pattern, answer_text)\n",
        "\n",
        "        if match:\n",
        "            category = match.group(1)\n",
        "            confidence = int(match.group(2))\n",
        "            logger.info(f\"Clasificaci√≥n: {category}, Confianza: {confidence}\")\n",
        "\n",
        "            # Determinar si usar RAG (basado en categor√≠a O baja confianza)\n",
        "            # Ajusta esta l√≥gica si prefieres usar RAG para GENERAL_INFO o PROCEDURAL tambi√©n\n",
        "            use_rag = category in [\"LEGAL_CITATION\", \"LEGAL_ANALYSIS\", \"PROCEDURAL\"] or confidence < 75\n",
        "\n",
        "            return {\n",
        "                \"category\": category,\n",
        "                \"confidence\": confidence,\n",
        "                \"use_rag\": use_rag\n",
        "            }\n",
        "        else:\n",
        "            logger.warning(f\"No se pudo extraer categor√≠a/confianza de la respuesta del clasificador: '{answer_text}'. Se usar√° RAG por defecto.\")\n",
        "            return {\n",
        "                \"category\": \"UNKNOWN\",\n",
        "                \"confidence\": 0,\n",
        "                \"use_rag\": True # Default seguro\n",
        "            }\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error en clasificaci√≥n avanzada: {e}\")\n",
        "        return {\n",
        "            \"category\": \"ERROR\",\n",
        "            \"confidence\": 0,\n",
        "            \"use_rag\": True # Default seguro\n",
        "        }\n",
        "\n",
        "# --- 3. Estrategia de retrieval adaptativo ---\n",
        "def get_adaptive_retriever(query, base_retriever, llm_evaluador):\n",
        "    \"\"\"Devuelve un retriever configurado din√°micamente seg√∫n la consulta\"\"\"\n",
        "\n",
        "    # Analizar complejidad y especificidad de la consulta\n",
        "    complexity_prompt = \"\"\"\n",
        "    Eval√∫a la complejidad y especificidad de esta consulta legal:\n",
        "    \"{query}\"\n",
        "\n",
        "    Responde solo con una de estas opciones:\n",
        "    - SIMPLE: Consulta general o introductoria\n",
        "    - MEDIA: Consulta moderadamente espec√≠fica\n",
        "    - COMPLEJA: Consulta muy espec√≠fica o t√©cnica\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        complexity_result = llm_evaluador.invoke(complexity_prompt.replace(\"{query}\", query))\n",
        "        complexity = complexity_result.strip().upper()\n",
        "\n",
        "        filter_dict = None # Inicializar filtro\n",
        "\n",
        "        # Ajustar par√°metros seg√∫n complejidad\n",
        "        if \"SIMPLE\" in complexity:\n",
        "            k_value = 3 # Aumentado ligeramente para consultas simples\n",
        "            fetch_k = 6\n",
        "            lambda_mult = 0.6\n",
        "        elif \"MEDIA\" in complexity:\n",
        "            k_value = 5 # Aumentado\n",
        "            fetch_k = 10\n",
        "            lambda_mult = 0.7\n",
        "        else:  # COMPLEJA\n",
        "            k_value = 7 # Aumentado\n",
        "            fetch_k = 15\n",
        "            lambda_mult = 0.8\n",
        "\n",
        "            # Intentar determinar qu√© ley es m√°s relevante para filtrar (si es compleja)\n",
        "            law_prompt = \"\"\"\n",
        "            Para esta consulta compleja: \"{query}\"\n",
        "            ¬øQu√© normativa parece M√ÅS relevante? Responde solo con UNA palabra clave:\n",
        "            - RGPD\n",
        "            - LOPDGDD\n",
        "            - CONSTITUCION\n",
        "            - CODIGO_PENAL\n",
        "            - OTRO (si no encaja claramente o requiere m√∫ltiples)\n",
        "            \"\"\"\n",
        "\n",
        "            law_result = llm_evaluador.invoke(law_prompt.replace(\"{query}\", query)).strip().upper()\n",
        "\n",
        "            # Configurar filtro si hay una ley espec√≠fica y clara\n",
        "            if law_result == \"RGPD\":\n",
        "                filter_dict = {\"tipo\": \"RGPD\"}\n",
        "            elif law_result == \"LOPDGDD\":\n",
        "                filter_dict = {\"tipo\": \"LOPDGDD\"}\n",
        "            elif law_result == \"CONSTITUCION\":\n",
        "                filter_dict = {\"tipo\": \"Constituci√≥n\"}\n",
        "            elif law_result == \"CODIGO_PENAL\":\n",
        "                filter_dict = {\"tipo\": \"C√≥digo Penal\"}\n",
        "            # else: filtro sigue siendo None\n",
        "\n",
        "        # Configurar retriever con los par√°metros adaptativos\n",
        "        # ¬°Importante! Crear una *nueva* instancia o clonar para no modificar el base_retriever original globalmente\n",
        "        # Nota: as_retriever() crea una nueva instancia configurada\n",
        "        adaptive_retriever = persisted_vectorstore.as_retriever( # Asume persisted_vectorstore es global o pasado como argumento\n",
        "            search_type=\"mmr\",\n",
        "            search_kwargs={\n",
        "                'k': k_value,\n",
        "                'fetch_k': fetch_k,\n",
        "                'lambda_mult': lambda_mult,\n",
        "                # Aplicar filtro si se determin√≥ uno\n",
        "                **({'filter': filter_dict} if filter_dict else {})\n",
        "            }\n",
        "        )\n",
        "\n",
        "        logger.info(f\"Retriever adaptativo configurado: k={k_value}, fetch_k={fetch_k}, lambda={lambda_mult}, filtro={filter_dict}\")\n",
        "        return adaptive_retriever\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Error configurando retriever adaptativo: {e}. Usando configuraci√≥n por defecto.\")\n",
        "        # Devolver el retriever base original o uno con config por defecto\n",
        "        return base_retriever # O reconfigurar base_retriever con valores por defecto\n",
        "\n",
        "\n",
        "# --- 4. Sistema de validaci√≥n y reintento ---\n",
        "def validate_legal_response(response, query, docs_used, llm): #Aqui usamos el modelo finetuneado porque es mas preciso para temas legales\n",
        "    \"\"\"Valida la calidad de una respuesta legal\"\"\"\n",
        "\n",
        "    validation_prompt = f\"\"\"\n",
        "    Eval√∫a la calidad de esta respuesta legal:\n",
        "\n",
        "    Consulta: {query}\n",
        "    Respuesta: {response}\n",
        "\n",
        "    Verifica SOLO estos 3 aspectos:\n",
        "    1. ¬øEs jur√≠dicamente precisa seg√∫n la legislaci√≥n espa√±ola y europea de protecci√≥n de datos? (S√≠/No)\n",
        "    2. ¬øResponde completamente a la consulta realizada? (S√≠/No)\n",
        "    3. ¬øContiene contradicciones internas o errores evidentes? (S√≠/No)\n",
        "\n",
        "    Responde estrictamente en este formato: [PRECISI√ìN: S√≠/No], [COMPLETITUD: S√≠/No], [ERRORES: S√≠/No]\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        result = llm.invoke(validation_prompt)\n",
        "\n",
        "        # Patrones de validaci√≥n m√°s robustos (ignorando may√∫sculas/min√∫sculas y espacios)\n",
        "        precision_match = re.search(r'PRECISI√ìN:\\s*(S√≠|No)', result, re.IGNORECASE)\n",
        "        completitud_match = re.search(r'COMPLETITUD:\\s*(S√≠|No)', result, re.IGNORECASE)\n",
        "        errores_match = re.search(r'ERRORES:\\s*(S√≠|No)', result, re.IGNORECASE) # Busca 'No' para sin_errores\n",
        "\n",
        "        # Extraer resultados\n",
        "        precision_ok = precision_match and \"s√≠\" in precision_match.group(1).lower()\n",
        "        completitud_ok = completitud_match and \"s√≠\" in completitud_match.group(1).lower()\n",
        "        sin_errores = errores_match and \"no\" in errores_match.group(1).lower() # Es bueno si NO hay errores\n",
        "\n",
        "        # Calcular validez general\n",
        "        is_valid = precision_ok and completitud_ok and sin_errores\n",
        "\n",
        "        validation_result = {\n",
        "            \"valid\": is_valid,\n",
        "            \"precision\": precision_ok,\n",
        "            \"completitud\": completitud_ok,\n",
        "            \"sin_errores\": sin_errores,\n",
        "            \"raw_validation_output\": result.strip() # Guardar la salida cruda para depuraci√≥n\n",
        "        }\n",
        "\n",
        "        logger.info(f\"Validaci√≥n: {validation_result}\")\n",
        "        return validation_result\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error en validaci√≥n de respuesta: {e}\")\n",
        "        return {\"valid\": True, \"error\": str(e)} # Asumir validez en caso de error para no bloquear\n",
        "\n",
        "# --- 6. Sistema de reordenamiento de documentos ---\n",
        "def rerank_documents(query, docs, llm, top_n=5):\n",
        "    \"\"\"Reordena documentos por relevancia usando LLM, devuelve los top_n\"\"\"\n",
        "    if not docs:\n",
        "        return []\n",
        "\n",
        "    logger.info(f\"Iniciando reranking para {len(docs)} documentos recuperados...\")\n",
        "    try:\n",
        "        results = []\n",
        "        # Limitar el n√∫mero de documentos a reordenar para eficiencia\n",
        "        docs_to_rerank = docs[:min(len(docs), 8)] # Reordenar hasta 8 documentos\n",
        "\n",
        "        for i, doc in enumerate(docs_to_rerank):\n",
        "            # Truncar contenido del documento para el prompt\n",
        "            content_preview = doc.page_content[:500] # Usar un fragmento m√°s largo para evaluaci√≥n\n",
        "\n",
        "            prompt = f\"\"\"\n",
        "            Eval√∫a la relevancia de este fragmento de texto legal para responder a la siguiente consulta espec√≠fica:\n",
        "\n",
        "            Consulta del usuario: \"{query}\"\n",
        "\n",
        "            Fragmento del documento ({doc.metadata.get('source', 'N/A')} - P√°g. {doc.metadata.get('page', 'N/A')}):\n",
        "            \"{content_preview}...\"\n",
        "\n",
        "            Asigna una puntuaci√≥n de relevancia del 0 al 10, donde 10 es extremadamente relevante y 0 es irrelevante.\n",
        "            Responde SOLAMENTE con el n√∫mero de la puntuaci√≥n:\"\"\"\n",
        "\n",
        "            score_text = llm.invoke(prompt).strip()\n",
        "\n",
        "            # Extraer puntuaci√≥n de forma m√°s robusta\n",
        "            score_match = re.search(r'\\b(10|[0-9])\\b', score_text) # Busca un n√∫mero del 0-10 como palabra completa\n",
        "            if score_match:\n",
        "                score = float(score_match.group(0))\n",
        "            else:\n",
        "                logger.warning(f\"No se pudo extraer puntuaci√≥n de reranking de: '{score_text}'. Usando 5.0 por defecto.\")\n",
        "                score = 5.0 # Valor neutral por defecto\n",
        "\n",
        "            results.append((doc, score))\n",
        "            logger.debug(f\"Doc {i} puntuado con {score}\")\n",
        "\n",
        "        # Ordenar por puntuaci√≥n descendente\n",
        "        sorted_results = sorted(results, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Devolver los top_n documentos reordenados\n",
        "        reranked_docs = [doc for doc, score in sorted_results[:top_n]]\n",
        "        logger.info(f\"Reranking completado. {len(reranked_docs)} documentos seleccionados.\")\n",
        "\n",
        "        # Opcional: A√±adir documentos no reordenados si top_n es mayor que los reordenados\n",
        "        # if len(reranked_docs) < top_n:\n",
        "        #     remaining_docs = docs[len(docs_to_rerank):]\n",
        "        #     reranked_docs.extend(remaining_docs[:top_n - len(reranked_docs)])\n",
        "\n",
        "        return reranked_docs\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error durante el reranking: {e}\")\n",
        "        return docs[:top_n] # Devolver los primeros N documentos originales en caso de error\n",
        "\n",
        "\n",
        "# --- 9. Selecci√≥n de prompt seg√∫n clasificaci√≥n ---\n",
        "def get_prompt_by_category(category, memory_text=\"\", query=\"\"):\n",
        "    \"\"\"Devuelve el ChatPromptTemplate adecuado seg√∫n la categor√≠a de consulta\"\"\"\n",
        "    system_template = system_prompt_rag # Default\n",
        "    human_template = \"{question}\" # Input directo del usuario para RAG\n",
        "    prompt_used =\"\"\n",
        "    patterns_multi = [\"todos los art√≠culos\", \"art√≠culos relacionados\", \"varios art√≠culos\", \"art√≠culos del rgpd\", \"citame los art√≠culos\", \"c√≠tame varios\", \"c√≠tame todos\"]\n",
        "\n",
        "    if is_follow_up_query(query) and is_action_request(query):\n",
        "        logger.info(\"Consulta de seguimiento orientada a acciones detectada. Usando action_oriented_prompt.\")\n",
        "        system_template = action_oriented_prompt\n",
        "    else:\n",
        "        if category == \"LEGAL_CITATION\" and any(pat in query.lower() for pat in patterns_multi):\n",
        "            system_template = legal_citation_prompt\n",
        "            prompt_used = \"legal_citation_prompt\"\n",
        "        elif category == \"LEGAL_CITATION\":\n",
        "            system_template = legal_multi_citation_prompt\n",
        "            prompt_used = \"legal_multi_citation_prompt\"\n",
        "        elif category == \"LEGAL_ANALYSIS\" and is_action_request(query):\n",
        "            system_template = action_oriented_prompt\n",
        "            prompt_used = \"action_oriented_prompt\"\n",
        "        elif category == \"LEGAL_ANALYSIS\":\n",
        "            system_template = legal_analysis_prompt\n",
        "            prompt_used = \"legal_analysis_prompt\"\n",
        "        elif category == \"PROCEDURAL\":\n",
        "            system_template = procedural_prompt\n",
        "            prompt_used = \"procedural_prompt\"\n",
        "        elif category == \"GENERAL_INFO\":\n",
        "            system_template = general_info_prompt # Usar prompt espec√≠fico para info general\n",
        "            prompt_used = \"general_info_prompt\"\n",
        "        elif category == \"CONVERSATION\":\n",
        "            # Para conversaci√≥n, no usamos contexto RAG, usamos memoria\n",
        "            system_template = conversation_prompt.format(memory=memory_text) # Inyectar memoria aqu√≠\n",
        "            human_template = \"{question}\" # Sigue siendo la pregunta del usuario\n",
        "            # Devolver directamente el prompt formateado para conversaci√≥n, ya que no pasar√° por load_qa_chain\n",
        "            # OJO: Esto requiere que el flujo principal maneje esto diferente.\n",
        "            # Por simplicidad ahora, devolvemos estructura similar, pero el flujo debe saber NO usar RAG.\n",
        "            # Alternativa: devolver None o un identificador especial.\n",
        "            # Vamos a devolver la estructura est√°ndar por ahora, asumiendo que el flujo principal lo maneja.\n",
        "            # PERO, el prompt de conversaci√≥n NO tiene variable {context}.\n",
        "            # => Mejor devolver None para indicar que no se use RAG/load_qa_chain.\n",
        "            logger.info(\"Categor√≠a CONVERSATION: No se usar√° RAG. Se generar√° respuesta directa.\")\n",
        "            prompt_used = \"conversation_prompt\"\n",
        "            # Construir un prompt simple para LLM directo\n",
        "            return ChatPromptTemplate.from_messages([\n",
        "                 (\"system\", system_template), # Ya formateado con memoria\n",
        "                 (\"human\", human_template)\n",
        "            ])\n",
        "\n",
        "    logger.info(f\"Usando el prompt: {prompt_used}\")\n",
        "    # Para las categor√≠as que usan RAG (con contexto)\n",
        "    # El human_template debe incluir la pregunta del usuario\n",
        "    # El system_template incluye {context} y {question}\n",
        "    # load_qa_chain se encargar√° de llenar {context} y {question}\n",
        "    return ChatPromptTemplate.from_messages([\n",
        "        SystemMessagePromptTemplate.from_template(system_template),\n",
        "        HumanMessagePromptTemplate.from_template(human_template) # Solo {question} aqu√≠, load_qa_chain lo maneja\n",
        "    ])\n",
        "\n",
        "\n",
        "# --- ¬°¬°¬°DEFINICI√ìN NECESARIA DE LA CADENA RAG!!! ---\n",
        "# Necesitamos definir c√≥mo se combinar√°n el LLM y el Prompt con los documentos.\n",
        "# Usamos load_qa_chain. El tipo de cadena (\"stuff\", \"map_reduce\", etc.) puede variar.\n",
        "# \"stuff\" es simple pero puede exceder el l√≠mite de tokens si hay muchos documentos.\n",
        "# \"map_reduce\" o \"refine\" son m√°s robustos para contextos largos.\n",
        "# Probemos con \"stuff\" inicialmente dado el chunk_size de 800.\n",
        "\n",
        "# Nota: La cadena se crea aqu√≠, pero el *prompt espec√≠fico* se pasar√° en cada invocaci√≥n.\n",
        "# Esto es m√°s flexible que crear una cadena diferente cada vez.\n",
        "# OJO: load_qa_chain espera un prompt espec√≠fico en su creaci√≥n.\n",
        "# Vamos a crear una funci√≥n que genere la cadena CON el prompt adecuado CADA VEZ.\n",
        "\n",
        "def create_rag_chain(llm, prompt):\n",
        "    \"\"\"Crea la cadena load_qa_chain con el LLM y el prompt espec√≠ficos.\"\"\"\n",
        "    # El prompt debe ser un BasePromptTemplate (como ChatPromptTemplate)\n",
        "    if not isinstance(prompt, (PromptTemplate, ChatPromptTemplate)):\n",
        "         logger.error(\"El prompt proporcionado a create_rag_chain no es v√°lido.\")\n",
        "         # Se puede lanzar un error o devolver None/cadena por defecto\n",
        "         return None\n",
        "\n",
        "    # Selecciona el tipo de cadena. 'stuff' es bueno para empezar.\n",
        "    # Ajusta 'chain_type' si tienes problemas de longitud de contexto.\n",
        "    qa_chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=prompt, verbose=False) # verbose=True para debug\n",
        "    return qa_chain\n",
        "\n",
        "def is_follow_up_query(query):\n",
        "    \"\"\"Detecta si la nueva pregunta es una continuaci√≥n (seguimiento) del contexto anterior.\"\"\"\n",
        "    query_lower = query.strip().lower()\n",
        "    follow_up_starts = [\n",
        "        \"entonces\", \"y\", \"pero\", \"por qu√©\", \"qu√© pueden hacer\",\n",
        "        \"qu√© derechos tienen\", \"c√≥mo reclamar\", \"porque\",\"qu√© consecuencias\",\n",
        "        \"qu√© opciones tienen\", \"c√≥mo actuar\", \"qu√© recursos tienen\"\n",
        "    ]\n",
        "    return any(query_lower.startswith(start) for start in follow_up_starts) or len(query.split()) <= 8\n",
        "\n",
        "\n",
        "def build_contextual_query(last_query, current_query):\n",
        "    \"\"\"Construye una consulta combinando la anterior y la nueva\"\"\"\n",
        "    return f\"Respecto a la situaci√≥n planteada previamente: {last_query}\\nNueva pregunta: {current_query}\"\n",
        "\n",
        "def classify_intent_with_cache(query, llm_evaluador):\n",
        "    \"\"\"\n",
        "    Clasifica la intenci√≥n de una consulta usando cach√© para evitar llamadas repetidas al LLM.\n",
        "    \"\"\"\n",
        "    cache_key = ' '.join(query.lower().split()[:10])  # Usamos las primeras 10 palabras para normalizar claves\n",
        "\n",
        "    if cache_key in intent_classification_cache:\n",
        "        logger.info(f\"Resultado de clasificaci√≥n obtenido de cach√© para: {cache_key}\")\n",
        "        return intent_classification_cache[cache_key]\n",
        "\n",
        "    # No est√° en cach√©: clasificamos\n",
        "    classification_result = classify_intent_advanced(query, llm_evaluador)\n",
        "\n",
        "    # Guardamos el resultado en cach√©\n",
        "    intent_classification_cache[cache_key] = classification_result\n",
        "\n",
        "    return classification_result\n",
        "\n",
        "def adapt_retriever(base_retriever, attempt):\n",
        "    \"\"\"Adapta el recuperador seg√∫n el intento para mejorar recuperaci√≥n de documentos.\"\"\"\n",
        "    if attempt == 1:\n",
        "        logger.info(\"Reintento 1: aumentando k y activando reranking.\")\n",
        "        return persisted_vectorstore.as_retriever(\n",
        "            search_type=\"mmr\",\n",
        "            search_kwargs={'k': 8, 'fetch_k': 16, 'lambda_mult': 0.75}\n",
        "        )\n",
        "    elif attempt == 2:\n",
        "        logger.info(\"Reintento 2: creando un retriever m√°s flexible (MMR + reformulaci√≥n posible).\")\n",
        "        return persisted_vectorstore.as_retriever(\n",
        "            search_type=\"mmr\",\n",
        "            search_kwargs={'k': 10, 'fetch_k': 20, 'lambda_mult': 0.6}\n",
        "        )\n",
        "    else:\n",
        "        return base_retriever\n",
        "\n",
        "\n",
        "# --- 5. Sistema de reintento con estrategias alternativas ---\n",
        "def get_response_with_retry(query, llm, base_retriever, memory: ImprovedMemory, last_query=None, max_attempts=3):\n",
        "    \"\"\"Obtiene respuesta con sistema de reintentos, clasificaci√≥n con cach√©, y detecci√≥n de continuidad conversacional.\"\"\"\n",
        "    from sentence_transformers import SentenceTransformer, util\n",
        "    reformulation_embedder = SentenceTransformer(\"paraphrase-multilingual-mpnet-base-v2\")\n",
        "\n",
        "    memory_text = memory.get_relevant_memory(query)\n",
        "\n",
        "    # --- Detectar si la pregunta depende de la anterior ---\n",
        "    if is_follow_up_query(query) and last_query:\n",
        "        logger.info(\"Detectada pregunta de seguimiento. Incorporando √∫ltima consulta como contexto.\")\n",
        "        query = f\"Contexto anterior: {last_query}\\n\\nNueva consulta: {query}\"\n",
        "\n",
        "    # --- 3. Clasificar la intenci√≥n usando cach√© ---\n",
        "    classifier_llm = llm_evaluador if 'llm_evaluador' in globals() and llm_evaluador else llm\n",
        "    classification_result = classify_intent_with_cache(query, classifier_llm)\n",
        "    category = classification_result[\"category\"]\n",
        "    confidence = classification_result[\"confidence\"]\n",
        "\n",
        "    use_rag = category in [\"LEGAL_CITATION\", \"LEGAL_ANALYSIS\", \"PROCEDURAL\"] or (category == \"GENERAL_INFO\" and confidence < 85) or confidence < 75\n",
        "    if category == \"CONVERSATION\":\n",
        "        use_rag = False\n",
        "\n",
        "    logger.info(f\"Intenci√≥n clasificada como: {category} (Confianza: {confidence}%) - Usar RAG: {use_rag}\")\n",
        "\n",
        "    response_data = None\n",
        "\n",
        "    # --- 4. Generar respuesta directa o RAG ---\n",
        "    if not use_rag:\n",
        "        # --- RESPUESTA DIRECTA ---\n",
        "        try:\n",
        "            system_prompt = conversation_prompt.format(memory=memory_text) if category == \"CONVERSATION\" else f\"\"\"\n",
        "            Sistema: Eres un asistente experto que responde preguntas con precisi√≥n y amabilidad.\n",
        "            Contexto conversacional previo:\n",
        "            {memory_text}\n",
        "\n",
        "            Usuario: {{question}}\n",
        "            \"\"\"\n",
        "            prompt = ChatPromptTemplate.from_messages([\n",
        "                (\"system\", system_prompt),\n",
        "                (\"human\", \"{question}\")\n",
        "            ])\n",
        "            llm_chain = LLMChain(llm=llm_conversacion, prompt=prompt, output_key='text')\n",
        "            result = llm_chain.invoke({\"question\": query})\n",
        "            response_text = result.get('text', str(result)).strip()\n",
        "\n",
        "            response_data = {\"response\": response_text, \"docs\": [], \"attempt\": 1, \"category\": category, \"validation\": {\"valid\": True}}\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generando respuesta directa: {e}\", exc_info=True)\n",
        "            response_data = {\"response\": f\"Lo siento, no pude generar una respuesta directa.\", \"docs\": [], \"attempt\": 1, \"category\": category, \"error\": str(e)}\n",
        "\n",
        "    else:\n",
        "        # --- RESPUESTA RAG ---\n",
        "        selected_prompt_template = get_prompt_by_category(category, memory_text, query)\n",
        "\n",
        "        for attempt in range(max_attempts):\n",
        "            try:\n",
        "                logger.info(f\"Intento RAG {attempt+1} para consulta: '{query}' (Categor√≠a: {category})\")\n",
        "\n",
        "                retriever = adapt_retriever(base_retriever, attempt)\n",
        "                retrieved_docs = retriever.get_relevant_documents(query)\n",
        "                logger.info(f\"Recuperados {len(retrieved_docs)} documentos.\")\n",
        "\n",
        "                if attempt > 0 and retrieved_docs:\n",
        "                    retrieved_docs = rerank_documents(query, retrieved_docs, llm, top_n=4)\n",
        "                    logger.info(f\"Documentos despu√©s de reranking: {len(retrieved_docs)}\")\n",
        "\n",
        "                if not retrieved_docs:\n",
        "                    logger.warning(\"No se encontraron documentos relevantes.\")\n",
        "\n",
        "                    if attempt == max_attempts - 1:\n",
        "                        response_data = {\n",
        "                            \"response\": \"No he encontrado informaci√≥n espec√≠fica para responder con seguridad, pero puedo intentar darte una orientaci√≥n general.\",\n",
        "                            \"docs\": [],\n",
        "                            \"attempt\": attempt + 1,\n",
        "                            \"category\": category,\n",
        "                            \"validation\": {\"valid\": False, \"error\": \"Sin documentos relevantes\"}\n",
        "                        }\n",
        "                        break  # Salir del bucle\n",
        "                    else:\n",
        "                        continue\n",
        "\n",
        "                chain = create_rag_chain(llm, selected_prompt_template)\n",
        "                result = chain.invoke({\"input_documents\": retrieved_docs, \"question\": query})\n",
        "                response_text = result.get('output_text', result if isinstance(result, str) else str(result)).strip()\n",
        "\n",
        "                validation = validate_legal_response(response_text, query, retrieved_docs, llm)\n",
        "                if validation.get(\"valid\", False):\n",
        "                    logger.info(f\"Respuesta validada en intento {attempt+1}.\")\n",
        "                    response_data = {\n",
        "                        \"response\": response_text,\n",
        "                        \"docs\": retrieved_docs,\n",
        "                        \"attempt\": attempt+1,\n",
        "                        \"category\": category,\n",
        "                        \"validation\": validation\n",
        "                    }\n",
        "                    break\n",
        "                else:\n",
        "                    logger.warning(f\"Respuesta no v√°lida en intento {attempt+1}: {validation}\")\n",
        "\n",
        "                    # Intentar mejorar el prompt si no es el √∫ltimo intento\n",
        "                    if attempt < max_attempts - 1:\n",
        "                        # Extraer sugerencias de mejora\n",
        "                        hints = extract_improvement_hints(validation.get(\"raw_validation_output\", \"\"))\n",
        "                        reformulated_query = reformulate_user_query(query, hints)\n",
        "                        # Volver a obtener el prompt base original\n",
        "                        selected_prompt_template = get_prompt_by_category(category, memory_text, reformulated_query)\n",
        "\n",
        "                        # Si es un prompt est√°ndar con {context}, entonces podemos modificarlo\n",
        "                        if isinstance(selected_prompt_template, ChatPromptTemplate):\n",
        "                            try:\n",
        "                                # Tomar solo el system prompt como texto\n",
        "                                original_system_msg = selected_prompt_template.messages[0].prompt.template\n",
        "                                new_system_msg = augment_prompt_with_validation(original_system_msg, hints)\n",
        "\n",
        "                                selected_prompt_template = ChatPromptTemplate.from_messages([\n",
        "                                    SystemMessagePromptTemplate.from_template(new_system_msg),\n",
        "                                    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
        "                                ])\n",
        "\n",
        "                                logger.info(\"Prompt adaptado con sugerencias del validador.\")\n",
        "                            except Exception as e:\n",
        "                                logger.warning(f\"No se pudo adaptar el prompt con sugerencias: {e}\")\n",
        "\n",
        "                    if attempt == max_attempts - 1:\n",
        "                        response_data = {\n",
        "                            \"response\": response_text,\n",
        "                            \"docs\": retrieved_docs,\n",
        "                            \"attempt\": attempt+1,\n",
        "                            \"category\": category,\n",
        "                            \"validation\": validation\n",
        "                        }\n",
        "                    debug=1\n",
        "                    if attempt == max_attempts - 1 and not validation.get(\"valid\", False) and debug ==1:\n",
        "                        logger.info(\"Iniciando auto-refinamiento de respuesta con feedback del validador.\")\n",
        "\n",
        "                        refinement_prompt = ChatPromptTemplate.from_messages([\n",
        "                            SystemMessagePromptTemplate.from_template(\n",
        "                                refinement_module_prompt\n",
        "                            ),\n",
        "                            HumanMessagePromptTemplate.from_template(\"Corrige la respuesta anterior respetando la estructura exacta y ajust√°ndola seg√∫n los fallos detectados.\")\n",
        "                        ])\n",
        "\n",
        "                        refinement_chain = LLMChain(llm=llm, prompt=refinement_prompt)\n",
        "                        refined_result = refinement_chain.invoke({\n",
        "                            \"fallos\": validation.get(\"raw_validation_output\", \"\"),\n",
        "                            \"respuesta\": response_text\n",
        "                        })\n",
        "\n",
        "                        response_data[\"response\"] = refined_result.get(\"text\", \"\").strip()\n",
        "                        response_data[\"validation\"][\"refinado\"] = True\n",
        "\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error en intento RAG {attempt+1}: {e}\", exc_info=True)\n",
        "                if attempt == max_attempts-1:\n",
        "                    response_data = {\"response\": f\"Ocurri√≥ un error grave al procesar tu consulta.\", \"docs\": [], \"attempt\": attempt+1, \"category\": category, \"error\": str(e)}\n",
        "\n",
        "            query = reformulated_query\n",
        "\n",
        "    # --- 5. Guardar turno en la memoria ---\n",
        "    if response_data and \"response\" in response_data:\n",
        "        memory.add_turn(query, response_data[\"response\"], llm)\n",
        "    else:\n",
        "        memory.add_turn(query, \"[Error al generar respuesta]\", llm=None)\n",
        "\n",
        "    return response_data\n",
        "\n",
        "# --- 8. Sistema de feedback y evaluaci√≥n ---\n",
        "def collect_feedback(query, response):\n",
        "    \"\"\"Solicita feedback al usuario sobre la respuesta\"\"\"\n",
        "    print(\"\\n\" + \"=\"*20 + \" FEEDBACK \" + \"=\"*20)\n",
        "    print(f\"Consulta: '{query}'\")\n",
        "    print(f\"\\nRespuesta proporcionada:\\n{response}\")\n",
        "    print(\"=\"*50)\n",
        "    while True:\n",
        "        try:\n",
        "            rating = input(\"¬øQu√© tan √∫til fue esta respuesta? (1=Nada √∫til, 5=Muy √∫til, 0=Saltar): \")\n",
        "            rating = int(rating)\n",
        "            if 0 <= rating <= 5:\n",
        "                 return rating if rating > 0 else None # Devolver None si es 0\n",
        "            else:\n",
        "                 print(\"Por favor, introduce un n√∫mero entre 0 y 5.\")\n",
        "        except ValueError:\n",
        "            print(\"Entrada inv√°lida. Por favor, introduce un n√∫mero.\")\n",
        "\n",
        "def extract_improvement_hints(validation_output):\n",
        "    # Extrae frases despu√©s de los puntos de fallo\n",
        "    hints = []\n",
        "    for match in re.finditer(r'\\[\\w+:\\s*No\\](.*?)\\n', validation_output, re.IGNORECASE):\n",
        "        hint = match.group(1).strip()\n",
        "        if hint:\n",
        "            hints.append(hint)\n",
        "    return hints\n",
        "\n",
        "def augment_prompt_with_validation(prompt_text, improvement_hints):\n",
        "    if not improvement_hints:\n",
        "        return prompt_text\n",
        "    hint_block = \"\\nIMPORTANTE: Al responder, aseg√∫rate de abordar tambi√©n los siguientes aspectos:\\n\"\n",
        "    for h in improvement_hints:\n",
        "        hint_block += f\"- {h}\\n\"\n",
        "    return prompt_text + hint_block\n",
        "\n",
        "def reformulate_user_query(original_query: str, improvement_hints: list[str]) -> str:\n",
        "    \"\"\"A√±ade instrucciones expl√≠citas a la consulta original usando sugerencias del validador.\"\"\"\n",
        "    if not improvement_hints:\n",
        "        return original_query\n",
        "    reformulation = \"\\n\\n Tambi√©n responde espec√≠ficamente a:\\n\"\n",
        "    for hint in improvement_hints:\n",
        "        reformulation += f\"- {hint.strip()}\\n\"\n",
        "    return original_query + reformulation\n",
        "\n",
        "def save_interaction(query, result_data, feedback=None):\n",
        "    \"\"\"Guarda la interacci√≥n completa para an√°lisis y mejora\"\"\"\n",
        "    try:\n",
        "        interaction = {\n",
        "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            \"query\": query,\n",
        "            \"response\": result_data.get(\"response\"),\n",
        "            \"category\": result_data.get(\"category\"),\n",
        "            \"attempt\": result_data.get(\"attempt\"),\n",
        "            \"validation\": result_data.get(\"validation\"),\n",
        "            \"error\": result_data.get(\"error\"),\n",
        "            \"feedback\": feedback, # A√±adir feedback del usuario\n",
        "            \"retrieved_docs\": [\n",
        "                {\n",
        "                    \"content_preview\": doc.page_content[:200] + \"...\", # Preview\n",
        "                    \"metadata\": doc.metadata,\n",
        "                    #\"score\": doc.score # A√±adir si el retriever devuelve score\n",
        "                }\n",
        "                # Limitar el n√∫mero de documentos guardados para no hacer el log enorme\n",
        "                for doc in result_data.get(\"docs\", [])[:5] # Guardar metadata de los primeros 5 docs usados\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        with open(\"interacciones_legales.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(json.dumps(interaction, ensure_ascii=False, default=str) + \"\\n\") # default=str para manejar tipos no serializables\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error guardando interacci√≥n: {e}\")\n",
        "\n",
        "def evaluate_response_quality(query, response, llm):\n",
        "    \"\"\"Eval√∫a la calidad de una respuesta utilizando el LLM\"\"\"\n",
        "    # (Se mantiene la estructura de evaluaci√≥n con prompts, pero se simplifica la llamada)\n",
        "    metrics = {}\n",
        "    prompts = {\n",
        "        \"legal_accuracy\": \"\"\"\n",
        "        Eval√∫a la PRECISI√ìN JUR√çDICA de esta respuesta sobre protecci√≥n de datos (Espa√±a/UE):\n",
        "        Consulta: {query}\n",
        "        Respuesta: {response}\n",
        "        ¬øLa respuesta es correcta seg√∫n RGPD/LOPDGDD? ¬øInterpreta bien las normas?\n",
        "        Puntuaci√≥n (0-10, solo n√∫mero):\"\"\",\n",
        "        \"relevance\": \"\"\"\n",
        "        Eval√∫a la RELEVANCIA de la respuesta respecto a la consulta:\n",
        "        Consulta: {query}\n",
        "        Respuesta: {response}\n",
        "        ¬øResponde directamente a lo preguntado? ¬øEvita informaci√≥n superflua?\n",
        "        Puntuaci√≥n (0-10, solo n√∫mero):\"\"\",\n",
        "        \"completeness\": \"\"\"\n",
        "        Eval√∫a la COMPLETITUD de esta respuesta legal:\n",
        "        Consulta: {query}\n",
        "        Respuesta: {response}\n",
        "        ¬øCubre todos los aspectos clave? ¬øOfrece suficiente detalle/fundamento?\n",
        "        Puntuaci√≥n (0-10, solo n√∫mero):\"\"\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        for metric, prompt_template in prompts.items():\n",
        "            eval_prompt = prompt_template.format(query=query, response=response)\n",
        "            eval_resp = llm.invoke(eval_prompt)\n",
        "            score_match = re.search(r'\\b(10|[0-9])\\b', eval_resp)\n",
        "            if score_match:\n",
        "                metrics[metric] = float(score_match.group(0)) / 10.0\n",
        "            else:\n",
        "                logger.warning(f\"No se pudo extraer puntuaci√≥n para m√©trica '{metric}' de: '{eval_resp}'\")\n",
        "                metrics[metric] = 0.5 # Default neutral\n",
        "\n",
        "        # Calcular m√©trica general (si todas las m√©tricas est√°n presentes)\n",
        "        if len(metrics) == 3:\n",
        "             metrics[\"overall\"] = sum(metrics.values()) / 3.0\n",
        "        else:\n",
        "             metrics[\"overall\"] = 0.5 # Default si faltan m√©tricas\n",
        "\n",
        "        logger.info(f\"Evaluaci√≥n de calidad autom√°tica: {metrics}\")\n",
        "        return metrics\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error evaluando calidad de respuesta: {e}\")\n",
        "        return {\"overall\": 0.5, \"error\": str(e)} # Default en caso de error\n",
        "\n",
        "def save_interaction_for_training(query, response, feedback_score, docs_retrieved):\n",
        "     \"\"\"Guarda interacci√≥n con feedback positivo para posible entrenamiento futuro.\"\"\"\n",
        "     # Solo guardar si el feedback es bueno (ej. 4 o 5)\n",
        "     if feedback_score is None or feedback_score < 4:\n",
        "         return\n",
        "\n",
        "     try:\n",
        "         training_example = {\n",
        "             \"query\": query,\n",
        "             \"positive_response\": response, # Marcado como positivo por el feedback\n",
        "             \"feedback_score\": feedback_score,\n",
        "             # Opcional: incluir contexto relevante si se quiere entrenar RAG-finetuning\n",
        "             \"relevant_context\": [doc.page_content for doc in docs_retrieved[:2]], # Ej: 2 docs m√°s relevantes\n",
        "             \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "         }\n",
        "\n",
        "         with open(\"training_data_positive.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
        "             f.write(json.dumps(training_example, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "         logger.info(f\"Guardada interacci√≥n con feedback {feedback_score} para posible entrenamiento.\")\n",
        "\n",
        "     except Exception as e:\n",
        "         logger.error(f\"Error guardando interacci√≥n para entrenamiento: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "def normalize_text(text):\n",
        "    \"\"\"Normaliza el texto: elimina tildes, min√∫sculas, quita signos de puntuaci√≥n.\"\"\"\n",
        "    text = unicodedata.normalize('NFD', text)\n",
        "    text = ''.join(c for c in text if unicodedata.category(c) != 'Mn')  # quitar tildes\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # elimina puntuaci√≥n\n",
        "    return text\n",
        "\n",
        "def is_action_request(query):\n",
        "    \"\"\"Detecta si la pregunta sugiere solicitud de acciones, derechos o pasos a seguir.\"\"\"\n",
        "    normalized_query = normalize_text(query)\n",
        "\n",
        "    action_phrases = [\n",
        "        \"que pueden hacer\",\n",
        "        \"que medidas pueden tomar\",\n",
        "        \"que derechos tienen\",\n",
        "        \"como pueden reclamar\",\n",
        "        \"como pueden actuar\",\n",
        "        \"como reclamar\",\n",
        "        \"como actuar\",\n",
        "        \"que opciones tienen\",\n",
        "        \"que pasos pueden seguir\",\n",
        "        \"que pueden solicitar\",\n",
        "        \"que acciones pueden emprender\",\n",
        "        \"como defenderse\",\n",
        "        \"como denunciar\",\n",
        "        \"como protegerse\",\n",
        "        \"como impugnar\",\n",
        "        \"como negarse\",\n",
        "        \"que recurso tienen\",\n",
        "        \"que alternativas tienen\",\n",
        "        \"que pueden exigir\",\n",
        "        \"que sanciones puede haber\",\n",
        "        \"que consecuencias hay\",\n",
        "    ]\n",
        "\n",
        "    return any(phrase in normalized_query for phrase in action_phrases)\n",
        "\n",
        "\n",
        "# --- Bloque Principal de Interacci√≥n ---\n",
        "intent_classification_cache = {}\n",
        "last_user_query = None #Para preguntas continuistas\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\": # Poner el c√≥digo de ejecuci√≥n aqu√≠ para evitar que se ejecute al importar\n",
        "\n",
        "    # Verificar que las variables necesarias existen\n",
        "    if 'llm' not in locals() or 'llm_evaluador' not in locals() or 'base_retriever' not in locals() or 'persisted_vectorstore' not in locals():\n",
        "         print(\"ERROR: Aseg√∫rate de haber ejecutado las celdas anteriores para inicializar llm, llm_evaluador, base_retriever y persisted_vectorstore.\")\n",
        "    else:\n",
        "         print(\"Modelo LLM y retriever listos.\")\n",
        "\n",
        "         # Inicializar memoria conversacional\n",
        "         conversation_memory = ImprovedMemory(max_turns=4) # Guardar 4 turnos\n",
        "         last_query=\"\"\n",
        "         print(\"\\nBienvenido al Asistente Legal de Protecci√≥n de Datos.\")\n",
        "         print(\"Escribe 'salir' para terminar.\")\n",
        "\n",
        "         while True:\n",
        "             user_query = input(\"\\nTu consulta: \")\n",
        "             if user_query.lower() == 'salir':\n",
        "                 break\n",
        "             if not user_query:\n",
        "                 continue\n",
        "\n",
        "             start_time = time.time()\n",
        "\n",
        "             # --- Llamada principal al sistema RAG con reintentos ---\n",
        "             result = get_response_with_retry(\n",
        "                 query=user_query,\n",
        "                 llm=llm,\n",
        "                 base_retriever=base_retriever,\n",
        "                 memory=conversation_memory, # Pasar la instancia de memoria\n",
        "                 last_query=last_query\n",
        "             )\n",
        "             last_query = user_query\n",
        "             end_time = time.time()\n",
        "\n",
        "             # --- Mostrar Resultados ---\n",
        "             print(\"\\n--- Respuesta del Asistente ---\")\n",
        "             print(result.get(\"response\", \"No se pudo generar respuesta.\"))\n",
        "             print(\"-\" * 30)\n",
        "             logger.info(f\"Respuesta generada en {end_time - start_time:.2f} segundos.\")\n",
        "             logger.info(f\"Categor√≠a: {result.get('category', 'N/A')}, Intentos: {result.get('attempt', 'N/A')}\")\n",
        "             if result.get(\"validation\"):\n",
        "                 logger.info(f\"Validaci√≥n: {result['validation']}\")\n",
        "             if result.get(\"error\"):\n",
        "                 logger.error(f\"Error reportado: {result['error']}\")\n",
        "\n",
        "             # Mostrar documentos fuente (opcional, para depuraci√≥n)\n",
        "             if result.get(\"docs\"):\n",
        "                 print(f\"\\nFuentes consultadas ({len(result['docs'])} documentos):\")\n",
        "                 for i, doc in enumerate(result[\"docs\"]):\n",
        "                     source = doc.metadata.get('source', 'Desconocido')\n",
        "                     page = doc.metadata.get('page', '?')\n",
        "                     tipo = doc.metadata.get('tipo', '')\n",
        "                     print(f\"  [{i+1}] {source} (P√°g: {page}, Tipo: {tipo})\") # Preview m√°s corto\n",
        "\n",
        "             # --- Feedback y Evaluaci√≥n (Opcional) ---\n",
        "             user_feedback = collect_feedback(user_query, result.get(\"response\"))\n",
        "             save_interaction(user_query, result, user_feedback)\n",
        "\n",
        "             if user_feedback:\n",
        "                 # Si hubo feedback positivo, guardar para posible entrenamiento\n",
        "                 save_interaction_for_training(user_query, result.get(\"response\"), user_feedback, result.get(\"docs\", []))\n",
        "\n",
        "             # Evaluar calidad autom√°ticamente (opcional, consume tokens)\n",
        "             # quality_metrics = evaluate_response_quality(user_query, result.get(\"response\"), llm)\n",
        "             # logger.info(f\"M√©tricas de calidad autom√°ticas: {quality_metrics}\")\n",
        "\n",
        "         print(\"\\n¬°Hasta luego!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoxVqhOEWj5b"
      },
      "source": [
        "# Comparaciones de modelos\n",
        "\n",
        "A continuaci√≥n esta el codigo para comparar las respuestas entre modelos\n",
        "\n",
        "Hay que resetear ollama entre consultas para evitar que use pueda recordar datos de la conversaci√≥n entre pruebas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ior64B0AfA6Y"
      },
      "outputs": [],
      "source": [
        "#Aqui pegamos nuestra pregunta\n",
        "pregunta=\"¬øCu√°les son los principios fundamentales que deben cumplirse para garantizar el tratamiento adecuado de datos personales seg√∫n la normativa de protecci√≥n de datos en Espa√±a?\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWBx9qAlajxd"
      },
      "outputs": [],
      "source": [
        "!pip install gputil\n",
        "!pip install psutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRlKHsidWshH"
      },
      "source": [
        "## Modelo Base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z-7dmDgZEpe"
      },
      "source": [
        "Descargamos el modelo base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ygs3fkqTY_1Y",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "!ollama run llama3:8b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRA0PR0yWjIu"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import psutil\n",
        "import GPUtil\n",
        "from statistics import mean\n",
        "import threading\n",
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# --- Reiniciar Ollama ---\n",
        "!kill -9 $(ps aux | grep '[o]llama' | awk '{print $2}')\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "time.sleep(3)  # Esperar a que Ollama est√© listo\n",
        "\n",
        "# --- Configuraci√≥n de prueba ---\n",
        "model_name = \"llama3:8b\"\n",
        "\n",
        "# --- Monitorear uso de RAM ---\n",
        "def get_system_metrics():\n",
        "    ram_used = psutil.virtual_memory().used / (1024 ** 3)  # GB\n",
        "    return ram_used\n",
        "\n",
        "# --- Monitorear uso de GPU (VRAM) ---\n",
        "def get_gpu_metrics():\n",
        "    gpus = GPUtil.getGPUs()\n",
        "    if gpus:\n",
        "        gpu = gpus[0]\n",
        "        return gpu.memoryUsed / 1024, gpu.load * 100  # GB, %\n",
        "    return 0, 0\n",
        "\n",
        "# --- Iniciar modelo ---\n",
        "llm = OllamaLLM(model=model_name, max_tokens=250)\n",
        "\n",
        "# --- Medici√≥n de rendimiento ---\n",
        "start_total = time.time()\n",
        "\n",
        "ram_before = get_system_metrics()\n",
        "gpu_mem_before, gpu_load_before = get_gpu_metrics()\n",
        "\n",
        "# --- Medici√≥n activa de CPU en paralelo ---\n",
        "cpu_usage_during_infer = []\n",
        "stop_cpu_monitor = False\n",
        "\n",
        "def monitor_cpu():\n",
        "    while not stop_cpu_monitor:\n",
        "        cpu = psutil.cpu_percent(interval=0.1)\n",
        "        cpu_usage_during_infer.append(cpu)\n",
        "\n",
        "cpu_thread = threading.Thread(target=monitor_cpu)\n",
        "cpu_thread.start()\n",
        "\n",
        "# --- Inferencia ---\n",
        "start_infer = time.time()\n",
        "response = llm.invoke(pregunta)\n",
        "end_infer = time.time()\n",
        "\n",
        "# --- Detener CPU monitor ---\n",
        "stop_cpu_monitor = True\n",
        "cpu_thread.join()\n",
        "\n",
        "ram_after = get_system_metrics()\n",
        "gpu_mem_after, gpu_load_after = get_gpu_metrics()\n",
        "end_total = time.time()\n",
        "\n",
        "# --- M√©tricas calculadas ---\n",
        "total_time = end_total - start_total\n",
        "infer_time = end_infer - start_infer\n",
        "token_count = len(response.split())\n",
        "avg_cpu = mean(cpu_usage_during_infer) if cpu_usage_during_infer else 0\n",
        "\n",
        "# --- Salida ---\n",
        "print(\"\\n--- RESPUESTA ---\")\n",
        "print(response)\n",
        "\n",
        "print(\"\\n--- M√âTRICAS ---\")\n",
        "print(f\"Modelo: {model_name}\")\n",
        "print(f\"Tokens generados: {token_count}\")\n",
        "print(f\"Tiempo total (incluye carga): {total_time:.2f} s\")\n",
        "print(f\"Tiempo de inferencia (solo generaci√≥n): {infer_time:.2f} s\")\n",
        "print(f\"Velocidad de generaci√≥n: {token_count / infer_time:.2f} tokens/seg\")\n",
        "print(f\"RAM usada: {ram_after - ram_before:.2f} GB\")\n",
        "print(f\"Uso CPU promedio durante inferencia: {avg_cpu:.1f}%\")\n",
        "print(f\"VRAM usada: {gpu_mem_after - gpu_mem_before:.2f} GB\")\n",
        "print(f\"Carga GPU: {gpu_load_after:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoxPozkRXX33"
      },
      "source": [
        "## Modelo Base + RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ix4heZm2XaKT"
      },
      "outputs": [],
      "source": [
        "#Para resetear ollama\n",
        "!kill -9 $(ps aux | grep '[o]llama' | awk '{print $2}')\n",
        "\n",
        "import subprocess\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3) # Esperar a que Ollama se cargue\n",
        "\n",
        "\n",
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# Initialize the LLaMA model\n",
        "llm = OllamaLLM(model=\"llama3:8b\") #aqui poinemos el modelo base\n",
        "\n",
        "\n",
        "\n",
        "# Importaci√≥n correcta para versiones recientes de Langchain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "import torch\n",
        "\n",
        "\n",
        "# Define la plantilla del prompt DETALLADA\n",
        "prompt_template = \"\"\"\n",
        "Eres un asistente legal experto en protecci√≥n de datos en Espa√±a.\n",
        "Tu tarea es analizar la legalidad de la situaci√≥n descrita en la 'Pregunta' bas√°ndote **principalmente** en los siguientes 'Textos Legales Recuperados'.\n",
        "Si los textos recuperados contienen suficiente informaci√≥n, responde √∫nicamente bas√°ndote en ellos.\n",
        "Si los textos no contienen una respuesta clara pero la pregunta se refiere a conceptos b√°sicos del RGPD, usa lo aprendido en el entrenamiento para dar una respuesta general, especificando que no proviene de los textos recuperados.\n",
        "Si la pregunta requiere informaci√≥n espec√≠fica que no est√° en los textos recuperados ni en tu entrenamiento, ind√≠calo claramente.\n",
        "\n",
        "Textos Legales Recuperados:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "An√°lisis Legal y Conclusi√≥n:\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Crea la cadena usando load_qa_chain con el prompt personalizado\n",
        "# chain_type=\"stuff\" es adecuado si los chunks recuperados + pregunta caben en la ventana de contexto del LLM.\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=PROMPT)\n",
        "\n",
        "print (\"-\"*50)\n",
        "print (\"Asistente legal de protecci√≥n de datos listo (usando load_qa_chain). Escribe 'Exit' para salir.\")\n",
        "print (\"-\"*50)\n",
        "\n",
        "\n",
        "query=pregunta\n",
        "# 1. Recupera los documentos relevantes\n",
        "# Se puede ajustar 'k' aqu√≠ si quieres m√°s/menos contexto: retriever.search_kwargs = {'k': 5}\n",
        "docs_retrieved = retriever.invoke(query)\n",
        "\n",
        "# Prepara el input para la cadena\n",
        "input_data = {\n",
        "    \"input_documents\": docs_retrieved,\n",
        "    \"question\": query\n",
        "}\n",
        "\n",
        "# 2. Ejecuta la cadena de generaci√≥n con los documentos recuperados y la pregunta\n",
        "# Si usas una versi√≥n de Langchain > 0.1.0, invoke devuelve un diccionario.\n",
        "# Si usas una versi√≥n anterior, podr√≠a devolver directamente el string.\n",
        "# El par√°metro return_only_outputs=True ya no es necesario/v√°lido en invoke para versiones > 0.1.0\n",
        "result = chain.invoke(input_data)\n",
        "\n",
        "# Imprime la salida del LLM (la clave suele ser 'output_text' en versiones recientes)\n",
        "if isinstance(result, dict) and 'output_text' in result:\n",
        "    print(\"\\nRespuesta:\")\n",
        "    print(result['output_text'])\n",
        "elif isinstance(result, str): # Compatibilidad con versiones anteriores\n",
        "      print(\"\\nRespuesta:\")\n",
        "      print(result)\n",
        "else:\n",
        "      print(\"\\nRespuesta recibida (formato inesperado):\")\n",
        "      print(result)\n",
        "\n",
        "\n",
        "print(\"\\nSaliendo del asistente.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8VmL4S2W7Zf"
      },
      "source": [
        "## Finetunning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkurbP2nW_9W"
      },
      "source": [
        "### 8bit Q8_0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWCIODDTLDQ3"
      },
      "outputs": [],
      "source": [
        "!ollama run hf.co/serdom02/Leyeneitor_8bitQ8_0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sI2VEYV6XWmD"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import psutil\n",
        "import GPUtil\n",
        "from statistics import mean\n",
        "import threading\n",
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# --- Reiniciar Ollama ---\n",
        "!kill -9 $(ps aux | grep '[o]llama' | awk '{print $2}')\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "time.sleep(3)  # Esperar a que Ollama est√© listo\n",
        "\n",
        "# --- Configuraci√≥n de prueba ---\n",
        "model_name = \"hf.co/serdom02/Leyeneitor_8bitQ8_0\"\n",
        "\n",
        "# --- Monitorear uso de RAM ---\n",
        "def get_system_metrics():\n",
        "    ram_used = psutil.virtual_memory().used / (1024 ** 3)  # GB\n",
        "    return ram_used\n",
        "\n",
        "# --- Monitorear uso de GPU (VRAM) ---\n",
        "def get_gpu_metrics():\n",
        "    gpus = GPUtil.getGPUs()\n",
        "    if gpus:\n",
        "        gpu = gpus[0]\n",
        "        return gpu.memoryUsed / 1024, gpu.load * 100  # GB, %\n",
        "    return 0, 0\n",
        "\n",
        "# --- Iniciar modelo ---\n",
        "llm = OllamaLLM(model=model_name, max_tokens=250)\n",
        "\n",
        "# --- Medici√≥n de rendimiento ---\n",
        "start_total = time.time()\n",
        "\n",
        "ram_before = get_system_metrics()\n",
        "gpu_mem_before, gpu_load_before = get_gpu_metrics()\n",
        "\n",
        "# --- Medici√≥n activa de CPU en paralelo ---\n",
        "cpu_usage_during_infer = []\n",
        "stop_cpu_monitor = False\n",
        "\n",
        "def monitor_cpu():\n",
        "    while not stop_cpu_monitor:\n",
        "        cpu = psutil.cpu_percent(interval=0.1)\n",
        "        cpu_usage_during_infer.append(cpu)\n",
        "\n",
        "cpu_thread = threading.Thread(target=monitor_cpu)\n",
        "cpu_thread.start()\n",
        "\n",
        "# --- Inferencia ---\n",
        "start_infer = time.time()\n",
        "response = llm.invoke(pregunta)\n",
        "end_infer = time.time()\n",
        "\n",
        "# --- Detener CPU monitor ---\n",
        "stop_cpu_monitor = True\n",
        "cpu_thread.join()\n",
        "\n",
        "ram_after = get_system_metrics()\n",
        "gpu_mem_after, gpu_load_after = get_gpu_metrics()\n",
        "end_total = time.time()\n",
        "\n",
        "# --- M√©tricas calculadas ---\n",
        "total_time = end_total - start_total\n",
        "infer_time = end_infer - start_infer\n",
        "token_count = len(response.split())\n",
        "avg_cpu = mean(cpu_usage_during_infer) if cpu_usage_during_infer else 0\n",
        "\n",
        "# --- Salida ---\n",
        "print(\"\\n--- RESPUESTA ---\")\n",
        "print(response)\n",
        "\n",
        "print(\"\\n--- M√âTRICAS ---\")\n",
        "print(f\"Modelo: {model_name}\")\n",
        "print(f\"Tokens generados: {token_count}\")\n",
        "print(f\"Tiempo total (incluye carga): {total_time:.2f} s\")\n",
        "print(f\"Tiempo de inferencia (solo generaci√≥n): {infer_time:.2f} s\")\n",
        "print(f\"Velocidad de generaci√≥n: {token_count / infer_time:.2f} tokens/seg\")\n",
        "print(f\"RAM usada: {ram_after - ram_before:.2f} GB\")\n",
        "print(f\"Uso CPU promedio durante inferencia: {avg_cpu:.1f}%\")\n",
        "print(f\"VRAM usada: {gpu_mem_after - gpu_mem_before:.2f} GB\")\n",
        "print(f\"Carga GPU: {gpu_load_after:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBPrNkBgXSRg"
      },
      "source": [
        "### 16bit GGUF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMDE2J2MLN42"
      },
      "outputs": [],
      "source": [
        "!ollama run hf.co/serdom02/model_16bitGGUF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NTWJXFTKXXHU"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import psutil\n",
        "import GPUtil\n",
        "from statistics import mean\n",
        "import threading\n",
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# --- Reiniciar Ollama ---\n",
        "!kill -9 $(ps aux | grep '[o]llama' | awk '{print $2}')\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "time.sleep(3)  # Esperar a que Ollama est√© listo\n",
        "\n",
        "# --- Configuraci√≥n de prueba ---\n",
        "model_name = \"hf.co/serdom02/model_16bitGGUF\"\n",
        "\n",
        "# --- Monitorear uso de RAM ---\n",
        "def get_system_metrics():\n",
        "    ram_used = psutil.virtual_memory().used / (1024 ** 3)  # GB\n",
        "    return ram_used\n",
        "\n",
        "# --- Monitorear uso de GPU (VRAM) ---\n",
        "def get_gpu_metrics():\n",
        "    gpus = GPUtil.getGPUs()\n",
        "    if gpus:\n",
        "        gpu = gpus[0]\n",
        "        return gpu.memoryUsed / 1024, gpu.load * 100  # GB, %\n",
        "    return 0, 0\n",
        "\n",
        "# --- Iniciar modelo ---\n",
        "llm = OllamaLLM(model=model_name, max_tokens=250)\n",
        "\n",
        "# --- Medici√≥n de rendimiento ---\n",
        "start_total = time.time()\n",
        "\n",
        "ram_before = get_system_metrics()\n",
        "gpu_mem_before, gpu_load_before = get_gpu_metrics()\n",
        "\n",
        "# --- Medici√≥n activa de CPU en paralelo ---\n",
        "cpu_usage_during_infer = []\n",
        "stop_cpu_monitor = False\n",
        "\n",
        "def monitor_cpu():\n",
        "    while not stop_cpu_monitor:\n",
        "        cpu = psutil.cpu_percent(interval=0.1)\n",
        "        cpu_usage_during_infer.append(cpu)\n",
        "\n",
        "cpu_thread = threading.Thread(target=monitor_cpu)\n",
        "cpu_thread.start()\n",
        "\n",
        "# --- Inferencia ---\n",
        "start_infer = time.time()\n",
        "response = llm.invoke(pregunta)\n",
        "end_infer = time.time()\n",
        "\n",
        "# --- Detener CPU monitor ---\n",
        "stop_cpu_monitor = True\n",
        "cpu_thread.join()\n",
        "\n",
        "ram_after = get_system_metrics()\n",
        "gpu_mem_after, gpu_load_after = get_gpu_metrics()\n",
        "end_total = time.time()\n",
        "\n",
        "# --- M√©tricas calculadas ---\n",
        "total_time = end_total - start_total\n",
        "infer_time = end_infer - start_infer\n",
        "token_count = len(response.split())\n",
        "avg_cpu = mean(cpu_usage_during_infer) if cpu_usage_during_infer else 0\n",
        "\n",
        "# --- Salida ---\n",
        "print(\"\\n--- RESPUESTA ---\")\n",
        "print(response)\n",
        "\n",
        "print(\"\\n--- M√âTRICAS ---\")\n",
        "print(f\"Modelo: {model_name}\")\n",
        "print(f\"Tokens generados: {token_count}\")\n",
        "print(f\"Tiempo total (incluye carga): {total_time:.2f} s\")\n",
        "print(f\"Tiempo de inferencia (solo generaci√≥n): {infer_time:.2f} s\")\n",
        "print(f\"Velocidad de generaci√≥n: {token_count / infer_time:.2f} tokens/seg\")\n",
        "print(f\"RAM usada: {ram_after - ram_before:.2f} GB\")\n",
        "print(f\"Uso CPU promedio durante inferencia: {avg_cpu:.1f}%\")\n",
        "print(f\"VRAM usada: {gpu_mem_after - gpu_mem_before:.2f} GB\")\n",
        "print(f\"Carga GPU: {gpu_load_after:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8SWdO38XT5k"
      },
      "source": [
        "### q4_k_m GGUF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d4j3eDmLRzH"
      },
      "outputs": [],
      "source": [
        "!ollama run hf.co/serdom02/model_q4_k_mGGUF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8VuZ5VWXXi1"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import psutil\n",
        "import GPUtil\n",
        "from statistics import mean\n",
        "import threading\n",
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# --- Reiniciar Ollama ---\n",
        "!kill -9 $(ps aux | grep '[o]llama' | awk '{print $2}')\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "time.sleep(3)  # Esperar a que Ollama est√© listo\n",
        "\n",
        "# --- Configuraci√≥n de prueba ---\n",
        "model_name = \"hf.co/serdom02/model_q4_k_mGGUF\"\n",
        "\n",
        "# --- Monitorear uso de RAM ---\n",
        "def get_system_metrics():\n",
        "    ram_used = psutil.virtual_memory().used / (1024 ** 3)  # GB\n",
        "    return ram_used\n",
        "\n",
        "# --- Monitorear uso de GPU (VRAM) ---\n",
        "def get_gpu_metrics():\n",
        "    gpus = GPUtil.getGPUs()\n",
        "    if gpus:\n",
        "        gpu = gpus[0]\n",
        "        return gpu.memoryUsed / 1024, gpu.load * 100  # GB, %\n",
        "    return 0, 0\n",
        "\n",
        "# --- Iniciar modelo ---\n",
        "llm = OllamaLLM(model=model_name, max_tokens=250)\n",
        "\n",
        "# --- Medici√≥n de rendimiento ---\n",
        "start_total = time.time()\n",
        "\n",
        "ram_before = get_system_metrics()\n",
        "gpu_mem_before, gpu_load_before = get_gpu_metrics()\n",
        "\n",
        "# --- Medici√≥n activa de CPU en paralelo ---\n",
        "cpu_usage_during_infer = []\n",
        "stop_cpu_monitor = False\n",
        "\n",
        "def monitor_cpu():\n",
        "    while not stop_cpu_monitor:\n",
        "        cpu = psutil.cpu_percent(interval=0.1)\n",
        "        cpu_usage_during_infer.append(cpu)\n",
        "\n",
        "cpu_thread = threading.Thread(target=monitor_cpu)\n",
        "cpu_thread.start()\n",
        "\n",
        "# --- Inferencia ---\n",
        "start_infer = time.time()\n",
        "response = llm.invoke(pregunta)\n",
        "end_infer = time.time()\n",
        "\n",
        "# --- Detener CPU monitor ---\n",
        "stop_cpu_monitor = True\n",
        "cpu_thread.join()\n",
        "\n",
        "ram_after = get_system_metrics()\n",
        "gpu_mem_after, gpu_load_after = get_gpu_metrics()\n",
        "end_total = time.time()\n",
        "\n",
        "# --- M√©tricas calculadas ---\n",
        "total_time = end_total - start_total\n",
        "infer_time = end_infer - start_infer\n",
        "token_count = len(response.split())\n",
        "avg_cpu = mean(cpu_usage_during_infer) if cpu_usage_during_infer else 0\n",
        "\n",
        "# --- Salida ---\n",
        "print(\"\\n--- RESPUESTA ---\")\n",
        "print(response)\n",
        "\n",
        "print(\"\\n--- M√âTRICAS ---\")\n",
        "print(f\"Modelo: {model_name}\")\n",
        "print(f\"Tokens generados: {token_count}\")\n",
        "print(f\"Tiempo total (incluye carga): {total_time:.2f} s\")\n",
        "print(f\"Tiempo de inferencia (solo generaci√≥n): {infer_time:.2f} s\")\n",
        "print(f\"Velocidad de generaci√≥n: {token_count / infer_time:.2f} tokens/seg\")\n",
        "print(f\"RAM usada: {ram_after - ram_before:.2f} GB\")\n",
        "print(f\"Uso CPU promedio durante inferencia: {avg_cpu:.1f}%\")\n",
        "print(f\"VRAM usada: {gpu_mem_after - gpu_mem_before:.2f} GB\")\n",
        "print(f\"Carga GPU: {gpu_load_after:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWMJxQ09X0f2"
      },
      "source": [
        "## Finetunning + RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFcUQV9HX2sX"
      },
      "source": [
        "### 8bit Q8_0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sA2brIQWYDVE"
      },
      "outputs": [],
      "source": [
        "#Para resetear ollama\n",
        "!kill -9 $(ps aux | grep '[o]llama' | awk '{print $2}')\n",
        "\n",
        "import subprocess\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3) # Esperar a que Ollama se cargue\n",
        "\n",
        "\n",
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# Initialize the LLaMA model\n",
        "llm = OllamaLLM(model=\"hf.co/serdom02/model_8bitQ8_0\") #aqui poinemos el modelo base\n",
        "\n",
        "# Importaci√≥n correcta para versiones recientes de Langchain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "import torch\n",
        "\n",
        "\n",
        "# Define la plantilla del prompt DETALLADA\n",
        "prompt_template = \"\"\"\n",
        "Eres un asistente legal experto en protecci√≥n de datos en Espa√±a.\n",
        "Tu tarea es analizar la legalidad de la situaci√≥n descrita en la 'Pregunta' bas√°ndote **principalmente** en los siguientes 'Textos Legales Recuperados'.\n",
        "Si los textos recuperados contienen suficiente informaci√≥n, responde √∫nicamente bas√°ndote en ellos.\n",
        "Si los textos no contienen una respuesta clara pero la pregunta se refiere a conceptos b√°sicos del RGPD, usa lo aprendido en el entrenamiento para dar una respuesta general, especificando que no proviene de los textos recuperados.\n",
        "Si la pregunta requiere informaci√≥n espec√≠fica que no est√° en los textos recuperados ni en tu entrenamiento, ind√≠calo claramente.\n",
        "\n",
        "Textos Legales Recuperados:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "An√°lisis Legal y Conclusi√≥n:\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Crea la cadena usando load_qa_chain con el prompt personalizado\n",
        "# chain_type=\"stuff\" es adecuado si los chunks recuperados + pregunta caben en la ventana de contexto del LLM.\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=PROMPT)\n",
        "\n",
        "print (\"-\"*50)\n",
        "print (\"Asistente legal de protecci√≥n de datos listo (usando load_qa_chain). Escribe 'Exit' para salir.\")\n",
        "print (\"-\"*50)\n",
        "\n",
        "\n",
        "query=pregunta\n",
        "# 1. Recupera los documentos relevantes\n",
        "# Se puede ajustar 'k' aqu√≠ si quieres m√°s/menos contexto: retriever.search_kwargs = {'k': 5}\n",
        "docs_retrieved = retriever.invoke(query)\n",
        "\n",
        "# Prepara el input para la cadena\n",
        "input_data = {\n",
        "    \"input_documents\": docs_retrieved,\n",
        "    \"question\": query\n",
        "}\n",
        "\n",
        "# 2. Ejecuta la cadena de generaci√≥n con los documentos recuperados y la pregunta\n",
        "# Si usas una versi√≥n de Langchain > 0.1.0, invoke devuelve un diccionario.\n",
        "# Si usas una versi√≥n anterior, podr√≠a devolver directamente el string.\n",
        "# El par√°metro return_only_outputs=True ya no es necesario/v√°lido en invoke para versiones > 0.1.0\n",
        "result = chain.invoke(input_data)\n",
        "\n",
        "# Imprime la salida del LLM (la clave suele ser 'output_text' en versiones recientes)\n",
        "if isinstance(result, dict) and 'output_text' in result:\n",
        "    print(\"\\nRespuesta:\")\n",
        "    print(result['output_text'])\n",
        "elif isinstance(result, str): # Compatibilidad con versiones anteriores\n",
        "      print(\"\\nRespuesta:\")\n",
        "      print(result)\n",
        "else:\n",
        "      print(\"\\nRespuesta recibida (formato inesperado):\")\n",
        "      print(result)\n",
        "\n",
        "\n",
        "print(\"\\nSaliendo del asistente.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz5jjApKYDsc"
      },
      "source": [
        "### 16bit GGUF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLKZ3HsTYG4l"
      },
      "outputs": [],
      "source": [
        "#Para resetear ollama\n",
        "!kill -9 $(ps aux | grep '[o]llama' | awk '{print $2}')\n",
        "\n",
        "import subprocess\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3) # Esperar a que Ollama se cargue\n",
        "\n",
        "\n",
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# Initialize the LLaMA model\n",
        "llm = OllamaLLM(model=\"hf.co/serdom02/model_16bitGGUF\") #aqui poinemos el modelo base\n",
        "\n",
        "# Importaci√≥n correcta para versiones recientes de Langchain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "import torch\n",
        "\n",
        "\n",
        "# Define la plantilla del prompt DETALLADA\n",
        "prompt_template = \"\"\"\n",
        "Eres un asistente legal experto en protecci√≥n de datos en Espa√±a.\n",
        "Tu tarea es analizar la legalidad de la situaci√≥n descrita en la 'Pregunta' bas√°ndote **principalmente** en los siguientes 'Textos Legales Recuperados'.\n",
        "Si los textos recuperados contienen suficiente informaci√≥n, responde √∫nicamente bas√°ndote en ellos.\n",
        "Si los textos no contienen una respuesta clara pero la pregunta se refiere a conceptos b√°sicos del RGPD, usa lo aprendido en el entrenamiento para dar una respuesta general, especificando que no proviene de los textos recuperados.\n",
        "Si la pregunta requiere informaci√≥n espec√≠fica que no est√° en los textos recuperados ni en tu entrenamiento, ind√≠calo claramente.\n",
        "\n",
        "Textos Legales Recuperados:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "An√°lisis Legal y Conclusi√≥n:\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Crea la cadena usando load_qa_chain con el prompt personalizado\n",
        "# chain_type=\"stuff\" es adecuado si los chunks recuperados + pregunta caben en la ventana de contexto del LLM.\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=PROMPT)\n",
        "\n",
        "print (\"-\"*50)\n",
        "print (\"Asistente legal de protecci√≥n de datos listo (usando load_qa_chain). Escribe 'Exit' para salir.\")\n",
        "print (\"-\"*50)\n",
        "\n",
        "\n",
        "query=pregunta\n",
        "# 1. Recupera los documentos relevantes\n",
        "# Se puede ajustar 'k' aqu√≠ si quieres m√°s/menos contexto: retriever.search_kwargs = {'k': 5}\n",
        "docs_retrieved = retriever.invoke(query)\n",
        "\n",
        "# Prepara el input para la cadena\n",
        "input_data = {\n",
        "    \"input_documents\": docs_retrieved,\n",
        "    \"question\": query\n",
        "}\n",
        "\n",
        "# 2. Ejecuta la cadena de generaci√≥n con los documentos recuperados y la pregunta\n",
        "# Si usas una versi√≥n de Langchain > 0.1.0, invoke devuelve un diccionario.\n",
        "# Si usas una versi√≥n anterior, podr√≠a devolver directamente el string.\n",
        "# El par√°metro return_only_outputs=True ya no es necesario/v√°lido en invoke para versiones > 0.1.0\n",
        "result = chain.invoke(input_data)\n",
        "\n",
        "# Imprime la salida del LLM (la clave suele ser 'output_text' en versiones recientes)\n",
        "if isinstance(result, dict) and 'output_text' in result:\n",
        "    print(\"\\nRespuesta:\")\n",
        "    print(result['output_text'])\n",
        "elif isinstance(result, str): # Compatibilidad con versiones anteriores\n",
        "      print(\"\\nRespuesta:\")\n",
        "      print(result)\n",
        "else:\n",
        "      print(\"\\nRespuesta recibida (formato inesperado):\")\n",
        "      print(result)\n",
        "\n",
        "\n",
        "print(\"\\nSaliendo del asistente.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QYAWjBEYFDn"
      },
      "source": [
        "### q4_k_m GGUF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dL5mYhqYEzo"
      },
      "outputs": [],
      "source": [
        "#Para resetear ollama\n",
        "!kill -9 $(ps aux | grep '[o]llama' | awk '{print $2}')\n",
        "\n",
        "import subprocess\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3) # Esperar a que Ollama se cargue\n",
        "\n",
        "\n",
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# Initialize the LLaMA model\n",
        "llm = OllamaLLM(model=\"hf.co/serdom02/model_q4_k_mGGUF\") #aqui poinemos el modelo base\n",
        "\n",
        "# Importaci√≥n correcta para versiones recientes de Langchain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "import torch\n",
        "\n",
        "\n",
        "# Define la plantilla del prompt DETALLADA\n",
        "prompt_template = \"\"\"\n",
        "Eres un asistente legal experto en protecci√≥n de datos en Espa√±a.\n",
        "Tu tarea es analizar la legalidad de la situaci√≥n descrita en la 'Pregunta' bas√°ndote **principalmente** en los siguientes 'Textos Legales Recuperados'.\n",
        "Si los textos recuperados contienen suficiente informaci√≥n, responde √∫nicamente bas√°ndote en ellos.\n",
        "Si los textos no contienen una respuesta clara pero la pregunta se refiere a conceptos b√°sicos del RGPD, usa lo aprendido en el entrenamiento para dar una respuesta general, especificando que no proviene de los textos recuperados.\n",
        "Si la pregunta requiere informaci√≥n espec√≠fica que no est√° en los textos recuperados ni en tu entrenamiento, ind√≠calo claramente.\n",
        "\n",
        "Textos Legales Recuperados:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "An√°lisis Legal y Conclusi√≥n:\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Crea la cadena usando load_qa_chain con el prompt personalizado\n",
        "# chain_type=\"stuff\" es adecuado si los chunks recuperados + pregunta caben en la ventana de contexto del LLM.\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=PROMPT)\n",
        "\n",
        "print (\"-\"*50)\n",
        "print (\"Asistente legal de protecci√≥n de datos listo (usando load_qa_chain). Escribe 'Exit' para salir.\")\n",
        "print (\"-\"*50)\n",
        "\n",
        "\n",
        "query=pregunta\n",
        "# 1. Recupera los documentos relevantes\n",
        "# Se puede ajustar 'k' aqu√≠ si quieres m√°s/menos contexto: retriever.search_kwargs = {'k': 5}\n",
        "docs_retrieved = retriever.invoke(query)\n",
        "\n",
        "# Prepara el input para la cadena\n",
        "input_data = {\n",
        "    \"input_documents\": docs_retrieved,\n",
        "    \"question\": query\n",
        "}\n",
        "\n",
        "# 2. Ejecuta la cadena de generaci√≥n con los documentos recuperados y la pregunta\n",
        "# Si usas una versi√≥n de Langchain > 0.1.0, invoke devuelve un diccionario.\n",
        "# Si usas una versi√≥n anterior, podr√≠a devolver directamente el string.\n",
        "# El par√°metro return_only_outputs=True ya no es necesario/v√°lido en invoke para versiones > 0.1.0\n",
        "result = chain.invoke(input_data)\n",
        "\n",
        "# Imprime la salida del LLM (la clave suele ser 'output_text' en versiones recientes)\n",
        "if isinstance(result, dict) and 'output_text' in result:\n",
        "    print(\"\\nRespuesta:\")\n",
        "    print(result['output_text'])\n",
        "elif isinstance(result, str): # Compatibilidad con versiones anteriores\n",
        "      print(\"\\nRespuesta:\")\n",
        "      print(result)\n",
        "else:\n",
        "      print(\"\\nRespuesta recibida (formato inesperado):\")\n",
        "      print(result)\n",
        "\n",
        "\n",
        "print(\"\\nSaliendo del asistente.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVn5G4j93f2x"
      },
      "source": [
        "# Servidor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VzaBVm53lVJ"
      },
      "source": [
        "En este apartado se configura el backend que da servicio a la pagina web que usa el usuario para interactuar con el modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9r5qu7R3lwa"
      },
      "source": [
        "## Paso 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQ3wKgOb3oN_"
      },
      "source": [
        "Creamos la funci√≥n que vamos a utilizar para interactuar con el modelo (Hay que ejecutar antes las celdas de la parte Aplicaci√≥n Final Mejorada)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1In3ms-6fe-V"
      },
      "source": [
        "Se Asocia una instancia de ImprovedMemory a cada nombre de usuario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwsZfjXJk6x-"
      },
      "outputs": [],
      "source": [
        "def responder_web(pregunta: str, usuario: str, app) -> str:\n",
        "    try:\n",
        "        sesiones = app.state.user_sessions\n",
        "        usuario = usuario.strip().lower()\n",
        "\n",
        "        # Obtener o crear sesi√≥n del usuario\n",
        "        if usuario not in sesiones:\n",
        "            sesiones[usuario] = UsuarioSession()\n",
        "\n",
        "        sesion = sesiones[usuario]\n",
        "\n",
        "        # Inyectar el last_query en get_response_with_retry (requiere adaptaci√≥n)\n",
        "        result = get_response_with_retry(\n",
        "            query=pregunta,\n",
        "            llm=llm,\n",
        "            base_retriever=base_retriever,\n",
        "            memory=sesion.memory,\n",
        "            last_query=sesion.last_query\n",
        "        )\n",
        "\n",
        "        # Actualizar last_query\n",
        "        sesion.last_query = pregunta\n",
        "\n",
        "        return result.get(\"response\", \"No se pudo generar una respuesta.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error en responder_web: {e}\", exc_info=True)\n",
        "        return f\"Ocurri√≥ un error al procesar tu consulta: {e}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rw4mRKzz36rJ"
      },
      "source": [
        "## Paso 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E448E2T39bL"
      },
      "source": [
        "Instalamos la dependencias y creamos la API usando FastAPI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLqVQo1j37cT"
      },
      "outputs": [],
      "source": [
        "!pip install fastapi uvicorn nest_asyncio pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdBIkRHy4B_y"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "import uvicorn\n",
        "from fastapi import FastAPI, Request\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Necesario para que FastAPI funcione dentro del notebook\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Crea la app\n",
        "app = FastAPI()\n",
        "app.state.user_sessions = {}\n",
        "\n",
        "# CORS para permitir acceso desde GitHub Pages u otro frontend\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # O restringe a tu dominio exacto\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "@app.post(\"/reset\") #peticiones HTTP POST que lleguen a la ruta /reset por ej POST https://ip.ngrok.io/reset\n",
        "async def reset_memoria(request: Request):\n",
        "    datos = await request.json()\n",
        "    usuario = datos.get(\"usuario\", \"\").strip().lower()\n",
        "    sesiones = request.app.state.user_sessions\n",
        "\n",
        "    logger.info(f\"Solicitud de reseteo para: {usuario}\")\n",
        "    if usuario in sesiones:\n",
        "        del sesiones[usuario]\n",
        "        logger.info(f\"Memoria borrada para {usuario}\")\n",
        "        return {\"status\": \"ok\", \"mensaje\": f\"Memoria reiniciada para {usuario}\"}\n",
        "    else:\n",
        "        logger.warning(f\"No se encontr√≥ memoria activa para {usuario}\")\n",
        "        return {\"status\": \"no-op\", \"mensaje\": \"Usuario no ten√≠a memoria activa\"}\n",
        "\n",
        "\n",
        "\n",
        "# Ruta del chat\n",
        "@app.post(\"/chat\")\n",
        "async def chat(request: Request):\n",
        "    datos = await request.json()\n",
        "    pregunta = datos.get(\"mensaje\", \"\")\n",
        "    usuario = datos.get(\"usuario\", \"invitado\").strip().lower()\n",
        "\n",
        "    respuesta = responder_web(pregunta, usuario, request.app)\n",
        "    return {\"respuesta\": respuesta}\n",
        "\n",
        "# Crear t√∫nel ngrok (puedes copiar la URL que imprime)\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"Tu servidor est√° disponible en: {public_url}\")\n",
        "\n",
        "# Lanzar el servidor\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VFohj9Ife-W"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "NgUHfiVbwTfQ"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}