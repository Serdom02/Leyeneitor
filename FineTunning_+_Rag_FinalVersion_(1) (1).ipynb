{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgUHfiVbwTfQ"
      },
      "source": [
        "# GENERAR DATASET FORMA AUTOMATICA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUqzE0k2U1lb"
      },
      "source": [
        "> **Para ejecutar el siguiente codigo no hace falta estar conectados a una GPU**\n",
        "\n",
        "\n",
        "El siguiente codigo nos permite generar preguntas para nuestro dataset usando OpenRouter, en esta web podemos usar casi todos los modelos del mercado pero para generar el dataset vamos a limitarnos a las versiones gratuitas de DeepSeek-R1 y DeepSeek-V3\n",
        "\n",
        "La API de OpenRouter tiene limitaciones cuando se usan modelos gratuitos:\n",
        "\n",
        "\n",
        "> Free limit: If you are using a free model variant (with an ID ending in :free), then you will be limited to 20 requests per minute and 200 requests per day.\n",
        "\n",
        "\n",
        "\n",
        "El siguiente codigo tiene dos modos:\n",
        "\n",
        "*   Modo de √∫nico Prompt: Le metemos solo un prompt del excel ([Prompts para generar el dataset usando distintos modelos](https://docs.google.com/spreadsheets/d/1MQF8Z5_HqVSOzDXD8Ya7zEljJ13rD1Kw9FsUd6xhEJo/edit?pli=1&gid=0#gid=0)) y nos va a generar las preguntas y las respuestas siguiendo las instrucciones del prompt\n",
        "\n",
        "*   Modo M√∫ltiples Prompts: Copiamos distintas preguntas creadas por nosotros o por otros modelos LLM a un archivo de texto o csv, este modo adem√°s permite darle un prompt inicial de contexto antes de que empiece a generar las respuestas a nuestras preguntas, hay un ejemplo de prompt de contexto en el ([excel de prompts](https://docs.google.com/spreadsheets/d/1MQF8Z5_HqVSOzDXD8Ya7zEljJ13rD1Kw9FsUd6xhEJo/edit?pli=1&gid=0#gid=0))\n",
        "\n",
        "Una vez genera el contenido, nos descarga el csv con las preguntas, las respuestas, y el modelo que ha utilizado\n",
        "\n",
        "Se pueden generar varios modelos para que ambos generen preguntas y se vayan alternando.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBo74hgrStBc"
      },
      "source": [
        "**Paquita dice que antes de dar a play lo pienses dos veces a la hora de agregar cuantas consultas quieres hacer, ya que si no tienes un conteo de las restantes en la API el codigo mostrar√° que no se pueden generar m√°s consultas y el csv no se generar√° con las consultas antes del error**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw-k3y8dY9c5"
      },
      "source": [
        "## Codigo para generar el contenido del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "edd2ae155b8f45658f0fed29db260588",
            "848cfb3b291b46b2b5ad89ae0b370abc",
            "9a4aa0c0a5844740abe4d275fe038c43",
            "76b661b674b845738960e48caba5ae30",
            "8aa62bcdb50444f68cbcb6d8da3c8a5b",
            "1681c2a889474147a281aa57ea49fa01",
            "8d3865be2e614d97a600f65d8fcd2f53",
            "f52a496379bf4f91bba56679c7495c67",
            "898936f8509843b089c36807a31e9f73",
            "2f913635f3df441b82a5374c8a4f7422",
            "536953943b1a44ad9c8953bd0e5e5260"
          ]
        },
        "id": "SZh0d5RstAcX",
        "outputId": "5bf8064d-b45e-4e5f-9145-eebaab902f18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.1.0\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<h3>Generador de Dataset para Fine-Tuning</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Introduce tu clave API de OpenRouter: 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\n",
            "\n",
            "Introduce el prompt general (escribe o pega el texto y presiona Enter dos veces para finalizar):\n",
            "Para terminar la entrada, escribe una l√≠nea que solo contenga '***FIN***'\n",
            "üí° Objetivo: Generar UN √öNICO par de pregunta-respuesta sobre privacidad y protecci√≥n de datos en Espa√±a para entrenar un modelo con RAG.  ‚ö†Ô∏è INSTRUCCI√ìN CRUCIAL: Genera S√ìLO UN par pregunta-respuesta en cada ejecuci√≥n. No incluyas m√∫ltiples ejemplos ni numeraci√≥n.  üìä Temas a cubrir (diversidad tem√°tica): üîπ Derechos digitales: acceso, rectificaci√≥n, supresi√≥n, oposici√≥n, portabilidad, limitaci√≥n, olvido. üîπ Consentimiento: validez, revocaci√≥n, menores, excepciones. üîπ Cookies y tracking: tipos, banners, rechazos, perfilado. üîπ Datos sensibles: salud, biom√©tricos, ideolog√≠a, orientaci√≥n sexual. üîπ Contextos espec√≠ficos: laboral, educativo, sanitario, financiero, comercial. üîπ Tecnolog√≠as emergentes: IA, reconocimiento facial, IoT, blockchain. üîπ Seguridad: brechas, notificaciones, medidas t√©cnicas. üîπ Transferencias: internacionales, entre empresas, cesiones. üîπ Videovigilancia: √°mbito privado, p√∫blico, laboral. üîπ Responsabilidades: empresas, DPO, encargados, autoridades. üîπ Sanciones: sin mencionar cantidades espec√≠ficas.  üîπ Caracter√≠sticas de las preguntas: ‚úîÔ∏è Pregunta concreta (10-30 palabras) enfocada en un √∫nico tema ‚úîÔ∏è Evita preguntas gen√©ricas; usa casos pr√°cticos realistas ‚úîÔ∏è Incluye contexto espec√≠fico (qui√©n, d√≥nde, situaci√≥n concreta) ‚úîÔ∏è Var√≠a la formulaci√≥n: usa \"¬øEs legal...?\", \"¬øQu√© derechos tengo si...?\", \"¬øQu√© ocurre cuando...?\" ‚úîÔ∏è Aseg√∫rate de que sea una pregunta ORIGINAL y diferente a los ejemplos  üîπ Caracter√≠sticas de las respuestas: ‚úÖ 2-4 frases concisas pero completas ‚úÖ Explica condiciones y matices (no solo \"s√≠/no\") ‚úÖ Lenguaje claro sin jerga legal ‚úÖ No menciones art√≠culos o leyes espec√≠ficas ‚úÖ Tono profesional pero accesible ‚úÖ Informaci√≥n actualizada seg√∫n RGPD y LOPDGDD  üîπ Precisi√≥n de las respuestas: ‚úÖ Aseg√∫rate de que la respuesta refleje con precisi√≥n el marco legal espa√±ol actual (RGPD y LOPDGDD) ‚úÖ Si existe ambig√ºedad legal, menci√≥nalo expl√≠citamente ‚úÖ Evita respuestas que puedan resultar enga√±osas por simplificar en exceso ‚úÖ No incluyas opiniones personales o interpretaciones controvertidas   üîπ Tono de la respuesta: ‚úÖ Profesional pero accesible ‚úÖ Objetivo y no alarmista ‚úÖ Informativo sin ser condescendiente ‚úÖ Directo sin ser brusco  üîπ Escenarios espec√≠ficos a considerar (Estos son solo ejemplos puedes generar tus propios escenarios, se original): - Una persona intentando ejercer sus derechos frente a una gran empresa - Un empleado con preocupaciones sobre la privacidad en su trabajo - Un padre/madre preocupado por los datos de sus hijos menores - Un usuario de aplicaciones m√≥viles o servicios online - Un propietario de peque√±o negocio que necesita cumplir con la normativa - Situaciones en comunidades de vecinos o espacios compartidos - Interacciones con administraciones p√∫blicas - Espionaje entre compa√±eros de trabajo - Empresas comerciando con datos privador  ‚ö†Ô∏è IMPORTANTE: Aseg√∫rate de que la pregunta sea ORIGINAL y no se parezca demasiado a los ejemplos proporcionados. Busca √°ngulos o situaciones novedosas dentro del tema elegido.   ‚ö†Ô∏è FORMATO EXACTO A SEGUIR:   Pregunta sobre los temas nombrados anteriormente RESPUESTAMODELO Respuesta clara, razonada, y convincente  üìù Ejemplos (para referencia):   \"Un comercio online me instal√≥ cookies de seguimiento sin avisarme. ¬øEsto es legal?\" RESPUESTAMODELO \"No, en Espa√±a un sitio web solo puede instalar cookies de seguimiento si obtiene tu consentimiento previo. Deben informarte de manera clara sobre su uso y permitirte aceptarlas o rechazarlas f√°cilmente. Solo las cookies estrictamente necesarias pueden activarse sin tu permiso.\"  \"Una empresa de cr√©dito ha rechazado mi solicitud bas√°ndose en un algoritmo. ¬øPuedo exigir una explicaci√≥n?\" RESPUESTAMODELO \"S√≠, cuando una decisi√≥n automatizada afecta significativamente tus derechos, puedes solicitar una explicaci√≥n clara sobre los criterios utilizados. Tambi√©n puedes pedir una revisi√≥n manual si crees que la decisi√≥n ha sido injusta o err√≥nea.\"  \"Trabajo desde casa y mi empresa usa software de monitoreo en mi ordenador. ¬øHasta qu√© punto es legal?\" RESPUESTAMODELO \"El monitoreo es legal si est√° justificado y comunicado de manera transparente. Tu empresa debe informarte sobre qu√© datos recopila, con qu√© finalidad y durante cu√°nto tiempo. Adem√°s, el control debe ser proporcional y no invadir tu privacidad m√°s all√° de lo necesario para evaluar tu rendimiento laboral.\"\n",
            "***FIN***\n",
            "Introduce el n√∫mero de ejemplos a generar por modelo [10]: 10\n",
            "Introduce los modelos a utilizar (separados por comas) [deepseek/deepseek-chat:free]: \n",
            "Introduce el nombre del archivo de salida [dataset.csv]: \n",
            "\n",
            "Resumen de la configuraci√≥n:\n",
            "- Prompt general: 'üí° Objetivo: Generar UN √öNICO par de pregunta-respuesta sobre privacidad y protecci√≥n de datos en Esp...'\n",
            "- N√∫mero de iteraciones por modelo: 10\n",
            "- Modelos: deepseek/deepseek-chat:free\n",
            "- Archivo de salida: dataset.csv\n",
            "\n",
            "¬øConfirmar y comenzar la generaci√≥n? (s/n): s\n",
            "Modo: Prompt √∫nico - Generando 10 ejemplos por modelo\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "edd2ae155b8f45658f0fed29db260588",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generando dataset:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Procesando modelo: deepseek/deepseek-chat:free\n",
            "\n",
            "Iteraci√≥n 1/10:\n",
            "Tema: cookies y navegaci√≥n web\n",
            "Pregunta: \"¬øPuede una p√°gina web bloquear el acceso si rechazo todas las cookies excepto las estrictamente nec...\n",
            "Respuesta: No, en Espa√±a no es legal bloquear el acceso a un sitio web por rechazar cookies no esenciales. Solo...\n",
            "\n",
            "Iteraci√≥n 2/10:\n",
            "Tema: derechos ARCO\n",
            "Pregunta: \"Una empresa ha rechazado mi solicitud de eliminar mis datos personales alegando que los necesita pa...\n",
            "Respuesta: \"S√≠, si la empresa debe conservar tus datos para cumplir con una obligaci√≥n legal, puede rechazar tu...\n",
            "\n",
            "Iteraci√≥n 3/10:\n",
            "Tema: redes sociales\n",
            "Pregunta: ¬øPuedo pedir a una red social que elimine publicaciones antiguas donde aparezco sin mi consentimient...\n",
            "Respuesta: S√≠, tienes derecho a solicitar la eliminaci√≥n de publicaciones en las que apareces sin tu consentimi...\n",
            "\n",
            "Iteraci√≥n 4/10:\n",
            "Tema: videovigilancia\n",
            "Pregunta: \"Mi comunidad de vecinos ha instalado c√°maras en las zonas comunes sin informar a los residentes. ¬øE...\n",
            "Respuesta: \"No, la instalaci√≥n de c√°maras en zonas comunes debe ser comunicada previamente a los vecinos median...\n",
            "\n",
            "Iteraci√≥n 5/10:\n",
            "Tema: menores y consentimiento\n",
            "Pregunta: \"Un profesor quiere crear un grupo de WhatsApp con alumnos menores de 14 a√±os para enviarles tareas....\n",
            "Respuesta: S√≠, el consentimiento debe ser otorgado por los padres o tutores legales, ya que los menores de 14 a...\n",
            "\n",
            "Iteraci√≥n 6/10:\n",
            "Tema: geolocalizaci√≥n\n",
            "Pregunta: \"Una app de transporte me pide acceso constante a mi ubicaci√≥n, incluso cuando no la estoy usando. ¬ø...\n",
            "Respuesta: \"No, una app solo puede acceder a tu ubicaci√≥n cuando es estrictamente necesario para su funcionamie...\n",
            "\n",
            "Iteraci√≥n 7/10:\n",
            "Tema: datos biom√©tricos\n",
            "Pregunta: \"Mi empresa quiere implantar un sistema de control de acceso mediante reconocimiento facial. ¬øQu√© de...\n",
            "Respuesta: \"Tienes derecho a que te informen de manera clara sobre el uso, almacenamiento y finalidad de tus da...\n",
            "\n",
            "Iteraci√≥n 8/10:\n",
            "Tema: marketing directo\n",
            "Pregunta: \"Una tienda online me ha enviado publicidad por correo electr√≥nico sin mi consentimiento. ¬øQu√© puedo...\n",
            "Respuesta: \"Puedes solicitar que dejen de enviarte publicidad, ya que el marketing directo requiere tu consenti...\n",
            "\n",
            "Iteraci√≥n 9/10:\n",
            "Tema: filtraciones de datos\n",
            "Pregunta: He descubierto que mis datos personales han sido filtrados en una brecha de seguridad de mi banco. ¬ø...\n",
            "Respuesta: Debes notificar inmediatamente al banco para que tomen medidas y te informen sobre el alcance de la ...\n",
            "\n",
            "Iteraci√≥n 10/10:\n",
            "Tema: transferencias internacionales\n",
            "Pregunta: \"Una empresa estadounidense quiere transferir mis datos personales desde Espa√±a a sus servidores en ...\n",
            "Respuesta: \"Para que la transferencia sea legal, la empresa debe garantizar que tus datos estar√°n protegidos ba...\n",
            "Dataset guardado exitosamente en dataset.csv\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_643eea60-61a2-4f02-8106-c8bc46ed0da3\", \"dataset.csv\", 5121)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Se han generado 10 ejemplos √∫nicos para el dataset.\n",
            "‚ùå Se han detectado 0 ejemplos con formato incorrecto o duplicados.\n"
          ]
        }
      ],
      "source": [
        "!pip install requests tqdm python-dotenv\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import time\n",
        "import requests\n",
        "import json\n",
        "import re\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "def extract_question_answer(response_text: str) -> Tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Extrae la pregunta y respuesta de un texto generado por el modelo usando varios m√©todos.\n",
        "\n",
        "    Args:\n",
        "        response_text: Texto generado por el modelo.\n",
        "\n",
        "    Returns:\n",
        "        Tupla con (pregunta, respuesta).\n",
        "    \"\"\"\n",
        "    # Normalizar el texto para lidiar con diferentes formatos\n",
        "    text = response_text.strip()\n",
        "\n",
        "    # M√©todo 1: Buscar el delimitador exacto\n",
        "    if \"RESPUESTAMODELO\" in text:\n",
        "        parts = text.split(\"RESPUESTAMODELO\", 1)\n",
        "        return parts[0].strip(), parts[1].strip()\n",
        "\n",
        "    # M√©todo 2: Buscar variaciones del delimitador\n",
        "    for delimiter in [\"RESPUESTA MODELO\", \"Respuesta Modelo\", \"Respuesta:\", \"Respuesta del modelo:\"]:\n",
        "        if delimiter in text:\n",
        "            parts = text.split(delimiter, 1)\n",
        "            return parts[0].strip(), parts[1].strip()\n",
        "\n",
        "    # M√©todo 3: Buscar un patr√≥n de pregunta y respuesta\n",
        "    # Asumiendo que la pregunta termina con un signo de interrogaci√≥n\n",
        "    # y la respuesta comienza en la siguiente l√≠nea\n",
        "    if \"?\" in text:\n",
        "        # Encontrar la √∫ltima pregunta en el texto\n",
        "        question_parts = text.split(\"?\")\n",
        "        # La pregunta es todo hasta el √∫ltimo signo de interrogaci√≥n\n",
        "        all_but_last = \"?\".join(question_parts[:-1]) + \"?\"\n",
        "        last_part = question_parts[-1]\n",
        "\n",
        "        # Si hay m√°s de una l√≠nea despu√©s del signo de interrogaci√≥n,\n",
        "        # la primera l√≠nea podr√≠a ser parte de la pregunta\n",
        "        lines_after = last_part.strip().split(\"\\n\")\n",
        "\n",
        "        if len(lines_after) > 1 and not lines_after[0]:\n",
        "            # La respuesta comienza despu√©s de una l√≠nea en blanco\n",
        "            question = all_but_last.strip()\n",
        "            answer = \"\\n\".join(lines_after[1:]).strip()\n",
        "        else:\n",
        "            # La respuesta comienza inmediatamente despu√©s del signo de interrogaci√≥n\n",
        "            question = all_but_last.strip()\n",
        "            answer = last_part.strip()\n",
        "\n",
        "        return question, answer\n",
        "\n",
        "    # Si nada funciona, intenta una divisi√≥n por la mitad (√∫ltima opci√≥n)\n",
        "    lines = text.strip().split(\"\\n\")\n",
        "    if len(lines) >= 2:\n",
        "        # Asumimos que la mitad es pregunta y la otra mitad respuesta\n",
        "        midpoint = len(lines) // 2\n",
        "        return \"\\n\".join(lines[:midpoint]).strip(), \"\\n\".join(lines[midpoint:]).strip()\n",
        "\n",
        "    # Si todo falla, devuelve un error\n",
        "    print(f\"ERROR: No se pudo extraer la pregunta y respuesta. Texto completo:\\n{text}\")\n",
        "    return \"ERROR: No se pudo extraer la pregunta\", \"ERROR: No se pudo extraer la respuesta\"\n",
        "\n",
        "def validate_qa_pair(question: str, answer: str) -> bool:\n",
        "    \"\"\"\n",
        "    Valida que el par pregunta-respuesta sea coherente.\n",
        "\n",
        "    Args:\n",
        "        question: La pregunta extra√≠da.\n",
        "        answer: La respuesta extra√≠da.\n",
        "\n",
        "    Returns:\n",
        "        True si parece un par v√°lido, False en caso contrario.\n",
        "    \"\"\"\n",
        "    # La pregunta deber√≠a terminar con signo de interrogaci√≥n\n",
        "    has_question_mark = \"?\" in question\n",
        "\n",
        "    # Verificar longitudes m√≠nimas\n",
        "    question_length_ok = len(question.split()) >= 3\n",
        "    answer_length_ok = len(answer.split()) >= 5\n",
        "\n",
        "    # La respuesta no debe contener la palabra \"pregunta\" o \"question\"\n",
        "    no_question_in_answer = \"pregunta\" not in answer.lower() and \"question\" not in answer.lower()\n",
        "\n",
        "    # Verificar que no contenga instrucciones del formato\n",
        "    no_instructions = \"formato\" not in question.lower() and \"instrucciones\" not in question.lower()\n",
        "\n",
        "    # Verificar que no haya mensajes de error\n",
        "    no_errors = \"ERROR:\" not in question and \"ERROR:\" not in answer\n",
        "\n",
        "    return has_question_mark and question_length_ok and answer_length_ok and no_question_in_answer and no_instructions and no_errors\n",
        "\n",
        "def is_similar_to_existing(question: str, existing_questions: List[str], threshold: float = 0.7) -> bool:\n",
        "    \"\"\"\n",
        "    Comprueba si una pregunta es similar a las existentes usando una comparaci√≥n simple.\n",
        "\n",
        "    Args:\n",
        "        question: Pregunta a comprobar\n",
        "        existing_questions: Lista de preguntas existentes\n",
        "        threshold: Umbral de similitud (0-1)\n",
        "\n",
        "    Returns:\n",
        "        True si es similar, False si no\n",
        "    \"\"\"\n",
        "    # Normalizar la pregunta (min√∫sculas, sin puntuaci√≥n)\n",
        "    normalized_question = re.sub(r'[^\\w\\s]', '', question.lower())\n",
        "    words = set(normalized_question.split())\n",
        "\n",
        "    for existing in existing_questions:\n",
        "        normalized_existing = re.sub(r'[^\\w\\s]', '', existing.lower())\n",
        "        existing_words = set(normalized_existing.split())\n",
        "\n",
        "        # Calcular similitud Jaccard (proporci√≥n de palabras en com√∫n)\n",
        "        intersection = len(words.intersection(existing_words))\n",
        "        union = len(words.union(existing_words))\n",
        "\n",
        "        if union > 0 and intersection / union > threshold:\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def call_openrouter_api(\n",
        "    prompt: str,\n",
        "    model: str,\n",
        "    api_key: str,\n",
        "    system_message: Optional[str] = None,\n",
        "    max_retries: int = 3\n",
        ") -> Optional[Dict[Any, Any]]:\n",
        "    \"\"\"\n",
        "    Realiza una llamada a la API de OpenRouter.\n",
        "\n",
        "    Args:\n",
        "        prompt: Prompt a enviar.\n",
        "        model: Nombre del modelo a utilizar.\n",
        "        api_key: Clave de API de OpenRouter.\n",
        "        system_message: Mensaje de sistema opcional.\n",
        "        max_retries: N√∫mero m√°ximo de reintentos en caso de fallo.\n",
        "\n",
        "    Returns:\n",
        "        Respuesta de la API o None si hubo un error.\n",
        "    \"\"\"\n",
        "    url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": \"https://colab.research.google.com/\"\n",
        "    }\n",
        "\n",
        "    messages = []\n",
        "    if system_message:\n",
        "        messages.append({\"role\": \"system\", \"content\": system_message})\n",
        "\n",
        "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    data = {\n",
        "        \"model\": model,\n",
        "        \"messages\": messages\n",
        "    }\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.post(url, headers=headers, json=data)\n",
        "            response.raise_for_status()\n",
        "            return response.json()\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error en la llamada a la API (intento {attempt+1}/{max_retries}): {e}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                # Esperar un tiempo exponencial antes de reintentar\n",
        "                wait_time = 2 ** attempt\n",
        "                print(f\"Reintentando en {wait_time} segundos...\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                print(\"Se alcanz√≥ el n√∫mero m√°ximo de reintentos.\")\n",
        "                return None\n",
        "\n",
        "def extract_response(api_response: Dict[Any, Any]) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Extrae la respuesta del modelo desde la respuesta de la API.\n",
        "\n",
        "    Args:\n",
        "        api_response: Respuesta de la API.\n",
        "\n",
        "    Returns:\n",
        "        Texto de la respuesta o None si no se puede extraer.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return api_response['choices'][0]['message']['content']\n",
        "    except (KeyError, IndexError, TypeError) as e:\n",
        "        print(f\"Error al extraer la respuesta: {e}\")\n",
        "        print(f\"Respuesta completa de la API: {json.dumps(api_response, indent=2)}\")\n",
        "        return None\n",
        "\n",
        "def save_to_csv(data: List[Dict[str, str]], output_file: str) -> None:\n",
        "    \"\"\"\n",
        "    Guarda los datos en un archivo CSV y permite descargarlo desde Colab.\n",
        "\n",
        "    Args:\n",
        "        data: Lista de diccionarios con los datos a guardar.\n",
        "        output_file: Ruta al archivo de salida.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(output_file, 'w', encoding='utf-8', newline='') as file:\n",
        "            fieldnames = ['instruction', 'output', 'model']\n",
        "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "\n",
        "            writer.writeheader()\n",
        "            for item in data:\n",
        "                writer.writerow(item)\n",
        "\n",
        "        print(f\"Dataset guardado exitosamente en {output_file}\")\n",
        "\n",
        "        # Permitir la descarga del archivo desde Colab\n",
        "        files.download(output_file)\n",
        "    except Exception as e:\n",
        "        print(f\"Error al guardar el dataset: {e}\")\n",
        "\n",
        "def generate_dataset_single_prompt(\n",
        "    prompt: str,\n",
        "    output_file: str,\n",
        "    models: List[str],\n",
        "    api_key: str,\n",
        "    num_iterations: int = 10\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Genera un dataset usando un √∫nico prompt general m√∫ltiples veces.\n",
        "\n",
        "    Args:\n",
        "        prompt: Prompt general para generar preguntas y respuestas.\n",
        "        output_file: Ruta al archivo de salida.\n",
        "        models: Lista de modelos a utilizar.\n",
        "        api_key: Clave de API de OpenRouter.\n",
        "        num_iterations: N√∫mero de veces que se utilizar√° el prompt.\n",
        "    \"\"\"\n",
        "    print(f\"Modo: Prompt √∫nico - Generando {num_iterations} ejemplos por modelo\")\n",
        "\n",
        "    # Definir temas para rotar\n",
        "    topics = [\n",
        "        \"cookies y navegaci√≥n web\",\n",
        "        \"derechos ARCO\",\n",
        "        \"redes sociales\",\n",
        "        \"videovigilancia\",\n",
        "        \"menores y consentimiento\",\n",
        "        \"geolocalizaci√≥n\",\n",
        "        \"datos biom√©tricos\",\n",
        "        \"marketing directo\",\n",
        "        \"filtraciones de datos\",\n",
        "        \"transferencias internacionales\",\n",
        "        \"derecho al olvido\",\n",
        "        \"uso de IA con datos personales\",\n",
        "        \"aplicaciones m√≥viles\",\n",
        "        \"datos en el √°mbito laboral\",\n",
        "        \"informaci√≥n de salud\",\n",
        "        \"datos bancarios y financieros\"\n",
        "    ]\n",
        "\n",
        "    # Crear archivo para ejemplos fallidos\n",
        "    fallidos_file = f\"ejemplos_fallidos_{int(time.time())}.txt\"\n",
        "\n",
        "    # Preparar el dataset\n",
        "    dataset = []\n",
        "    existing_questions = []\n",
        "    fallidos = 0\n",
        "\n",
        "    # Procesar cada modelo y generar m√∫ltiples ejemplos\n",
        "    total_iterations = len(models) * num_iterations\n",
        "    with tqdm(total=total_iterations, desc=\"Generando dataset\") as pbar:\n",
        "        for model in models:\n",
        "            print(f\"\\nProcesando modelo: {model}\")\n",
        "\n",
        "            for i in range(num_iterations):\n",
        "                # Seleccionar tema para esta iteraci√≥n\n",
        "                current_topic = topics[i % len(topics)]\n",
        "\n",
        "                # Crear un prompt espec√≠fico para esta iteraci√≥n\n",
        "                iteration_prompt = f\"\"\"\n",
        "{prompt}\n",
        "\n",
        "‚ö†Ô∏è INSTRUCCI√ìN CRUCIAL: Genera UN √öNICO par de pregunta-respuesta sobre protecci√≥n de datos.\n",
        "\n",
        "TEMA ESPEC√çFICO: Genera una pregunta relacionada con \"{current_topic}\".\n",
        "Aseg√∫rate de que sea una pregunta concreta y relevante para usuarios espa√±oles.\n",
        "\n",
        "FORMATO EXACTO A SEGUIR:\n",
        "[Escribe aqu√≠ UNA √öNICA pregunta sobre {current_topic}]\n",
        "RESPUESTAMODELO\n",
        "[Escribe aqu√≠ la respuesta a esa √∫nica pregunta]\n",
        "\n",
        "RECUERDA: Solo UN par pregunta-respuesta. Termina tu respuesta despu√©s de contestar la pregunta.\n",
        "\"\"\"\n",
        "\n",
        "                # Intentar hasta 3 veces si obtenemos duplicados\n",
        "                max_attempts = 3\n",
        "                success = False\n",
        "\n",
        "                for attempt in range(max_attempts):\n",
        "                    # Llamar a la API\n",
        "                    response_data = call_openrouter_api(iteration_prompt, model, api_key)\n",
        "\n",
        "                    if response_data:\n",
        "                        # Extraer la respuesta\n",
        "                        response_text = extract_response(response_data)\n",
        "\n",
        "                        if response_text:\n",
        "                            # Extraer pregunta y respuesta del texto generado\n",
        "                            question, answer = extract_question_answer(response_text)\n",
        "\n",
        "                            # Validar el par pregunta-respuesta\n",
        "                            if validate_qa_pair(question, answer):\n",
        "                                # Verificar si es similar a preguntas existentes\n",
        "                                if not is_similar_to_existing(question, existing_questions):\n",
        "                                    # A√±adir al dataset\n",
        "                                    dataset.append({\n",
        "                                        'instruction': question,\n",
        "                                        'output': answer,\n",
        "                                        'model': model\n",
        "                                    })\n",
        "                                    # Guardar para futuras comparaciones\n",
        "                                    existing_questions.append(question)\n",
        "\n",
        "                                    print(f\"\\nIteraci√≥n {i+1}/{num_iterations}:\")\n",
        "                                    print(f\"Tema: {current_topic}\")\n",
        "                                    print(f\"Pregunta: {question[:100]}...\")\n",
        "                                    print(f\"Respuesta: {answer[:100]}...\")\n",
        "\n",
        "                                    success = True\n",
        "                                    break\n",
        "                                else:\n",
        "                                    print(f\"\\n‚ö†Ô∏è Intento {attempt+1}: Pregunta similar ya existe, reintentando...\")\n",
        "                                    # A√±adir m√°s variaci√≥n al prompt\n",
        "                                    iteration_prompt += f\"\\n\\nIMPORTANTE: Aseg√∫rate de que la pregunta sea ORIGINAL y DIFERENTE a esta: \\\"{question}\\\"\"\n",
        "                            else:\n",
        "                                print(f\"\\n‚ö†Ô∏è Intento {attempt+1}: Par pregunta-respuesta no v√°lido, reintentando...\")\n",
        "                                print(f\"\\nRespuesta completa del modelo:\")\n",
        "                                print(\"-\" * 50)\n",
        "                                print(response_text)\n",
        "                                print(\"-\" * 50)\n",
        "\n",
        "                if not success:\n",
        "                    fallidos += 1\n",
        "                    print(f\"\\n‚ùå No se pudo generar un par √∫nico en {max_attempts} intentos para el tema: {current_topic}\")\n",
        "\n",
        "                    # Guardar ejemplo fallido\n",
        "                    with open(fallidos_file, \"a\", encoding=\"utf-8\") as f:\n",
        "                        f.write(f\"--- EJEMPLO FALLIDO {fallidos} ---\\n\")\n",
        "                        f.write(f\"MODELO: {model}\\n\")\n",
        "                        f.write(f\"TEMA: {current_topic}\\n\")\n",
        "                        if 'response_text' in locals():\n",
        "                            f.write(f\"√öLTIMO INTENTO:\\n{response_text}\\n\\n\")\n",
        "                        f.write(\"-\" * 50 + \"\\n\\n\")\n",
        "\n",
        "                pbar.update(1)\n",
        "\n",
        "                # Peque√±a pausa para evitar sobrecargar la API\n",
        "                time.sleep(0.5)\n",
        "\n",
        "    # Guardar el dataset\n",
        "    if dataset:\n",
        "        save_to_csv(dataset, output_file)\n",
        "        print(f\"\\n‚úÖ Se han generado {len(dataset)} ejemplos √∫nicos para el dataset.\")\n",
        "        print(f\"‚ùå Se han detectado {fallidos} ejemplos con formato incorrecto o duplicados.\")\n",
        "        if fallidos > 0:\n",
        "            print(f\"üìù Los ejemplos fallidos se han guardado en '{fallidos_file}'.\")\n",
        "    else:\n",
        "        print(\"‚ùå No se pudieron generar ejemplos v√°lidos para el dataset.\")\n",
        "\n",
        "# Ejecutar en Colab (interfaz interactiva)\n",
        "def main_colab():\n",
        "    from IPython.display import display, HTML\n",
        "\n",
        "    display(HTML(\"<h3>Generador de Dataset para Fine-Tuning</h3>\"))\n",
        "\n",
        "    # Solicitar los par√°metros\n",
        "    api_key = input(\"Introduce tu clave API de OpenRouter: \")\n",
        "\n",
        "\n",
        "    # Entrada directa del prompt\n",
        "    print(\"\\nIntroduce el prompt general (escribe o pega el texto y presiona Enter dos veces para finalizar):\")\n",
        "    print(\"Para terminar la entrada, escribe una l√≠nea que solo contenga '***FIN***'\")\n",
        "\n",
        "    lines = []\n",
        "    while True:\n",
        "        line = input()\n",
        "        if line == \"***FIN***\":\n",
        "            break\n",
        "        lines.append(line)\n",
        "\n",
        "    general_prompt = \"\\n\".join(lines)\n",
        "\n",
        "    if not general_prompt.strip():\n",
        "        print(\"El prompt no puede estar vac√≠o.\")\n",
        "        return\n",
        "\n",
        "\n",
        "    # Solicitar el n√∫mero de iteraciones\n",
        "    num_iterations = input(\"Introduce el n√∫mero de ejemplos a generar por modelo [10]: \")\n",
        "    num_iterations = int(num_iterations) if num_iterations.strip() else 10\n",
        "\n",
        "    # Solicitar los modelos\n",
        "    models_input = input(\"Introduce los modelos a utilizar (separados por comas) [deepseek/deepseek-chat:free]: \")\n",
        "    models = [m.strip() for m in models_input.split(\",\")] if models_input.strip() else [\"deepseek/deepseek-chat:free\"]\n",
        "\n",
        "    # Solicitar el nombre del archivo de salida\n",
        "    output_file = input(\"Introduce el nombre del archivo de salida [dataset.csv]: \")\n",
        "    output_file = output_file if output_file.strip() else \"dataset.csv\"\n",
        "\n",
        "    print(\"\\nResumen de la configuraci√≥n:\")\n",
        "    print(f\"- Prompt general: '{general_prompt[:100]}...'\")\n",
        "    print(f\"- N√∫mero de iteraciones por modelo: {num_iterations}\")\n",
        "    print(f\"- Modelos: {', '.join(models)}\")\n",
        "    print(f\"- Archivo de salida: {output_file}\")\n",
        "\n",
        "    confirm = input(\"\\n¬øConfirmar y comenzar la generaci√≥n? (s/n): \")\n",
        "    if confirm.lower() in [\"s\", \"si\", \"s√≠\", \"y\", \"yes\"]:\n",
        "        generate_dataset_single_prompt(\n",
        "            prompt=general_prompt,\n",
        "            output_file=output_file,\n",
        "            models=models,\n",
        "            api_key=api_key,\n",
        "            num_iterations=num_iterations\n",
        "        )\n",
        "    else:\n",
        "        print(\"Operaci√≥n cancelada.\")\n",
        "\n",
        "\n",
        "\n",
        "# C√≥digo para ejecutar directamente en una celda de Colab\n",
        "main_colab()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn823uUfWxYC"
      },
      "source": [
        "Prompt √∫nico para generar el dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0t6fjKVrwSWJ"
      },
      "outputs": [],
      "source": [
        "üí° Objetivo: Generar UN √öNICO par de pregunta-respuesta sobre privacidad y protecci√≥n de datos en Espa√±a para entrenar un modelo con RAG.\n",
        "\n",
        "‚ö†Ô∏è INSTRUCCI√ìN CRUCIAL: Genera S√ìLO UN par pregunta-respuesta en cada ejecuci√≥n. No incluyas m√∫ltiples ejemplos ni numeraci√≥n.\n",
        "\n",
        "üìä Temas a cubrir (diversidad tem√°tica):\n",
        "üîπ Derechos digitales: acceso, rectificaci√≥n, supresi√≥n, oposici√≥n, portabilidad, limitaci√≥n, olvido.\n",
        "üîπ Consentimiento: validez, revocaci√≥n, menores, excepciones.\n",
        "üîπ Cookies y tracking: tipos, banners, rechazos, perfilado.\n",
        "üîπ Datos sensibles: salud, biom√©tricos, ideolog√≠a, orientaci√≥n sexual.\n",
        "üîπ Contextos espec√≠ficos: laboral, educativo, sanitario, financiero, comercial.\n",
        "üîπ Tecnolog√≠as emergentes: IA, reconocimiento facial, IoT, blockchain.\n",
        "üîπ Seguridad: brechas, notificaciones, medidas t√©cnicas.\n",
        "üîπ Transferencias: internacionales, entre empresas, cesiones.\n",
        "üîπ Videovigilancia: √°mbito privado, p√∫blico, laboral.\n",
        "üîπ Responsabilidades: empresas, DPO, encargados, autoridades.\n",
        "üîπ Sanciones: sin mencionar cantidades espec√≠ficas.\n",
        "\n",
        "üîπ Caracter√≠sticas de las preguntas:\n",
        "‚úîÔ∏è Pregunta concreta (10-30 palabras) enfocada en un √∫nico tema\n",
        "‚úîÔ∏è Evita preguntas gen√©ricas; usa casos pr√°cticos realistas\n",
        "‚úîÔ∏è Incluye contexto espec√≠fico (qui√©n, d√≥nde, situaci√≥n concreta)\n",
        "‚úîÔ∏è Var√≠a la formulaci√≥n: usa \"¬øEs legal...?\", \"¬øQu√© derechos tengo si...?\", \"¬øQu√© ocurre cuando...?\"\n",
        "‚úîÔ∏è Aseg√∫rate de que sea una pregunta ORIGINAL y diferente a los ejemplos\n",
        "\n",
        "üîπ Caracter√≠sticas de las respuestas:\n",
        "‚úÖ 2-4 frases concisas pero completas\n",
        "‚úÖ Explica condiciones y matices (no solo \"s√≠/no\")\n",
        "‚úÖ Lenguaje claro sin jerga legal\n",
        "‚úÖ No menciones art√≠culos o leyes espec√≠ficas\n",
        "‚úÖ Tono profesional pero accesible\n",
        "‚úÖ Informaci√≥n actualizada seg√∫n RGPD y LOPDGDD\n",
        "\n",
        "üîπ Precisi√≥n de las respuestas:\n",
        "‚úÖ Aseg√∫rate de que la respuesta refleje con precisi√≥n el marco legal espa√±ol actual (RGPD y LOPDGDD)\n",
        "‚úÖ Si existe ambig√ºedad legal, menci√≥nalo expl√≠citamente\n",
        "‚úÖ Evita respuestas que puedan resultar enga√±osas por simplificar en exceso\n",
        "‚úÖ No incluyas opiniones personales o interpretaciones controvertidas\n",
        "\n",
        "\n",
        "üîπ Tono de la respuesta:\n",
        "‚úÖ Profesional pero accesible\n",
        "‚úÖ Objetivo y no alarmista\n",
        "‚úÖ Informativo sin ser condescendiente\n",
        "‚úÖ Directo sin ser brusco\n",
        "\n",
        "üîπ Escenarios espec√≠ficos a considerar (Estos son solo ejemplos puedes generar tus propios escenarios, se original):\n",
        "- Una persona intentando ejercer sus derechos frente a una gran empresa\n",
        "- Un empleado con preocupaciones sobre la privacidad en su trabajo\n",
        "- Un padre/madre preocupado por los datos de sus hijos menores\n",
        "- Un usuario de aplicaciones m√≥viles o servicios online\n",
        "- Un propietario de peque√±o negocio que necesita cumplir con la normativa\n",
        "- Situaciones en comunidades de vecinos o espacios compartidos\n",
        "- Interacciones con administraciones p√∫blicas\n",
        "- Espionaje entre compa√±eros de trabajo\n",
        "- Empresas comerciando con datos privador\n",
        "\n",
        "‚ö†Ô∏è IMPORTANTE: Aseg√∫rate de que la pregunta sea ORIGINAL y no se parezca demasiado a los ejemplos proporcionados. Busca √°ngulos o situaciones novedosas dentro del tema elegido.\n",
        "\n",
        "\n",
        "‚ö†Ô∏è FORMATO EXACTO A SEGUIR:\n",
        "\n",
        "\n",
        "Pregunta sobre los temas nombrados anteriormente\n",
        "RESPUESTAMODELO\n",
        "Respuesta clara, razonada, y convincente\n",
        "\n",
        "üìù Ejemplos (para referencia):\n",
        "\n",
        "\n",
        "\"Un comercio online me instal√≥ cookies de seguimiento sin avisarme. ¬øEsto es legal?\"\n",
        "RESPUESTAMODELO\n",
        "\"No, en Espa√±a un sitio web solo puede instalar cookies de seguimiento si obtiene tu consentimiento previo. Deben informarte de manera clara sobre su uso y permitirte aceptarlas o rechazarlas f√°cilmente. Solo las cookies estrictamente necesarias pueden activarse sin tu permiso.\"\n",
        "\n",
        "\"Una empresa de cr√©dito ha rechazado mi solicitud bas√°ndose en un algoritmo. ¬øPuedo exigir una explicaci√≥n?\"\n",
        "RESPUESTAMODELO\n",
        "\"S√≠, cuando una decisi√≥n automatizada afecta significativamente tus derechos, puedes solicitar una explicaci√≥n clara sobre los criterios utilizados. Tambi√©n puedes pedir una revisi√≥n manual si crees que la decisi√≥n ha sido injusta o err√≥nea.\"\n",
        "\n",
        "\"Trabajo desde casa y mi empresa usa software de monitoreo en mi ordenador. ¬øHasta qu√© punto es legal?\"\n",
        "RESPUESTAMODELO\n",
        "\"El monitoreo es legal si est√° justificado y comunicado de manera transparente. Tu empresa debe informarte sobre qu√© datos recopila, con qu√© finalidad y durante cu√°nto tiempo. Adem√°s, el control debe ser proporcional y no invadir tu privacidad m√°s all√° de lo necesario para evaluar tu rendimiento laboral.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHSZV3dZ9X0Y"
      },
      "source": [
        "Para detectar preguntas duplicadas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "NId9n1E59ZmX",
        "outputId": "9d443bf7-1cb9-4a6c-cc5e-7d593baf79f3"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_4a617a46-ea2b-4463-946d-5f3e6936ff7f\", \"dataframe_limpio.csv\", 309598)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv('/content/FinalData - Hoja 1 (4).csv')\n",
        "\n",
        "# Calcular similitud entre preguntas\n",
        "vectorizer = TfidfVectorizer(stop_words='english')  # Puedes ajustar a espa√±ol si necesitas\n",
        "vectors = vectorizer.fit_transform(df['instruction'])\n",
        "similarity = cosine_similarity(vectors)\n",
        "\n",
        "# Umbral de similaridad (ajustar seg√∫n necesidad)\n",
        "threshold = 0.8\n",
        "\n",
        "# Identificar grupos de preguntas similares\n",
        "grupos_similares = []\n",
        "ya_procesadas = set()\n",
        "\n",
        "for i in range(len(df)):\n",
        "    if i in ya_procesadas:\n",
        "        continue\n",
        "\n",
        "    similar_indices = []\n",
        "    for j in range(len(df)):\n",
        "        if i != j and similarity[i, j] > threshold:\n",
        "            similar_indices.append(j)\n",
        "\n",
        "    if similar_indices:\n",
        "        grupo = [i] + similar_indices\n",
        "        grupos_similares.append(grupo)\n",
        "        ya_procesadas.update(similar_indices)\n",
        "\n",
        "# Mostrar los grupos de preguntas similares\n",
        "print(f\"Se encontraron {len(grupos_similares)} grupos de preguntas similares:\\n\")\n",
        "\n",
        "for idx, grupo in enumerate(grupos_similares):\n",
        "    print(f\"Grupo {idx+1} (Similaridad > {threshold}):\")\n",
        "    for i, ind in enumerate(grupo):\n",
        "        print(f\"  {i+1}. √çndice {ind}: {df['instruction'].iloc[ind][:100]}...\")\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "# Opcional: Exportar los resultados a un CSV para revisar m√°s f√°cilmente\n",
        "output_rows = []\n",
        "\n",
        "for group_idx, grupo in enumerate(grupos_similares):\n",
        "    for idx in grupo:\n",
        "        output_rows.append({\n",
        "            'grupo': group_idx + 1,\n",
        "            'indice_original': idx,\n",
        "            'pregunta': df['instruction'].iloc[idx],\n",
        "            'respuesta': df['output'].iloc[idx],\n",
        "            'modelo': df['model'].iloc[idx] if 'model' in df.columns else 'Unknown'\n",
        "        })\n",
        "\n",
        "grupos_df = pd.DataFrame(output_rows)\n",
        "output_file = 'grupos_similares.csv'\n",
        "grupos_df.to_csv(output_file, index=False)\n",
        "print(f\"Resultados exportados a {output_file}\")\n",
        "\n",
        "# Descargar el archivo\n",
        "files.download(output_file)\n",
        "\n",
        "# Contar cu√°ntas preguntas est√°n en los grupos (posibles duplicados)\n",
        "num_duplicados = sum(len(grupo) for grupo in grupos_similares) - len(grupos_similares)\n",
        "print(f\"\\nN√∫mero total de posibles preguntas duplicadas: {num_duplicados}\")\n",
        "print(f\"N√∫mero de filas en el dataset original: {len(df)}\")\n",
        "print(f\"N√∫mero estimado de filas despu√©s de eliminar duplicados: {len(df) - num_duplicados}\")\n",
        "\n",
        "grupos_similares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ7qPeJLTl28"
      },
      "source": [
        "# Finetunning Llama 3\n",
        "---Resumen\n",
        "Aqui poner que es unsloth que usamos y que modelos se pueden usar para hacer finetunning\n",
        "\n",
        "La biblioteca Unsloth permite completar el proceso de entrenamiento y entrenamiento fino 2x m√°s r√°pido y requiere mucha menos VRAM gracias a derivaciones matem√°ticas complejas y kernels de GPUs optimizados manualmente\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSX6xhJ5TbIZ"
      },
      "outputs": [],
      "source": [
        "#%%Capture para evitar que genere salida en collab el comando de install\n",
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install unsloth\n",
        "# Get latest Unsloth\n",
        "!pip install --upgrade --no-deps \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS0fAdKLiWlX"
      },
      "source": [
        "Debido a que tenemos recursos limitados vamos a usar modelos cuantizados en 4 bits para reducir el consumo de memoria y el espacio en disco que utilizan.\n",
        "\n",
        "¬øQue significa cuantizar modelos?\n",
        "\n",
        "Normalmente los modelos almacenan sus par√°metros en Punto Flotante de 16 Bits, al cuantizar los modelos de 16 Bits a 4 Bits conseguimos:\n",
        "\n",
        "‚úÖ Menos uso de VRAM/RAM ‚Üí Nos permite utilizar modelos m√°s grandes que debido a limitaciones de google collab no podr√≠amos ejecutar en este entorno.\n",
        "\n",
        "‚úÖ Inferencia m√°s r√°pida ‚Üí Al reducir los datos aumenta la velocidad de c√°lculo.\n",
        "\n",
        "‚úÖ Descarga m√°s r√°pida ‚Üí Al reducir el tama√±o en disco que ocupan conseguimos ahorrarnos tiempo en descargar/subir los distintos modelos adem√°s de ahorrar espacio en google collab, el cual esta muy limitado.\n",
        "\n",
        "¬øQue desventajas tiene?\n",
        "\n",
        "\n",
        "üî¥ P√©rdida de precisi√≥n ‚Üí Reducir los bits disminuye la calidad de las respuestas, afectando tareas complejas.\n",
        "\n",
        "üî¥ Problemas en c√°lculos precisos ‚Üí Modelos cuantizados pueden fallar en matem√°ticas avanzadas o generaci√≥n de c√≥digo detallado.\n",
        "\n",
        "üü†  Fine-tuning m√°s dif√≠cil ‚Üí La cuantizaci√≥n a 4 bits reduce la precisi√≥n de los pesos. Unsloth utiliza t√©cnicas optimizadas para reducir el efecto negativo que esto produce en el finetunning, pero sigue habiendo ligeras restricciones comparado con modelos en FP16 o FP32."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333,
          "referenced_widgets": [
            "27ae2827671c4143b0b2b2075fc1f972",
            "592e543174ac4c149b978a9b076e6947",
            "0a248e4eeb884e16966688be3c5f815b",
            "9c0b992d847a4edc8bae02bdb0a47a52",
            "c9b8ab31024948dbab3b9df93a5cc238",
            "62abec66de98471dbac0a2993bea23ad",
            "c1d1a9e630ae45a896a48c40d2ef9849",
            "2b0226de01884c16bd3824355fd2daa2",
            "6c0fad39a89342be9404a09ec7c4fcb9",
            "db50691252d44ad2ad8059a948b2a05f",
            "6a3b9333aeae4a00a21d6b857c5c54c1",
            "808f22940b3e4d56a044858af7717a82",
            "375e18cf4fae4b38935c18b1b026d194",
            "f7039d5c73024c2bb49232de870c0753",
            "b013a83d84e340ca9bdb032e04a4fe82",
            "629ec748ee184a8eb69a3cb65ec65e0f",
            "95dd39f5325241719f5838f21828ca20",
            "ff2bab5e8aad485a86195e2f69834c5e",
            "a86a7074d2aa4c0b9cc07304df896a09",
            "80ca55c4cb834e07a222a05eed953108",
            "f50acf3c440e46da9e2445e7a010f81d",
            "20804d2b3c7e4ef1ba3d67916d9768d1",
            "7661221145d04590a76c181231d8eb56",
            "df917f2ba37a41039d8305bce24a3c6d",
            "1b9f60aac66949cba9a3f20759d1f5c4",
            "c672b2b5cb574c33955eb644fbe35b98",
            "c47b1a19346a4a568abf1a3281233887",
            "1fbc81be877b4712933eb6e5c0a28cd4",
            "00a9009b6cb34da0b8d11233bafe42f9",
            "a816656658de4212968acb301df96937",
            "989344ac67d24b4098b718695ece3f9b",
            "4e3861b0138546da835a21cb88267d2e",
            "41c05166642345b0a00f7d186218be49",
            "75e83248694a401283c01c1f1052519d",
            "ebca870c675e4ade83fd5712b97a96f8",
            "d223583186b748afa0af48cf021b1ac6",
            "f802cc878c98461598a166e9b59fcd3f",
            "db15ee6f267a4620978d4ef3b822d711",
            "b264b66edb224b469dde034cec7af3a9",
            "a581b54fb35b483dbe8db53c11ab2e07",
            "13f56339d8ff4e548f3389b99b6cd09f",
            "256205400ed448e0a51efdd73e593eb8",
            "2fd1ce058941425da07b9eb0b22fd09f",
            "e80f507e19904e4fa56d9da77d4f535e",
            "6895588ff3ae4dfdb44f545b9f2c9a0a",
            "77256958d12141eb809dd0c585983dee",
            "0176b9971228499fa742bfce6f754663",
            "fdf7e9de3889464e940f1637fc8ec65e",
            "9e6500d06fad492783db0849ceb7f26b",
            "61950496fcb54235a6a0e1fff0991037",
            "b587a76b966544dba862b0eff56543b4",
            "3dcd01f64e374bc6ae6c1d587b81bac5",
            "939fd41a1efd49f6a0e5e880c8a6aa7e",
            "5e5c95ab789c470090fbee9c41bc596a",
            "503b8be124f24ea49d8061863ad68f0b"
          ]
        },
        "id": "g4aEbvOzUK-d",
        "outputId": "717721f4-7035-4211-ec56-06d5a6c1bb97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.51.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27ae2827671c4143b0b2b2075fc1f972",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "808f22940b3e4d56a044858af7717a82",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/220 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7661221145d04590a76c181231d8eb56",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/51.1k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75e83248694a401283c01c1f1052519d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6895588ff3ae4dfdb44f545b9f2c9a0a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/345 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Se puede poner la longitud que se quiera, Unsloth utiliza RoPE Scaling\n",
        "dtype = None # None para detecci√≥n autom√°tica de la GPU. Float16 para Tesla T4, V100, Bfloat16 para Ampere+\n",
        "load_in_4bit = True # Usamos 4bit para reducir el uso de memoria. Can be False.\n",
        "\n",
        "\n",
        "#Modelos 4bit cuantizados por unsloth\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
        "    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGPMaCTDqUyj"
      },
      "source": [
        "Ahora vamos a utilizar LoRA (Low-Rank Adapter), esto nos permite re-entrenar un modelo sin modificar toda su estructura. En lugar de ajustar todos los par√°metros del modelo base, LoRA introduce dos nuevas matrices que se encargan de aprender y almacenar las actualizaciones espec√≠ficas necesarias durante el fine-tuning.\n",
        "\n",
        "Una matriz se especializa en capturar las modificaciones necesarias durante el entrenamiento y la otra ayuda a combinarlas con los par√°metros originales del modelo.\n",
        "\n",
        "Gracias a que solo se actualizan estas nuevas matrices en lugar de todos los par√°metros del modelo original, podemos realizar fine-tunning de modelos m√°s grandes con un hardware limitado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNWXr-KltxGq",
        "outputId": "0ffcb475-e060-483b-d066-5a541d0742d4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.3.19 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEZyUecvuras"
      },
      "source": [
        "## Preparaci√≥n del dataset\n",
        "\n",
        "Para que Ollama y llama.cpp funcionen como un chatbot personalizado como por ejemplo ChatGPT, solo debemos tener 2 columnas: una de instrucciones y una de salida, por lo que nuestro dataset solo consta de esta dos columnas.\n",
        "\n",
        "Vamos a utilizar la librer√≠a de load_dataset del paquete datasets de hugging face, la cual nos facilita el uso de datasets para fine-tunning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "288b72f069454a308bbcdd817d8e0af2",
            "774c667bb01445a79cf8381a7dc73314",
            "4f2a96b0b9654e56a30bfe89a5386a23",
            "92c00c5cb9c24de1af7e40aa0ed15b47",
            "fc8181c289ff44c5a83a88684203906e",
            "08e35bdac488451b82a2f9b785aa6138",
            "f93543b90d1f4db491d07f743561389f",
            "e1fd1ac2fbb6434a97aedd66419f3a1f",
            "38afaf81d45f45fca24a96b6ff25a4b4",
            "5a5e5d22b7ae4273bbb8664a877397e2",
            "131b5d0fad894cdf9ac88fff9c123efe"
          ]
        },
        "id": "QmWNeFp-vHGZ",
        "outputId": "91e3d0fb-3389-4407-a20c-2946802e9e94"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "288b72f069454a308bbcdd817d8e0af2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['instruction', 'output', 'model']\n",
            "{'instruction': 'Si accedo a mi correo personal desde el ordenador del trabajo, ¬øpuede mi empleador leer mis mensajes?', 'output': 'No, tu empleador no puede acceder a tus correos personales sin tu consentimiento, incluso si usas un dispositivo corporativo. Solo puede supervisar las comunicaciones laborales y debe informarte sobre cualquier pol√≠tica de monitoreo. Sin embargo, es recomendable no usar equipos de empresa para asuntos personales.', 'model': 'deepseek/deepseek-chat-v3-0324:free'}\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\n",
        "    \"csv\",\n",
        "    data_files = \"/content/input.csv\",\n",
        "    split = \"train\",\n",
        ")\n",
        "print(dataset.column_names)\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-tD_iyYxTR4"
      },
      "source": [
        "Una vez ya tenemos cargado el dataset le vamos a dar un formato adecuado para fine-tunning.\n",
        "\n",
        "Vamos a utilizar la libreria to_sharegpt de Unsloth, para generar conversaciones largas combinando los inputs y outputs del dataset.\n",
        "\n",
        "Gracias a esto en vez de hacer el finetunning con preguntas sueltas, realizamos el entrenamiento con conversaciones simuladas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "dacc18e6fa914351b71468698aea27ff",
            "fd33365745b64ffe80031385cb1ae772",
            "a4b5c9120f6d484285cad69f75a4c4e7",
            "cd4a034727aa4a5eabe5d0160d2ca331",
            "22d6e496881c4d6d89d060a4c785c22c",
            "39ae9634aef84b949cef2f9107773320",
            "066a8a3400384015a2ab91be7925e856",
            "70c1b4c89dc04af4b88cec3d9fd76143",
            "4d5d028e5e304226b654fca8859514a4",
            "d7c0d3b5f98d4943aae99cb39a1a8974",
            "0ae11327bf7f40fa9a452c313fb4be49",
            "8879a8c8c73442f1913797a796271b23",
            "67f6191f7d934ba995d4deda864aa147",
            "f84e15027cc044b18041b2c14a6ca5b9",
            "c31c0805bee54f6e9d2ee25efb5455b3",
            "33f28884e71c4a3bba92ef0615ee3a48",
            "5dcf287617bb4db5ac7b65bcf81d1887",
            "c9bd4c42e09a459abd7574111b60812b",
            "f5943977ac2e47f79c80772f7aed2bd8",
            "53bd285ba55e428bbdcad85c808dca54",
            "d19d330b75eb4951ba2f4a65825d7eba",
            "08974acddcf14d3684295695bbe6fbac",
            "eaa5c7ea39344301bab459bc04d5716b",
            "2337590ca184415e9c27920a91f2bddb",
            "1a9d2ffe04bf4ba2a2f9c372307ab239",
            "c315c9e0934e4965ade0eddda9035103",
            "cdc832f653e340999903760c0fe93d51",
            "73d06604b4f540b58e9cf87c7f56152b",
            "3e53b04b3f754fc282692b31ea10baae",
            "9d4c6e36a1b8485d9f8371bd4e88e859",
            "7926b22c81d24b3faa488d7e8cd45e25",
            "7fdf5fdff9f64e749803ad4c4f4a5db7",
            "ac984ea7b70c4cda959655d375c978b3",
            "0067bf50ac57403d95bed599f67e05a2",
            "0aed6b8fffc9441dbe28059150085e39",
            "5650051de25b49b3a3cd2ec15dd6ef35",
            "e0b9c95bd1cb4c34ad24e7983d5fb019",
            "b2d07be45d1f4b1ea9845b302c32ef9e",
            "0104974262494a418b716c5fcbd36207",
            "dc68039b2b7840dabb1d39d90e90afd1",
            "6b61c3734c88480e84c4ca8d2ede49bf",
            "f6971aba51584d6e879ae4a026f71110",
            "f6a99c6a2a1d4de7b8ae59675718052f",
            "0dbbc41cb5da49c09557d77318e1d89a",
            "731686f94c174dc0b65556bca9eca0b1",
            "c01604f930e94bc79dc10369f36819b0",
            "931336892af0406cb159b2ebef825df3",
            "fa438c641a1e41c08b2ebbb4e87a2018",
            "8106f79da6974840a0c228354066bdac",
            "bf4f47b2da0344fe8f03eb26d982b96a",
            "9a665075297c4dd5b36cb32d5a4e76f8",
            "0d29128b3b8d42178f3ae6d6b4772d2f",
            "5095763539d846d39b9348e51bd7a627",
            "d34e92f473bf4d60924f5d9018132646",
            "fce4bd29180641d29cc7e6310706d109",
            "946f98b1fd3544509b709249cb96374b",
            "1843dac8987a4441819a184141373de2",
            "5c1ac9846bfb48d19298a110c29bf2e7",
            "278d6110c5d946748d5dd90256c326c9",
            "03ffb1d6834b444383139a123f373dd1",
            "5bd00d3d8b214109973049badc3cb4be",
            "39bc31df5c4947ae90a0bf862e739ae8",
            "4a8bbd1eb7a54f94a0933ea719e27a2e",
            "b5a109286ea842b0b2ba9534713c1ffe",
            "174183bcb63a4efa8f1420bfc4d0b5c8",
            "66106b47a3ba43ceb3b943dee3dbe3a0",
            "332acfff1e5e459c8d229d3a0d2169d1",
            "559cf60479a44452900421c985949a3a",
            "bdf00aa9b8da43b0b229c5a4566ff4bd",
            "fd633cad3d994284a8084f93c2668669",
            "6a329639b41149069cd87e5df3b7b5a9",
            "711c3e4a2fc94774931c340fcdd94039",
            "3cbca6985d8340a9b0e073a24622004f",
            "c7a644cb693544c9b42f2764cbd4abf8",
            "6e4fb3251724456fae6b6f13a6c12097",
            "05af9e6bb60047a3ad7e3142a177ca14",
            "1eabbd3c84064a2a86020c1c28b0426c",
            "a5fc373283e74ad38980e2a1c588619e",
            "a3b7c42a2be84209a595cc209ba12755",
            "69de33eb6326457298070f7fe84cfff1",
            "a8fb0593650f4007be70d0fbb0fdfced",
            "bdb97cdc500d4ef6be5175bdfd970914",
            "b5250b86c3a544929037a12dac2d898d",
            "887830af532541a88b552b499e751cf2",
            "743d80e6489542f7b9493fef1b9998a3",
            "2b400f884db74feb8be26093bdd792a2",
            "ab828a2583c44d1183da61e4663e1bba",
            "bb1368132ca94034bbae49c2150ad2e1"
          ]
        },
        "id": "U8H4EsAxxS1U",
        "outputId": "6986b6f9-f839-466e-d1c9-0ec7296cb4b5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dacc18e6fa914351b71468698aea27ff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Merging columns:   0%|          | 0/796 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8879a8c8c73442f1913797a796271b23",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Converting to ShareGPT:   0%|          | 0/796 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eaa5c7ea39344301bab459bc04d5716b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Flattening the indices:   0%|          | 0/796 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0067bf50ac57403d95bed599f67e05a2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Flattening the indices:   0%|          | 0/796 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "731686f94c174dc0b65556bca9eca0b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Flattening the indices:   0%|          | 0/796 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "946f98b1fd3544509b709249cb96374b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Flattening the indices:   0%|          | 0/796 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "332acfff1e5e459c8d229d3a0d2169d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Flattening the indices:   0%|          | 0/796 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5fc373283e74ad38980e2a1c588619e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extending conversations:   0%|          | 0/796 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'conversations': [{'from': 'human',\n",
            "                    'value': \"La pregunta es: ('Si accedo a mi correo personal \"\n",
            "                             'desde el ordenador del trabajo, ¬øpuede mi '\n",
            "                             \"empleador leer mis mensajes?',).\"},\n",
            "                   {'from': 'gpt',\n",
            "                    'value': 'No, tu empleador no puede acceder a tus correos '\n",
            "                             'personales sin tu consentimiento, incluso si '\n",
            "                             'usas un dispositivo corporativo. Solo puede '\n",
            "                             'supervisar las comunicaciones laborales y debe '\n",
            "                             'informarte sobre cualquier pol√≠tica de '\n",
            "                             'monitoreo. Sin embargo, es recomendable no usar '\n",
            "                             'equipos de empresa para asuntos personales.'},\n",
            "                   {'from': 'human',\n",
            "                    'value': \"La pregunta es: ('¬øC√≥mo puedo saber qu√© datos \"\n",
            "                             'personales recopila una aplicaci√≥n m√≥vil que he '\n",
            "                             \"instalado en mi m√≥vil en Espa√±a?',).\"},\n",
            "                   {'from': 'gpt',\n",
            "                    'value': 'Las aplicaciones m√≥viles que operan en Espa√±a '\n",
            "                             'deben informarte de manera clara y accesible '\n",
            "                             'sobre los datos personales que recopilan. Puedes '\n",
            "                             'encontrar esta informaci√≥n en la pol√≠tica de '\n",
            "                             'privacidad de la app, la cual suele estar '\n",
            "                             'disponible tanto en la tienda de aplicaciones '\n",
            "                             'antes de descargarla como dentro de la propia '\n",
            "                             'app (normalmente en secciones como \"Ajustes\", '\n",
            "                             '\"Informaci√≥n legal\" o \"Privacidad\"). Esta '\n",
            "                             'pol√≠tica debe detallar qu√© datos se recogen, con '\n",
            "                             'qu√© finalidad se usan y si se comparten con '\n",
            "                             'terceros.'},\n",
            "                   {'from': 'human',\n",
            "                    'value': 'La pregunta es: (\\'\"Una red social donde tengo '\n",
            "                             'cuenta ha sufrido una filtraci√≥n que ha expuesto '\n",
            "                             'mis mensajes privados. ¬øQu√© derechos tengo como '\n",
            "                             'afectado y qu√© obligaciones tiene la '\n",
            "                             'plataforma?\"\\',).'},\n",
            "                   {'from': 'gpt',\n",
            "                    'value': '\"Como afectado, tienes derecho a ser informado '\n",
            "                             'de forma clara sobre el alcance de la filtraci√≥n '\n",
            "                             'y los riesgos potenciales. La plataforma debe '\n",
            "                             'notificar el incidente a la AEPD en 72 horas si '\n",
            "                             'existe riesgo para los usuarios, tomar medidas '\n",
            "                             't√©cnicas para mitigar el da√±o y ofrecerte '\n",
            "                             'orientaci√≥n sobre c√≥mo protegerte. Puedes '\n",
            "                             'ejercer tus derechos de acceso, supresi√≥n o '\n",
            "                             'reclamaci√≥n si consideras que no han actuado con '\n",
            "                             'la diligencia debida.\"'},\n",
            "                   {'from': 'human',\n",
            "                    'value': \"La pregunta es: ('¬øPuede una aplicaci√≥n m√≥vil \"\n",
            "                             'recopilar mi ubicaci√≥n en segundo plano sin mi '\n",
            "                             \"permiso expl√≠cito en Espa√±a?',).\"},\n",
            "                   {'from': 'gpt',\n",
            "                    'value': 'No, en Espa√±a una aplicaci√≥n m√≥vil necesita tu '\n",
            "                             'consentimiento expl√≠cito e informado para '\n",
            "                             'acceder a tu ubicaci√≥n en segundo plano, es '\n",
            "                             'decir, cuando no la est√°s usando activamente. '\n",
            "                             'Deben explicarte claramente para qu√© necesitan '\n",
            "                             'esa informaci√≥n y s√≥lo pueden recogerla si es '\n",
            "                             'estrictamente necesario para la finalidad que te '\n",
            "                             'han comunicado. Siempre debes tener la opci√≥n de '\n",
            "                             'retirar ese permiso o desactivar la '\n",
            "                             'geolocalizaci√≥n f√°cilmente desde los ajustes de '\n",
            "                             'la app o del propio dispositivo.'},\n",
            "                   {'from': 'human',\n",
            "                    'value': \"La pregunta es: ('Si una empresa en Espa√±a usa \"\n",
            "                             'IA para procesar mis datos, ¬øest√°n obligados a '\n",
            "                             \"dec√≠rmelo y explicarme para qu√©?',).\"},\n",
            "                   {'from': 'gpt',\n",
            "                    'value': 'S√≠, las organizaciones que usan inteligencia '\n",
            "                             'artificial para tratar tus datos personales en '\n",
            "                             'Espa√±a deben informarte de forma transparente '\n",
            "                             'sobre ello. Esta informaci√≥n debe incluir '\n",
            "                             'claramente la finalidad para la que se usan tus '\n",
            "                             'datos y si la IA toma decisiones automatizadas '\n",
            "                             'que te afecten significativamente. Si esas '\n",
            "                             'decisiones tienen efectos importantes (legales o '\n",
            "                             'similares), tienes adem√°s derecho a obtener '\n",
            "                             'informaci√≥n significativa sobre la l√≥gica '\n",
            "                             'utilizada, as√≠ como a impugnar la decisi√≥n y '\n",
            "                             'solicitar intervenci√≥n humana.'}]}\n"
          ]
        }
      ],
      "source": [
        "from unsloth import to_sharegpt\n",
        "dataset = to_sharegpt(\n",
        "    dataset,\n",
        "    merged_prompt = \\\n",
        "        \"[[La pregunta es: {instruction}.]]\"\n",
        "        ,\n",
        "    conversation_extension = 5, # Este par√°metro agrupa aleatoriamente hasta x preguntas y respuestas en una sola conversaci√≥n. Esto es √∫til para simular conversaciones m√°s largas.\n",
        "    output_column_name = \"output\",\n",
        ")\n",
        "#Para imprimir como se ve ahora el dataset\n",
        "from pprint import pprint\n",
        "pprint(dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXKKSvVgzYB-"
      },
      "source": [
        "El dataset ahora utiliza etiquetas como \"human\" y \"gpt\":\n",
        "\n",
        "```\n",
        "{'conversations': [{'from': 'human',\n",
        "                    'value': \"La pregunta es: ('Antonio instala c√°maras en su \"\n",
        "                             'tienda sin avisar a empleados y clientes. ¬øQu√© '\n",
        "                             \"consecuencias puede tener?',).\"},\n",
        "                   {'from': 'gpt',\n",
        "                    'value': 'Seg√∫n el Art√≠culo 22 de la Ley Org√°nica 3/2018 '\n",
        "                             'de Protecci√≥n de Datos Personales y Garant√≠a de '\n",
        "                             'los Derechos Digitales, debe informar a los '\n",
        "                             'afectados sobre la videovigilancia. Si no lo '\n",
        "                             'hace, puede enfrentar sanciones de la AEPD.'},\n",
        "```\n",
        "Sin embargo para un modelo de OpenAI o Hugging Face, se requieren etiquetas est√°ndar como **user** para el usuario y **assistant** para el modelo.\n",
        "\n",
        "Para arreglar esto vamos a utilizar la libreria standardize_sharegpt de Unsloth que cambia todas las etiquetas como human, gpt, system, etc... a **user** y **assistant**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "34b786a9cd8d42b7b1c69ca91d3c9359",
            "ccdb1a7be02b4f77a4635f1612be9360",
            "6daa63e3012b435a887107e392aa6651",
            "a3ee21e71ae44f1ebbe0f6e2e06bcbcb",
            "36c4e3436ae64ee9bbf784aa8c39c06b",
            "014e363aae62423787d393d949df2c9b",
            "157dcb1f0bf242139f3546cc93b7b936",
            "c893c659fe674f73a391219bcc9ce9f2",
            "fe2cb4a4a87e425bb741dc2e96cb3c19",
            "9585c39899db42858cecf93aff9d5c15",
            "982efc40a70245b7ad8468c8bede757b"
          ]
        },
        "id": "4HR7_HxXysv7",
        "outputId": "3277c44c-6aae-486a-cd1f-ce1d9028eb70"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "34b786a9cd8d42b7b1c69ca91d3c9359",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Unsloth: Standardizing formats (num_proc=2):   0%|          | 0/796 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'conversations': [{'content': \"La pregunta es: ('Si accedo a mi correo \"\n",
            "                               'personal desde el ordenador del trabajo, '\n",
            "                               \"¬øpuede mi empleador leer mis mensajes?',).\",\n",
            "                    'role': 'user'},\n",
            "                   {'content': 'No, tu empleador no puede acceder a tus '\n",
            "                               'correos personales sin tu consentimiento, '\n",
            "                               'incluso si usas un dispositivo corporativo. '\n",
            "                               'Solo puede supervisar las comunicaciones '\n",
            "                               'laborales y debe informarte sobre cualquier '\n",
            "                               'pol√≠tica de monitoreo. Sin embargo, es '\n",
            "                               'recomendable no usar equipos de empresa para '\n",
            "                               'asuntos personales.',\n",
            "                    'role': 'assistant'},\n",
            "                   {'content': \"La pregunta es: ('¬øC√≥mo puedo saber qu√© datos \"\n",
            "                               'personales recopila una aplicaci√≥n m√≥vil que '\n",
            "                               \"he instalado en mi m√≥vil en Espa√±a?',).\",\n",
            "                    'role': 'user'},\n",
            "                   {'content': 'Las aplicaciones m√≥viles que operan en Espa√±a '\n",
            "                               'deben informarte de manera clara y accesible '\n",
            "                               'sobre los datos personales que recopilan. '\n",
            "                               'Puedes encontrar esta informaci√≥n en la '\n",
            "                               'pol√≠tica de privacidad de la app, la cual '\n",
            "                               'suele estar disponible tanto en la tienda de '\n",
            "                               'aplicaciones antes de descargarla como dentro '\n",
            "                               'de la propia app (normalmente en secciones '\n",
            "                               'como \"Ajustes\", \"Informaci√≥n legal\" o '\n",
            "                               '\"Privacidad\"). Esta pol√≠tica debe detallar qu√© '\n",
            "                               'datos se recogen, con qu√© finalidad se usan y '\n",
            "                               'si se comparten con terceros.',\n",
            "                    'role': 'assistant'},\n",
            "                   {'content': 'La pregunta es: (\\'\"Una red social donde tengo '\n",
            "                               'cuenta ha sufrido una filtraci√≥n que ha '\n",
            "                               'expuesto mis mensajes privados. ¬øQu√© derechos '\n",
            "                               'tengo como afectado y qu√© obligaciones tiene '\n",
            "                               'la plataforma?\"\\',).',\n",
            "                    'role': 'user'},\n",
            "                   {'content': '\"Como afectado, tienes derecho a ser informado '\n",
            "                               'de forma clara sobre el alcance de la '\n",
            "                               'filtraci√≥n y los riesgos potenciales. La '\n",
            "                               'plataforma debe notificar el incidente a la '\n",
            "                               'AEPD en 72 horas si existe riesgo para los '\n",
            "                               'usuarios, tomar medidas t√©cnicas para mitigar '\n",
            "                               'el da√±o y ofrecerte orientaci√≥n sobre c√≥mo '\n",
            "                               'protegerte. Puedes ejercer tus derechos de '\n",
            "                               'acceso, supresi√≥n o reclamaci√≥n si consideras '\n",
            "                               'que no han actuado con la diligencia debida.\"',\n",
            "                    'role': 'assistant'},\n",
            "                   {'content': \"La pregunta es: ('¬øPuede una aplicaci√≥n m√≥vil \"\n",
            "                               'recopilar mi ubicaci√≥n en segundo plano sin mi '\n",
            "                               \"permiso expl√≠cito en Espa√±a?',).\",\n",
            "                    'role': 'user'},\n",
            "                   {'content': 'No, en Espa√±a una aplicaci√≥n m√≥vil necesita tu '\n",
            "                               'consentimiento expl√≠cito e informado para '\n",
            "                               'acceder a tu ubicaci√≥n en segundo plano, es '\n",
            "                               'decir, cuando no la est√°s usando activamente. '\n",
            "                               'Deben explicarte claramente para qu√© necesitan '\n",
            "                               'esa informaci√≥n y s√≥lo pueden recogerla si es '\n",
            "                               'estrictamente necesario para la finalidad que '\n",
            "                               'te han comunicado. Siempre debes tener la '\n",
            "                               'opci√≥n de retirar ese permiso o desactivar la '\n",
            "                               'geolocalizaci√≥n f√°cilmente desde los ajustes '\n",
            "                               'de la app o del propio dispositivo.',\n",
            "                    'role': 'assistant'},\n",
            "                   {'content': \"La pregunta es: ('Si una empresa en Espa√±a usa \"\n",
            "                               'IA para procesar mis datos, ¬øest√°n obligados a '\n",
            "                               \"dec√≠rmelo y explicarme para qu√©?',).\",\n",
            "                    'role': 'user'},\n",
            "                   {'content': 'S√≠, las organizaciones que usan inteligencia '\n",
            "                               'artificial para tratar tus datos personales en '\n",
            "                               'Espa√±a deben informarte de forma transparente '\n",
            "                               'sobre ello. Esta informaci√≥n debe incluir '\n",
            "                               'claramente la finalidad para la que se usan '\n",
            "                               'tus datos y si la IA toma decisiones '\n",
            "                               'automatizadas que te afecten '\n",
            "                               'significativamente. Si esas decisiones tienen '\n",
            "                               'efectos importantes (legales o similares), '\n",
            "                               'tienes adem√°s derecho a obtener informaci√≥n '\n",
            "                               'significativa sobre la l√≥gica utilizada, as√≠ '\n",
            "                               'como a impugnar la decisi√≥n y solicitar '\n",
            "                               'intervenci√≥n humana.',\n",
            "                    'role': 'assistant'}]}\n"
          ]
        }
      ],
      "source": [
        "from unsloth import standardize_sharegpt\n",
        "dataset = standardize_sharegpt(dataset)\n",
        "\n",
        "#Para imprimir como se ve ahora el dataset\n",
        "from pprint import pprint\n",
        "pprint(dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo0Yv5Mt00NF"
      },
      "source": [
        "## Plantilla de conversaci√≥n con el modelo\n",
        "\n",
        "Una plantilla de chat es √∫til para el fine-tuning porque proporciona una estructura coherente que ense√±a al modelo c√≥mo interactuar de una mejor manera en las conversaciones, y por lo tanto mejorar la calidad de sus respuestas.\n",
        "\n",
        "Este es el formato de  un Prompt de Llama-3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e0058619ab004c3891f1f211e7138bf2",
            "8521f1b081c8426ab052535bccb87a51",
            "777674f274264d22ade6581c5b9de8e6",
            "cecb244f1d094f188023e10ea1863471",
            "fda1be9042914fec8a145e0cac60ca01",
            "3d274df88eac4e16b4c220b1a6372a57",
            "e5caecafe5ba42fbbdeabcc688980df9",
            "d7e598dfeef2432eb628d35ff2a1628e",
            "9f97bbebf0e140fab4945e412b6ed2fa",
            "3e4bc9f043854488b878656483ad6688",
            "0561192c90dd4ea6a120a2b9940e5ab8"
          ]
        },
        "id": "J2SRFD38096V",
        "outputId": "689bd2bd-301d-4a5f-c54b-d55a6d55338a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0058619ab004c3891f1f211e7138bf2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/796 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#chat_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "#{SYSTEM}<|end_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "#{INPUT}<|end_of_text|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "#{OUTPUT}<|end_of_text|>\"\"\"\n",
        "\n",
        "chat_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "{SYSTEM}<|eot_id|>\n",
        "<|start_header_id|>user<|end_header_id|>\n",
        "{INPUT}<|eot_id|>\n",
        "<|start_header_id|>assistant<|end_header_id|>\n",
        "{OUTPUT}<|eot_id|>\"\"\"\n",
        "\n",
        "from unsloth import apply_chat_template\n",
        "dataset = apply_chat_template(\n",
        "    dataset,\n",
        "    tokenizer = tokenizer,\n",
        "    chat_template = chat_template,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htc-yqdK6Ny_"
      },
      "source": [
        "## Entrenamiento del modelo\n",
        "\n",
        "Vamos a utilizar el Transformers Reinforcement Learning (TRL) de Hugginface **SFTTrainer** (Supervised Fine-Tuning) el cual permite entrenar modelos preexistentes con datos etiquetados para mejorar su desempe√±o en tareas espec√≠ficas.\n",
        "\n",
        "Tambien se puede utilizar DPOTrainer: Utiliza el aprendizaje por refuerzo directo (Direct Preference Optimization) para mejorar las respuestas generadas por el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "6ce15f1736b343948196bb520cae9a84",
            "a42fb19dbd8f4557bf3bc62f79c729bd",
            "2b8e98da62ad465793e858bbdc7db9ab",
            "8bb5db854b6743218980a1909031f9d1",
            "6a4f221663c444b3a82a71f850162467",
            "de7427201d2d46b0a507f4e43dec20ed",
            "15a9b66c449a43ee9e759b29457e1df3",
            "40217752dc6245a1b2c67ec4e79e379c",
            "d9476b1342c74f95b72318a5b809402b",
            "f604e6f0f77c4d63bf49f4728d411f73",
            "4bebb5119aab4cd196509bd84f5e6015"
          ]
        },
        "id": "6R0i0OqB597z",
        "outputId": "f2472b7f-f5dc-4c65-d448-c1bef834c3cd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ce15f1736b343948196bb520cae9a84",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/796 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = Tesla T4. Max memory = 14.741 GB.\n",
            "7.117 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = 2048,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 40,\n",
        "        #max_steps=None,\n",
        "        num_train_epochs=4,\n",
        "        learning_rate =  5e-5,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 10,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "#Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdUVpTcH7ThB"
      },
      "source": [
        "Es necesario tener una cuenta en https://wandb.ai/authorize y obtener la clave API para poder hacer el entrenamiento\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "F3SWM2RF7Vu7",
        "outputId": "a8b89a15-488a-4162-b940-753cce76f85e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 796 | Num Epochs = 4 | Total steps = 396\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 20,971,520/8,000,000,000 (0.26% trained)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mserdom02\u001b[0m (\u001b[33mserdom02-complutense-university-of-madrid\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250420_104748-ronsf90v</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/serdom02-complutense-university-of-madrid/huggingface/runs/ronsf90v' target=\"_blank\">outputs</a></strong> to <a href='https://wandb.ai/serdom02-complutense-university-of-madrid/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/serdom02-complutense-university-of-madrid/huggingface' target=\"_blank\">https://wandb.ai/serdom02-complutense-university-of-madrid/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/serdom02-complutense-university-of-madrid/huggingface/runs/ronsf90v' target=\"_blank\">https://wandb.ai/serdom02-complutense-university-of-madrid/huggingface/runs/ronsf90v</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='396' max='396' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [396/396 1:45:36, Epoch 3/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.698900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.139500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.624100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.315700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.149500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.028300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.977400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.931700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.898700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.865500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.781900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.750500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.720900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.674200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.627300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.592300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.568200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.546100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.510200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.472400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.401300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.360700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.355300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.334800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.314600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.301000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.273000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.257100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.247900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.233100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.192100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.178700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.172900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.162700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.160000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.154000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.154600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.145100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.143300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8iM7gau7ZYo"
      },
      "source": [
        "### Memoria Final y estadisticas de tiempo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvKJml8M7eBY",
        "outputId": "5b397e9d-6d70-4ded-8104-4f8f7d634950"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4530.4901 seconds used for training.\n",
            "75.51 minutes used for training.\n",
            "Peak reserved memory = 7.23 GB.\n",
            "Peak reserved memory for training = 1.812 GB.\n",
            "Peak reserved memory % of max memory = 49.047 %.\n",
            "Peak reserved memory for training % of max memory = 12.292 %.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRBXK1wG7u4U"
      },
      "source": [
        "\n",
        "### Inferencia\n",
        "Vamos a ejecutar el modelo, Unsloth hace la inferencia de manera nativa 2 veces m√°s r√°pida. Hay que usar promtp similares al finetunning para obtener buenos resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-i_hr1J73N0",
        "outputId": "0f1288b8-62d2-4938-938f-7dfaec7691d1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El colegio necesita el consentimiento expl√≠cito de los padres o tutores legales para publicar im√°genes de alumnos menores de edad en su web o redes sociales.<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "messages = [                    # Change below!\n",
        "    #{\"role\": \"user\", \"content\": '¬øCu√°l es el objeto de la Ley Org√°nica 3/2018 seg√∫n el Art√≠culo 1? \\n'},\n",
        "    {\"role\": \"user\", \"content\": '\"¬øPuede el colegio hacer fotos a los alumnos y publicarlas en la web del colegio?\"\\n'},\n",
        "    #{\"role\": \"user\", \"content\": '\"¬øCu√°l es el deber de confidencialidad seg√∫n el Art√≠culo 5 de la Ley Org√°nica 3/2018?\"\\n'},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGohnNGT8cLm"
      },
      "source": [
        "## Guardar y Cargar el modelo finetuneado\n",
        "\n",
        "Para guardar el modelo final como adaptadores LoRA, utiliza `push_to_hub` de Huggingface para guardarlo en l√≠nea o `save_pretrained` para guardarlo localmente.\n",
        "\n",
        "[NOTA] Esto SOLO guarda los adaptadores LoRA, no el modelo completo. ¬°Para guardarlo en 16bit o GGUF, baja m√°s abajo!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXD4le6y94uc",
        "outputId": "575d74e3-332e-46f5-8e9e-4e7de86999a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('lora_model/tokenizer_config.json',\n",
              " 'lora_model/special_tokens_map.json',\n",
              " 'lora_model/tokenizer.json')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(\"lora_model\") # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29AH6HEI98bC"
      },
      "source": [
        "Ahora, si deseas cargar los adaptadores LoRA que acabamos de guardar para inferencia, cambia False a True:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "QJma3s7D97yP",
        "outputId": "616afe56-d15e-4dca-d40e-b16b5f8049de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.51.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-552d0118ec86>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     model, tokenizer = FastLanguageModel.from_pretrained(\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"lora_model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# YOUR MODEL YOU USED FOR TRAINING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/loader.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, *args, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         model, tokenizer = dispatch_model.from_pretrained(\n\u001b[0m\u001b[1;32m    364\u001b[0m             \u001b[0mmodel_name\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0mmax_seq_length\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1779\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfast_inference\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1780\u001b[0;31m             model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m   1781\u001b[0m                 \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0mdevice_map\u001b[0m              \u001b[0;34m=\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    572\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4378\u001b[0m         \u001b[0;31m# Prepare the full device map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevice_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4380\u001b[0;31m             \u001b[0mdevice_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_in_fp32_regex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4382\u001b[0m         \u001b[0;31m# Finalize model weight initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_get_device_map\u001b[0;34m(model, device_map, max_memory, hf_quantizer, torch_dtype, keep_in_fp32_regex)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m             \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdevice_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"disk\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    105\u001b[0m                     \u001b[0;34m\"Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                     \u001b[0;34m\"quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
          ]
        }
      ],
      "source": [
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "pass\n",
        "\n",
        "messages = [                    # Change below!\n",
        "    {\"role\": \"user\", \"content\": '¬øPuede un trabajador de un supermercado pedirme el DNI?\\n'\\\n",
        "                                '\\n'\\\n",
        "                                ''},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XnT-Voc_6Hr"
      },
      "source": [
        "A continuaci√≥n, guardaremos el modelo en GGUF / llama.cpp.\n",
        "\n",
        "Clonamos llama.cpp y por defecto lo guardamos en q8_0. Permitimos todos los m√©todos como q4_k_m. Utiliza `save_pretrained_gguf` para guardarlo localmente y `push_to_hub_gguf` para subirlo a Hugging Face.\n",
        "\n",
        "Algunos m√©todos de cuantificaci√≥n compatibles (lista completa en nuestra p√°gina de Wiki):\n",
        "\n",
        "- q8_0: Conversi√≥n r√°pida. Uso de recursos alto, pero generalmente aceptable.\n",
        "- q4_k_m: Recomendado. Utiliza Q6_K para la mitad de los tensores `attention.wv` y `feed_forward.w2`, el resto usa Q4_K.\n",
        "- q5_k_m: Recomendado. Utiliza Q6_K para la mitad de los tensores `attention.wv` y `feed_forward.w2`, el resto usa Q5_K.\n",
        "\n",
        "¬°Tambi√©n soportamos guardar en m√∫ltiples opciones de GGUF en formato de lista! Esto puede acelerar el proceso en 10 minutos o m√°s si deseas varios formatos de exportaci√≥n.\n",
        "\n",
        "El siguiente codigo es para guardar el modelo en la carpeta /model, es necesario tener una api key en hugginface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "08f864ad78c8485bb45fa50dda7f188a",
            "6cbfef56108b4992b385d82ff36cdbc4",
            "1c7015ec55f545b8a1a26cbf8e32cc39",
            "a4e479e28ce94b0f9b63dd4c435e6e7a",
            "81acfacf75ef46d88c4b530896c1d6bb",
            "58e4b815ec4a4747a0b56e33a540c9bb",
            "51ff0c36700e4c92866ea7239e40e7f3",
            "741258f9827546319a1dc4d2f892210a",
            "e10e80363b754204887fffc4af7cd3d8",
            "08746819f1ba4895bcbc13fd0ecbd0ac",
            "fec29fe05b334fb488537ab92066ddb5",
            "968ab1d17382458daefc7e2724c6912e",
            "26d9440af08a4404abe52a76f096bb6e",
            "355096deee0842068a23019ab8d5ee1c",
            "c5f6ba1f51a1425b90b3946ca66cc685",
            "43ec4ef991ca471fb7b409eee3553ed0",
            "138f00d720ac475c9c752dd46e878db2",
            "3c65196ca805431ebb766846980c6d11",
            "0ae3f54842c04d6ea1b5fa1be3bfbc5e",
            "f7a60f172ea441df9fde0e5852305f57",
            "9392415e91b049ac9b173eaecb6e8c5f",
            "33893db8b278422ba6f508f5a9c78177"
          ]
        },
        "collapsed": true,
        "id": "0TIoxYH5__Au",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "1b5a38ee-6601-4ca0-8d51-bf382b357e86"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: ##### The current model auto adds a BOS token.\n",
            "Unsloth: ##### Your chat template has a BOS token. We shall remove it temporarily.\n",
            "Unsloth: You have 1 CPUs. Using `safe_serialization` is 10x slower.\n",
            "We shall switch to Pytorch saving, which might take 3 minutes and not 30 minutes.\n",
            "To force `safe_serialization`, set it to `None` instead.\n",
            "Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n",
            "model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\n",
            "Unsloth: Will remove a cached repo with size 5.7G\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 4.87 out of 12.67 RAM for saving.\n",
            "Unsloth: Saving model... This might take 5 minutes ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  9%|‚ñâ         | 3/32 [00:00<00:04,  6.69it/s]\n",
            "We will save to Disk and not RAM now.\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32/32 [03:10<00:00,  5.96s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Unsloth: Saving serdom02/Leyeneitor_8bitQ8_0/pytorch_model-00001-of-00004.bin...\n",
            "Unsloth: Saving serdom02/Leyeneitor_8bitQ8_0/pytorch_model-00002-of-00004.bin...\n",
            "Unsloth: Saving serdom02/Leyeneitor_8bitQ8_0/pytorch_model-00003-of-00004.bin...\n",
            "Unsloth: Saving serdom02/Leyeneitor_8bitQ8_0/pytorch_model-00004-of-00004.bin...\n",
            "Done.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Converting llama model. Can use fast conversion = False.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
            "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
            "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
            "\\        /    [2] Converting GGUF 16bits to ['q8_0'] might take 10 minutes each.\n",
            " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
            "\n",
            "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
            "Unsloth: CMAKE detected. Finalizing some steps for installation.\n",
            "Unsloth: [1] Converting model at serdom02/Leyeneitor_8bitQ8_0 into q8_0 GGUF format.\n",
            "The output location will be /content/serdom02/Leyeneitor_8bitQ8_0/unsloth.Q8_0.gguf\n",
            "This might take 3 minutes...\n",
            "INFO:hf-to-gguf:Loading model: Leyeneitor_8bitQ8_0\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'pytorch_model.bin.index.json'\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00001-of-00004.bin'\n",
            "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> Q8_0, shape = {4096, 128256}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00002-of-00004.bin'\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00003-of-00004.bin'\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00004-of-00004.bin'\n",
            "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:output.weight,               torch.float16 --> Q8_0, shape = {4096, 128256}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 8192\n",
            "INFO:hf-to-gguf:gguf: embedding length = 4096\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 14336\n",
            "INFO:hf-to-gguf:gguf: head count = 32\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
            "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
            "INFO:hf-to-gguf:gguf: file type = 7\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:gguf.vocab:Adding 280147 merge(s).\n",
            "INFO:gguf.vocab:Setting special token type bos to 128000\n",
            "INFO:gguf.vocab:Setting special token type eos to 128009\n",
            "INFO:gguf.vocab:Setting special token type pad to 128255\n",
            "INFO:gguf.vocab:Setting chat_template to {% if messages[0]['role'] == 'system' %}{{ '<|start_header_id|>system<|end_header_id|>\n",
            "' + messages[0]['content'] + '<|eot_id|>' }}{% set loop_messages = messages[1:] %}{% else %}{{ '<|start_header_id|>system<|end_header_id|>\n",
            "Below are some instructions that describe some tasks. Write responses that appropriately complete each request.<|eot_id|>' }}{% set loop_messages = messages %}{% endif %}{% for message in loop_messages %}{% if message['role'] == 'user' %}{{ '\n",
            "<|start_header_id|>user<|end_header_id|>\n",
            "' + message['content'] + '<|eot_id|>' }}{% elif message['role'] == 'assistant' %}{{ '\n",
            "<|start_header_id|>assistant<|end_header_id|>\n",
            "' + message['content'] + '<|eot_id|>' }}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '\n",
            "<|start_header_id|>assistant<|end_header_id|>\n",
            "' }}{% endif %}\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:/content/serdom02/Leyeneitor_8bitQ8_0/unsloth.Q8_0.gguf: n_tensors = 291, total_size = 8.5G\n",
            "Writing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8.53G/8.53G [03:59<00:00, 35.7Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to /content/serdom02/Leyeneitor_8bitQ8_0/unsloth.Q8_0.gguf\n",
            "Unsloth: Conversion completed! Output location: /content/serdom02/Leyeneitor_8bitQ8_0/unsloth.Q8_0.gguf\n",
            "Unsloth: Uploading GGUF to Huggingface Hub...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "08f864ad78c8485bb45fa50dda7f188a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "968ab1d17382458daefc7e2724c6912e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "unsloth.Q8_0.gguf:   0%|          | 0.00/8.54G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: ##### The current model auto adds a BOS token.\n",
            "Unsloth: ##### We removed it in GGUF's chat template for you.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved GGUF to https://huggingface.co/serdom02/Leyeneitor_8bitQ8_0\n"
          ]
        }
      ],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if True: model.push_to_hub_gguf(\"serdom02/Leyeneitor_8bitQ8_0\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"serdom02/model_16bitGGUF\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"serdom02/model_q4_k_mGGUF\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"serdom02/model\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTjohupyAgKP"
      },
      "source": [
        "## Si no subimos el modelo a Hugging Face tenemos que crear el modelo de Ollama\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cXYtIa2EMH3"
      },
      "source": [
        "Ollama necesita un archivo de modelo (Modelfile), que especifica el formato del prompt del modelo. Vamos a imprimir el generado autom√°ticamente por Unsloth:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "vmgNLwsbERBS",
        "outputId": "225ab7e3-56bc-409a-86ed-336bfc122672"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tokenizer' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-e1f679953d38>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ollama_modelfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ],
      "source": [
        "print(tokenizer._ollama_modelfile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5P8Ddl2E5wL"
      },
      "source": [
        "Ahora crearemos un modelo de Ollama llamado `unsloth_model` utilizando el archivo de modelo (Modelfile) que generamos autom√°ticamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBlAipXXE6C3"
      },
      "outputs": [],
      "source": [
        "!ollama create unsloth_model -f ./model/Modelfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGlR5y7pBkQ0"
      },
      "source": [
        "## Descargar el modelo a nuestro ordenador\n",
        "\n",
        "Con este codigo creamos un zip con el modelo para poder descargarlo todo junto a nuestro ordenador local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GhWuu_nBnQ3"
      },
      "outputs": [],
      "source": [
        "!zip -r /content/file.zip /content/model\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"/content/file.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I0aeb-eD26h"
      },
      "source": [
        "Si se ejecuta en el ordenador personal en vez de en colab hay que cambiar la ruta dentro del archivo Modelfile: ![Sin t√≠tulo.png](data:image/png;base64,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)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJdUc4ji-Q8c"
      },
      "source": [
        "# Ollama\n",
        "\n",
        "Unsloth ahora permite ajustar el modelo autom√°ticamente y crear un archivo de modelo, exportando a Ollama.\n",
        "\n",
        "Primero vamos a installar Ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O15au36B-Vyp",
        "outputId": "75742f0a-c04f-4774-dd23-0ec9b1bf9fa2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsSmwZzNCQeU"
      },
      "source": [
        "### Usar ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPbHMS-6CmS4"
      },
      "source": [
        "Usamos `subprocess` para iniciar Ollama de forma no bloqueante. En tu propio escritorio, simplemente puedes abrir una nueva terminal y escribir `ollama serve`, pero en Colab, necesitamos usar este truco."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wloHHZfCKKz"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3) # Esperar a que Ollama se cargue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuCqqUq4AMLO"
      },
      "source": [
        "### Usar Modelo subido a Hugginface\n",
        "Si hemos subido el modelo a hugging face podemos descargarlo con el siguiente comando (si el modelo se llama model como este caso, si no habria que sustituir el nombre del modelo por el que sea)\n",
        "Una vez termine de descargar el modelo hay que parar la ejecucion de la celda para usar el resto del cuaderno"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqEM3Lbl_x2t",
        "outputId": "9068702a-1976-46c4-cdc0-7e4b1c0e7788"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "#!ollama run hf.co/serdom02/model_8bitQ8_0\n",
        "#!ollama run hf.co/serdom02/model_16bitGGUF\n",
        "#!ollama run hf.co/serdom02/model_q4_k_mGGUF\n",
        "!ollama run llama3:8b\n",
        "#!ollama run hf.co/serdom02/Leyeneitor_8bitQ8_0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zUqVHKrBhC7"
      },
      "source": [
        "Hablar con el modelo de Hugging Face (Solo cambiamos el nombre del modelo)\n",
        "\n",
        "Hay que detener la celda anterior y volver a iniciar Ollama para poder hablar con el modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN-zen9pXhXg"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3) # Esperar a que Ollama se cargue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X3VD1EaAtnL"
      },
      "source": [
        "### Hablar con el modelo usando ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICFGEuz2GpH3"
      },
      "source": [
        "Una vez ya tenemos ollama instalado y el modelo descargado podemos interactuar con el modelo usando el siguiente codigo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSjE5z7fGpyH",
        "outputId": "e099d929-9c5a-4564-c4f5-783681972c31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Puedes presentar una denuncia ante la Agencia Espa√±ola de Protecci√≥n de Datos (AEPD) o el juez competente por el delito de vulneraci√≥n de privacidad. Si el v√≠deo se ha publicado en redes sociales, tambi√©n puedes ejercer el derecho al olvido solicitando su retirada. En Espa√±a, grabar a alguien desnudo en su propia casa sin su consentimiento es una violaci√≥n grave de la privacidad, incluso si ocurre en un lugar p√∫blico como un balc√≥n. Si la grabaci√≥n se ha difundido y perjudica tu reputaci√≥n o intimidad, puedes exigir que la eliminen inmediatamente. Si no estabas al tanto de la grabaci√≥n y no hay prueba de que consentiste, es probable que sea ilegal. En este caso, podr√≠as presentar una denuncia ante la polic√≠a por el delito de vulneraci√≥n de privacidad, ya que se ha afectado a tus derechos fundamentales. Para protegerte a√∫n m√°s, considera cambiar las contrase√±as de tus redes sociales y revisar cualquier publicaci√≥n tuya que pueda estar relacionada con este incidente.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "import re\n",
        "\n",
        "url = \"http://localhost:11434/api/chat\"\n",
        "data = {\n",
        "    \"model\": \"hf.co/serdom02/Leyeneitor_8bitQ8_0\", #Solo cambiamos esta parte\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"El vecino del bloque de enfrente me ha grabado desde su casa y me ha pillado desnudo, creo que lo ha publicado en las redes, puedo denunciar?\"}] #Aqui ponemos el mensaje\n",
        "}\n",
        "\n",
        "response = requests.post(url, json=data, stream=True)\n",
        "\n",
        "# Concatenar la respuesta completa y limpiar espacios\n",
        "full_response = \"\"\n",
        "for line in response.iter_lines():\n",
        "    if line:\n",
        "        decoded_line = json.loads(line)\n",
        "        if \"message\" in decoded_line and \"content\" in decoded_line[\"message\"]:\n",
        "            # Limpiar espacios dobles y unir las partes\n",
        "            full_response += decoded_line[\"message\"][\"content\"]\n",
        "\n",
        "# Eliminar saltos de l√≠nea y limpiar los espacios extras\n",
        "full_response = re.sub(r'\\s+', ' ', full_response).strip()\n",
        "\n",
        "print(full_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s20qCTdICnt2"
      },
      "source": [
        "Si ollama se queda congelado podemos usar kill para matar el proceso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upycRraGCsvP"
      },
      "outputs": [],
      "source": [
        "#Para resetear ollama\n",
        "!kill -9 $(ps aux | grep '[o]llama' | awk '{print $2}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQVgDT5zH57k"
      },
      "source": [
        "# Rag con Llama 3\n",
        "\n",
        "Fuente: https://medium.com/@danushidk507/rag-with-llama-using-ollama-a-deep-dive-into-retrieval-augmented-generation-c58b9a1cfcd3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiPLtIAUIFBw",
        "outputId": "38b8179f-def9-44c5-a288-5af4e67abb7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting langchain-huggingface\n",
            "  Downloading langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Collecting langchain-core<1.0.0,>=0.3.58 (from langchain)\n",
            "  Downloading langchain_core-0.3.58-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.39)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.30.2)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (3.4.1)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.21.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.13.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.15.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (11.2.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.6.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.0.2)\n",
            "Downloading langchain-0.3.25-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\n",
            "Downloading langchain_core-0.3.58-py3-none-any.whl (437 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m437.6/437.6 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, langchain-core, langchain-huggingface, langchain\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.56\n",
            "    Uninstalling langchain-core-0.3.56:\n",
            "      Successfully uninstalled langchain-core-0.3.56\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.24\n",
            "    Uninstalling langchain-0.3.24:\n",
            "      Successfully uninstalled langchain-0.3.24\n",
            "Successfully installed langchain-0.3.25 langchain-core-0.3.58 langchain-huggingface-0.1.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Collecting langchain-ollama\n",
            "  Downloading langchain_ollama-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting ollama<1,>=0.4.4 (from langchain-ollama)\n",
            "  Downloading ollama-0.4.8-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.52 in /usr/local/lib/python3.11/dist-packages (from langchain-ollama) (0.3.58)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (0.3.39)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (4.13.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (2.11.4)\n",
            "Requirement already satisfied: httpx<0.29,>=0.27 in /usr/local/lib/python3.11/dist-packages (from ollama<1,>=0.4.4->langchain-ollama) (0.28.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (3.10.18)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (2.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.3.1)\n",
            "Downloading langchain_ollama-0.3.2-py3-none-any.whl (20 kB)\n",
            "Downloading ollama-0.4.8-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: ollama, langchain-ollama\n",
            "Successfully installed langchain-ollama-0.3.2 ollama-0.4.8\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Downloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain langchain-huggingface transformers\n",
        "!pip install -U langchain-ollama\n",
        "!pip install sentence-transformers\n",
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Geh84e61IG8K"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain-community faiss-cpu\n",
        "from langchain.vectorstores import FAISS\n",
        "#print(faiss.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0txzLu3IvgQ"
      },
      "source": [
        "## Ingesta de Datos\n",
        "Comenzamos cargando y dividiendo los documentos. Utilizamos PyPDFLoader para cargar un archivo PDF y dividirlo en fragmentos m√°s peque√±os y superpuestos, lo que mejora la precisi√≥n de la recuperaci√≥n de informaci√≥n."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kpz4Ar6cT7-"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "#from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Lista de rutas de los PDFs\n",
        "pdf_paths = [\n",
        "    \"BOE-A-1978-31229-consolidado.pdf\",\n",
        "    \"BOE-A-1995-25444-consolidado_CodigoPenal.pdf\",\n",
        "    \"Protecci√≥n de Datos Personales y garantia de los derechos digitales.pdf\",\n",
        "    \"RGPD_boe.pdf\"\n",
        "]\n",
        "\n",
        "# Cargar documentos de todos los PDFs\n",
        "documents = []\n",
        "for path in pdf_paths:\n",
        "    try:\n",
        "        loader = PyPDFLoader(path)\n",
        "        documents.extend(loader.load())\n",
        "    except Exception as e:\n",
        "        print(f\"Error cargando {path}: {e}\")\n",
        "\n",
        "for doc in documents:\n",
        "    doc.page_content = doc.page_content.replace('\\r\\n', '\\n').replace('\\n\\n', '\\n').strip()\n",
        "\n",
        "\n",
        "# Dividir los documentos en fragmentos\n",
        "#text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=30, separator=\"\\n\")\n",
        "#docs = text_splitter.split_documents(documents=documents)\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=800,\n",
        "    chunk_overlap=200,\n",
        "    separators=[\"\\n\\n\\n\", \"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
        "    tokenizer_name=\"cl100k_base\"  # Compatible con modelos m√°s modernos\n",
        ")\n",
        "\n",
        "\n",
        "docs = text_splitter.split_documents(documents=documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wmAhvLleAo3"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksK6pmIeJait"
      },
      "source": [
        "## Embeddings de Datos y Almacenamiento con FAISS\n",
        "\n",
        "FAISS (Facebook AI Similarity Search) es una biblioteca vers√°til y eficiente para la b√∫squeda de similitudes en vectores. Permite la recuperaci√≥n escalable y r√°pida de embeddings.\n",
        "Elecci√≥n del Modelo de Embedding\n",
        "\n",
        "Utilizamos sentence-transformers/all-mpnet-base-v2, conocido por su rendimiento robusto en diversas tareas de procesamiento de texto. Alternativas como BGE o MiniLM pueden ser utilizadas para equilibrar la velocidad y la precisi√≥n seg√∫n el caso espec√≠fico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNUGuqwXJn6u"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import faiss\n",
        "\n",
        "# Load embedding model\n",
        "#embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "embedding_model_name = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
        "#model_kwargs = {\"device\": \"cuda\"}\n",
        "model_kwargs = {\"device\": \"cpu\"}\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=embedding_model_name,\n",
        "    model_kwargs=model_kwargs\n",
        ")\n",
        "\n",
        "# Create FAISS vector store\n",
        "vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "# Save and reload the vector store\n",
        "vectorstore.save_local(\"faiss_index_\")\n",
        "persisted_vectorstore = FAISS.load_local(\"faiss_index_\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "# Create a retriever\n",
        "retriever = persisted_vectorstore.as_retriever()\n",
        "\n",
        "print(f\"Se indexaron {len(docs)} fragmentos.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ3LCsD0JplS"
      },
      "source": [
        "## Seleccionamos el modelo\n",
        "\n",
        "Aqui podemos probar como responde el modelo sin RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cI3DdntQT9EB",
        "outputId": "86df65ab-32fd-4adf-de0f-c3b8788c513b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**\"La web de la seguridad\"**\n",
            "\n",
            "Mar√≠a era una estudiante que hab√≠a creado una cuenta en la web de un concurso de programaci√≥n para practicar sus habilidades. Sin embargo, meses despu√©s, comenz√≥ a recibir publicidad en su correo electr√≥nico sobre productos financieros que nunca hab√≠a solicitado. Al investigar, descubri√≥ que el concurso hab√≠a compartido sus datos personales con empresas de marketing sin informarle previamente.\n",
            "\n",
            "Mar√≠a decidi√≥ ejercer sus derechos y contact√≥ con la organizaci√≥n para reclamar la eliminaci√≥n de estos datos y solicitar informaci√≥n sobre qu√© se hab√≠an enviado. La respuesta inicial fue evasiva, pero despu√©s de insistir, la empresa confirm√≥ que hab√≠a sufrido una filtraci√≥n de datos y que algunos registros hab√≠an sido compartidos con terceros sin el consentimiento expl√≠cito del participante.\n",
            "\n",
            "Este incidente le record√≥ la importancia de leer los t√©rminos y condiciones antes de registrarse en cualquier web. Adem√°s, se asegur√≥ de cambiar sus credenciales y desactivar las notificaciones no esenciales para minimizar el riesgo de nuevas filtraciones.\n",
            "\n",
            "**¬øQu√© pod√≠a hacer Mar√≠a?**\n",
            "\n",
            "* Ejercer su derecho de supresi√≥n (derecho al olvido) para que eliminaran sus datos personales de los sistemas del concurso, incluyendo las env√≠os publicitarios.\n",
            "* Solicitar a la organizaci√≥n detalles sobre qu√© datos se hab√≠an compartido y con qu√© finalidad.\n",
            "* Presentar una denuncia ante la autoridad de protecci√≥n de datos si consideraba que hab√≠a existido una vulneraci√≥n grave de las normas de protecci√≥n.\n",
            "\n",
            "Este caso ilustra la importancia de transparencia en el tratamiento de datos personales, especialmente cuando se comparten con terceros. Las organizaciones deben informar claramente a los usuarios sobre el uso de sus datos y ofrecerles herramientas para ejercer sus derechos.\n"
          ]
        }
      ],
      "source": [
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# Initialize the LLaMA model\n",
        "llm = OllamaLLM(model=\"hf.co/serdom02/Leyeneitor_8bitQ8_0\", base_url=\"http://127.0.0.1:11434\") #aqui ponemos el modelo finetuneado\n",
        "\n",
        "# Test with a sample prompt\n",
        "response = llm.invoke(\"Un cuento sobre proteccion de datos\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taF4AoOWMhqh"
      },
      "source": [
        "## Ahora podemos interactuar con el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYF2u4QOa8T-"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3) # Esperar a que Ollama se cargue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "wuv0msS5i3ww",
        "outputId": "c39db064-30c8-4edc-a364-454806e41b06",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\sdominguez\\AppData\\Local\\Temp\\ipykernel_17028\\10883580.py:34: LangChainDeprecationWarning: This class is deprecated. See the following migration guides for replacements based on `chain_type`:\n",
            "stuff: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain\n",
            "map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\n",
            "refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\n",
            "map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\n",
            "\n",
            "See also guides on retrieval and question-answering here: https://python.langchain.com/docs/how_to/#qa-with-rag\n",
            "  chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=chat_prompt)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "Asistente legal de protecci√≥n de datos listo (usando load_qa_chain con prompt separado). Escribe 'Exit' para salir.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stdin",
          "output_type": "stream",
          "text": [
            "\n",
            "Pregunta:  Hola me llamo sergio\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Respuesta:\n",
            "**Problema:** Un colegio desea enviar notificaciones a padres y alumnos mediante una plataforma de mensajer√≠a m√≥vil, almacenando datos personales y actividades de los usuarios.\n",
            "\n",
            "**An√°lisis:** La normativa espa√±ola exige que el tratamiento de datos en esta plataforma cumpla con el principio de minimizaci√≥n. Para cumplir con este requisito, la escuela debe informar claramente a los padres y alumnos sobre qu√© informaci√≥n se almacena, con qui√©n se comparte y para qu√© finalidad. Adem√°s, debe ofrecer la posibilidad de ejercer los derechos de acceso, rectificaci√≥n o supresi√≥n de estos datos.\n",
            "\n",
            "**Conclusi√≥n:** El colegio debe informar de forma clara y transparente sobre el tratamiento de datos personales en la plataforma de mensajer√≠a m√≥vil, obteniendo el consentimiento adecuado para su uso.\n",
            "\n",
            "Fuentes Recuperadas:\n",
            "  [1] Fuente: BOE-A-1995-25444-consolidado_CodigoPenal.pdf, P√°gina: 93\n",
            "     Fragmento: comunicaci√≥n interior con √©l, y con el cual formen una unidad f√≠sica.\n",
            "BOLET√çN OFICIAL DEL ESTADO\n",
            "LEGISLACI√ìN CONSOLIDADA\n",
            "P√°gina 94...\n",
            "  [2] Fuente: RGPD_boe.pdf, P√°gina: 38\n",
            "     Fragmento: tratamient o, en f or ma concisa, transparente, inteligible y de f √°cil acceso, con un lenguaj e claro y sencillo, en par ticular \n",
            "cualquier inf or ma...\n",
            "  [3] Fuente: RGPD_boe.pdf, P√°gina: 21\n",
            "     Fragmento: desempe √±ar las funciones que se le conf ieran de conf or midad con el presente Reg lamento. Lo anter ior debe \n",
            "4.5.2016 L 119/22 Diar io Oficial de l...\n",
            "  [4] Fuente: RGPD_boe.pdf, P√°gina: 75\n",
            "     Fragmento: ar t√≠culos 64 y 65, sin per juicio de las funciones de las autori dades de control nacionales; \n",
            "4.5.2016 L 119/76 Diar io Oficial de la U ni√≥n Europea...\n",
            "  [5] Fuente: BOE-A-1995-25444-consolidado_CodigoPenal.pdf, P√°gina: 55\n",
            "     Fragmento: relevante de todos ellos.\n",
            "BOLET√çN OFICIAL DEL ESTADO\n",
            "LEGISLACI√ìN CONSOLIDADA\n",
            "P√°gina 56...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# --- 3. Bucle de Consulta Interactivo ---\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPregunta: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1263\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1266\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1267\u001b[0m )\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "# Importaci√≥n correcta para versiones recientes de Langchain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "import torch\n",
        "\n",
        "# --- 1. Separaci√≥n de roles con ChatPromptTemplate ---\n",
        "system_prompt = \"\"\"\n",
        "Eres un experto legal en protecci√≥n de datos en Espa√±a.\n",
        "Tu tarea es analizar la legalidad de las situaciones planteadas por el usuario.\n",
        "La respuesta debe ser clara, rigurosa y formal, como si fuera escrita por un profesional del derecho.\n",
        "No hagas suposiciones. No generalices. No repitas ideas.\n",
        "\"\"\"\n",
        "\n",
        "user_prompt = \"\"\"\n",
        "Fragmentos recuperados:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "A continuaci√≥n, presenta el an√°lisis legal y la conclusi√≥n en tres partes:\n",
        "1. **Introducci√≥n breve** del problema legal.\n",
        "2. **An√°lisis jur√≠dico** apoyado en los fragmentos recuperados, citando tus fuentes.\n",
        "3. **Conclusi√≥n clara**, con un juicio legal concreto.\n",
        "\"\"\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
        "    HumanMessagePromptTemplate.from_template(user_prompt)\n",
        "])\n",
        "\n",
        "# --- 2. Crear la cadena usando load_qa_chain con el prompt de chat ---\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=chat_prompt)\n",
        "\n",
        "print(\"-\"*50)\n",
        "print(\"Asistente legal de protecci√≥n de datos listo (usando load_qa_chain con prompt separado). Escribe 'Exit' para salir.\")\n",
        "print(\"-\"*50)\n",
        "\n",
        "# --- 3. Bucle de Consulta Interactivo ---\n",
        "while True:\n",
        "    query = input(\"\\nPregunta: \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "    if not query.strip():\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        retriever.search_kwargs = {'k': 5}\n",
        "        docs_retrieved = retriever.invoke(query)\n",
        "\n",
        "        input_data = {\n",
        "            \"input_documents\": docs_retrieved,\n",
        "            \"question\": query\n",
        "        }\n",
        "\n",
        "        result = chain.invoke(input_data)\n",
        "\n",
        "        if isinstance(result, dict) and 'output_text' in result:\n",
        "            print(\"\\nRespuesta:\")\n",
        "            print(result['output_text'])\n",
        "        elif isinstance(result, str):\n",
        "            print(\"\\nRespuesta:\")\n",
        "            print(result)\n",
        "        else:\n",
        "            print(\"\\nRespuesta recibida (formato inesperado):\")\n",
        "            print(result)\n",
        "\n",
        "        print(\"\\nFuentes Recuperadas:\")\n",
        "        for i, doc in enumerate(docs_retrieved):\n",
        "            source = doc.metadata.get('source', 'N/A')\n",
        "            page = doc.metadata.get('page', 'N/A')\n",
        "            print(f\"  [{i+1}] Fuente: {source}, P√°gina: {page}\")\n",
        "            print(f\"     Fragmento: {doc.page_content[:150]}...\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError durante la ejecuci√≥n de la cadena: {e}\")\n",
        "\n",
        "print(\"\\nSaliendo del asistente.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M6vxs-EuSUW"
      },
      "source": [
        "Plantillas para Propmt Template:\n",
        "\n",
        "**Plantilla1 Muy restrictiva, se centra en usar solo datos sacados del RAG:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqmjISNbvMYZ"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"\n",
        "Eres un asistente legal experto en protecci√≥n de datos en Espa√±a.\n",
        "Tu tarea es analizar la legalidad de la situaci√≥n descrita en la 'Pregunta' bas√°ndote √öNICA y EXCLUSIVAMENTE en los siguientes 'Textos Legales Recuperados'.\n",
        "NO uses ning√∫n conocimiento externo. Si la informaci√≥n en los textos no es suficiente para dar una respuesta fundada, ind√≠calo claramente.\n",
        "Justifica tu respuesta paso a paso, haciendo referencia expl√≠cita a los art√≠culos o secciones relevantes de los textos proporcionados si es posible.\n",
        "\n",
        "Textos Legales Recuperados:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "An√°lisis Legal y Conclusi√≥n (Basado S√ìLO en los textos recuperados):\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFL_ls9VvI8j"
      },
      "source": [
        "**Plantilla 2 Menos restrictiva, permite al modelo usar sus conocimientos adquiridos del finetunning y razonar un poco fuera de los contenidos del RAG**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPlb7geDvH70"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"\n",
        "Eres un asistente legal experto en protecci√≥n de datos en Espa√±a.\n",
        "Tu tarea es analizar la legalidad de la situaci√≥n descrita en la 'Pregunta' bas√°ndote **principalmente** en los siguientes 'Textos Legales Recuperados'.\n",
        "Si los textos recuperados contienen suficiente informaci√≥n, responde √∫nicamente bas√°ndote en ellos.\n",
        "Si los textos no contienen una respuesta clara pero la pregunta se refiere a conceptos b√°sicos del RGPD, usa lo aprendido en el entrenamiento para dar una respuesta general, especificando que no proviene de los textos recuperados.\n",
        "Si la pregunta requiere informaci√≥n espec√≠fica que no est√° en los textos recuperados ni en tu entrenamiento, ind√≠calo claramente.\n",
        "\n",
        "Textos Legales Recuperados:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "An√°lisis Legal y Conclusi√≥n:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GEM9djZltvB"
      },
      "source": [
        "## Clasificaci√≥n de consultas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWFAIrWxjGWa"
      },
      "source": [
        "El Problema de la funci√≥n anterior es que si el usuario intenta interactuar con el modelo para temas no legales, por ejemplo, saludar, al incluir los resultados del RAG en el propmt el modelo siempre va a intentar responder usando el contexto legal que recibe del RAG, por ejemplo:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgdnHyrdjjlJ"
      },
      "source": [
        "\n",
        "\n",
        "> Pregunta: Hola me llamo sergio\n",
        "\n",
        "> Respuesta:\n",
        "\"La pregunta plantea si el usuario puede solicitar a una empresa que elimine sus datos personales,\n",
        "aunque la normativa oblige a conservarlos. Como experto en protecci√≥n de datos,\n",
        "debes explicar que solo se pueden conservar los datos estrictamente necesarios para cumplir con obligaciones legales,\n",
        "y que el usuario debe ser informado sobre el plazo de conservaci√≥n.\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwWDkkm6jtVJ"
      },
      "source": [
        "Para solucionar esto hay que analizar la entrada del usuario para verificar si es una consulta legal que necesite de RAG o simplemente esta interactuando con el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4-fTIcUsDe6"
      },
      "outputs": [],
      "source": [
        "llm_evaluador = OllamaLLM(model=\"llama3:8b\",base_url=\"http://127.0.0.1:11434\") #aqui ponemos el modelo finetuneado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xqpL5-k9km_e",
        "outputId": "3171be9f-cda3-4d23-b448-8970d71d7237",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\sdominguez\\AppData\\Local\\Temp\\ipykernel_9964\\334576307.py:87: LangChainDeprecationWarning: This class is deprecated. See the following migration guides for replacements based on `chain_type`:\n",
            "stuff: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain\n",
            "map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\n",
            "refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\n",
            "map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\n",
            "\n",
            "See also guides on retrieval and question-answering here: https://python.langchain.com/docs/how_to/#qa-with-rag\n",
            "  chain_rag = load_qa_chain(llm, chain_type=\"stuff\", prompt=chat_prompt_rag)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "Asistente listo. Escribe 'Exit' para salir.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[15], line 96\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPregunta: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     98\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1263\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1266\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1267\u001b[0m )\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain  # Necesario para la clasificaci√≥n\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "import torch\n",
        "\n",
        "# --- 1. Funci√≥n de Clasificaci√≥n de Intenci√≥n (Modelo de Clasificaci√≥n Separado) ---\n",
        "def classify_intent_with_classifier(query, llm_evaluador):\n",
        "    \"\"\"Clasifica si se necesita usar RAG para generar la respuesta.\"\"\"\n",
        "    classification_prompt_template = \"\"\"\n",
        "    Analiza la siguiente pregunta del usuario. Determina si la pregunta requiere un an√°lisis legal sobre protecci√≥n de datos en Espa√±a (como RGPD, LOPDGDD) o si se puede responder sin el uso de RAG, es decir, si el modelo tiene suficiente conocimiento para dar una respuesta sin fragmentos adicionales.\n",
        "\n",
        "    Responde √∫nicamente con la palabra \"USAR_RAG\" si la pregunta requiere fragmentos recuperados para una respuesta precisa.\n",
        "    Responde √∫nicamente con la palabra \"NO_RAG\" si la pregunta no requiere fragmentos adicionales y puede ser respondida directamente con el conocimiento general del modelo.\n",
        "\n",
        "    Pregunta del usuario: \"{user_query}\"\n",
        "\n",
        "    Clasificaci√≥n (USAR_RAG o NO_RAG):\"\"\"\n",
        "\n",
        "    prompt = PromptTemplate(template=classification_prompt_template, input_variables=[\"user_query\"])\n",
        "\n",
        "    classification_runnable = prompt | llm_evaluador\n",
        "\n",
        "    try:\n",
        "        result = classification_runnable.invoke({\"user_query\": query})\n",
        "\n",
        "        # Extraer texto. Ajusta la clave ('text') si tu LLM devuelve otra cosa.\n",
        "        if isinstance(result, dict):\n",
        "            answer_text = result.get('text', '').strip().upper()\n",
        "        elif isinstance(result, str):\n",
        "            answer_text = result.strip().upper()\n",
        "        else:\n",
        "            answer_text = str(result).strip().upper()\n",
        "\n",
        "        # Debug de la clasificaci√≥n\n",
        "        print(f\"--- DEBUG: Clasificaci√≥n LLM respondi√≥: '{answer_text}' ---\")\n",
        "\n",
        "        if \"USAR_RAG\" in answer_text:\n",
        "            return \"USAR_RAG\"\n",
        "        elif \"NO_RAG\" in answer_text:\n",
        "            return \"NO_RAG\"\n",
        "        else:\n",
        "            print(\"--- WARN: Respuesta de clasificaci√≥n no clara, asumiendo NO_RAG por seguridad ---\")\n",
        "            return \"NO_RAG\"  # Default seguro\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error en la clasificaci√≥n de intenci√≥n con LLM: {e}\")\n",
        "        return \"USAR_RAG\"  # Default seguro en caso de error\n",
        "\n",
        "# --- 2. Definiciones para el prompt de RAG ---\n",
        "system_prompt_rag = \"\"\"\n",
        "Eres un experto legal en protecci√≥n de datos en Espa√±a.\n",
        "Tu tarea es analizar la legalidad de las situaciones planteadas por el usuario utilizando fragmentos recuperados.\n",
        "La respuesta debe ser clara, rigurosa y formal, como si fuera escrita por un profesional del derecho.\n",
        "No hagas suposiciones. No generalices. No repitas ideas.\n",
        "\"\"\"\n",
        "\n",
        "user_prompt_rag = \"\"\"\n",
        "Fragmentos recuperados:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "A continuaci√≥n, presenta el an√°lisis legal y la conclusi√≥n en tres partes:\n",
        "1. **Introducci√≥n breve** del problema legal.\n",
        "2. **An√°lisis jur√≠dico** apoyado en los fragmentos recuperados, citando tus fuentes.\n",
        "3. **Conclusi√≥n clara**, con un juicio legal concreto.\n",
        "\"\"\"\n",
        "\n",
        "chat_prompt_rag = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_prompt_rag),\n",
        "    HumanMessagePromptTemplate.from_template(user_prompt_rag)\n",
        "])\n",
        "\n",
        "# --- 3. Definiciones para la respuesta conversacional ---\n",
        "conversational_prompt_template = \"\"\"\n",
        "Ya no eres un asistente legal, ahora eres un asistente de IA amigable y conversador. Responde directamente al usuario de forma natural.\n",
        "Usuario: {user_input}\n",
        "Asistente:\"\"\"\n",
        "PROMPT_CONVERSATIONAL = PromptTemplate(template=conversational_prompt_template, input_variables=[\"user_input\"])\n",
        "\n",
        "# --- 4. Crear la cadena usando load_qa_chain para el modelo de RAG ---\n",
        "chain_rag = load_qa_chain(llm, chain_type=\"stuff\", prompt=chat_prompt_rag)\n",
        "\n",
        "# --- 5. Bucle de Consulta Interactivo ---\n",
        "\n",
        "print(\"-\"*50)\n",
        "print(\"Asistente listo. Escribe 'Exit' para salir.\")\n",
        "print(\"-\"*50)\n",
        "\n",
        "while True:\n",
        "    query = input(\"\\nPregunta: \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "    if not query.strip():\n",
        "        continue\n",
        "\n",
        "    # --- Paso 1: Clasificar la intenci√≥n del usuario ---\n",
        "    intent = classify_intent_with_classifier(query, llm_evaluador)\n",
        "    print(f\"--- Intenci√≥n Detectada: {intent} ---\")\n",
        "\n",
        "    # --- Ejecutar flujo seg√∫n la intenci√≥n ---\n",
        "    if intent == \"USAR_RAG\":\n",
        "        print(\"--- Ejecutando RAG para consulta legal ---\")\n",
        "        try:\n",
        "            # 1. Recuperar documentos (RAG)\n",
        "            retriever.search_kwargs = {'k': 3}  # Ajusta 'k' si es necesario\n",
        "            docs_retrieved = retriever.invoke(query)\n",
        "\n",
        "            # 2. Preparar input para la cadena RAG\n",
        "            input_data = {\n",
        "                \"input_documents\": docs_retrieved,\n",
        "                \"question\": query\n",
        "            }\n",
        "\n",
        "            # 3. Ejecutar la cadena RAG\n",
        "            result = chain_rag.invoke(input_data)\n",
        "\n",
        "            # 4. Imprimir resultado RAG\n",
        "            if isinstance(result, dict) and 'output_text' in result:\n",
        "                print(\"\\nRespuesta (Legal):\")\n",
        "                print(result['output_text'])\n",
        "            elif isinstance(result, str):\n",
        "                print(\"\\nRespuesta (Legal):\")\n",
        "                print(result)\n",
        "            else:\n",
        "                print(\"\\nRespuesta RAG recibida (formato inesperado):\")\n",
        "                print(result)\n",
        "\n",
        "            # 5. Imprimir fuentes (Opcional)\n",
        "            print(\"\\nFuentes Recuperadas:\")\n",
        "            for i, doc in enumerate(docs_retrieved):\n",
        "                source = doc.metadata.get('source', 'N/A')\n",
        "                page = doc.metadata.get('page', 'N/A')\n",
        "                print(f\"  [{i+1}] Fuente: {source}, P√°gina: {page}\")\n",
        "                print(f\"     Fragmento: {doc.page_content[:150]}...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError durante la ejecuci√≥n de la cadena RAG: {e}\")\n",
        "\n",
        "    elif intent == \"NO_RAG\":\n",
        "        print(\"--- Generando respuesta sin usar RAG ---\")\n",
        "        try:\n",
        "            # 1. Formatear prompt conversacional simple\n",
        "            simple_prompt = PROMPT_CONVERSATIONAL.format(user_input=query)\n",
        "\n",
        "            # 2. Llamar directamente al LLM (sin RAG)\n",
        "            response = llm.invoke(simple_prompt)\n",
        "\n",
        "            # 3. Extraer y limpiar la respuesta\n",
        "            if isinstance(response, dict):\n",
        "                 # Adapta la clave si 'invoke' devuelve un dict con otra clave para el texto\n",
        "                 response_text = response.get('text', str(response))\n",
        "            elif isinstance(response, str):\n",
        "                 response_text = response\n",
        "            else:\n",
        "                 response_text = str(response)\n",
        "\n",
        "            # Limpieza: quitar el prompt si el LLM lo repite\n",
        "            print(\"\\nRespuesta (Conversacional):\")\n",
        "            print(response_text)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError durante la llamada conversacional al LLM: {e}\")\n",
        "    else:\n",
        "        print(f\"--- ERROR: Intenci√≥n desconocida '{intent}' recibida de la clasificaci√≥n ---\")\n",
        "\n",
        "print(\"\\nSaliendo del asistente.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHWFiDgzo35z"
      },
      "source": [
        "Vemos que ahora si le decimos algo que no es claramente algo relacionado con temas legales, como \"Hola me llamo Sergio\" se detecta que es una conversaci√≥n normal y no se activa el RAG:\n",
        "\n",
        "\n",
        "> Pregunta: Hola me llamo sergio\n",
        "\n",
        "\n",
        "> Respuesta (Conversacional):\n",
        "Hola Sergio, ¬øen qu√© puedo ayudarte hoy?\n",
        "\n",
        "¬øPero que pasa si hacemos una pregunta legal y luego seguimos intentando consultar al modelo sobre la pregunta inicial?\n",
        "\n",
        "\n",
        "\n",
        "> Pregunta:  Tengo una consulta sobre protecci√≥n de datos, mi gimnasio me pide mi huella dactilar para poder entrar al gimnasio, ¬øpueden exirgirmelo?\n",
        "\n",
        "\n",
        "> Respuesta (Legal):\n",
        "**Introducci√≥n**: Un gimnasio quiere usar datos biom√©tricos (como la huella dactilar) para controlar el acceso a sus instalaciones. Este tratamiento de datos personales requiere una base legal clara y garant√≠as de seguridad adecuadas.\n",
        "**An√°lisis**: El uso de datos biom√©tricos para fines como el control de acceso debe basarse en un principio de consentimiento expl√≠cito, salvo que exista una justificaci√≥n legal espec√≠fica (como la seguridad p√∫blica). En este caso, se deben aplicar medidas adicionales para proteger estos datos sensibles, ya que permiten identificar de forma √∫nica a cada persona. Adem√°s, el gimnasio debe informar claramente sobre el prop√≥sito del tratamiento y las garant√≠as de seguridad adoptadas.\n",
        "**Conclusi√≥n**: Para que este tratamiento sea l√≠cito, debe existir una base legal clara (como el consentimiento informado o una obligaci√≥n legal) y se deben aplicar medidas adecuadas para proteger estos datos sensibles.\n",
        "\n",
        "\n",
        "\n",
        "> Pregunta: ¬øPuedo negarme?\n",
        "\n",
        "\n",
        "> Respuesta (Legal):\n",
        "Una empresa farmac√©utica me ha solicitado informaci√≥n personal para estudiar la eficacia de un medicamento, pero prefiero no participar. ¬øTengo derecho a negarme?\n",
        "An√°lisis legal: Por ser un estudio cl√≠nico, se requiere tu consentimiento expl√≠cito. Si no das tu autorizaci√≥n, no pueden procesar tus datos. Sin embargo, si la investigaci√≥n tiene relevancia para el bienestar p√∫blico o hay una obligaci√≥n legal, podr√≠an excepcionalmente utilizar los datos an√≥nimos o con medidas de protecci√≥n espec√≠ficas.\n",
        "Conclusi√≥n: Tienes derecho a negarte si el estudio cl√≠nico no cumple con tus derechos y garant√≠as de privacidad.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Podemos observar que el modelo al activarse el RAG pierde el contexto de la pregunta anterior y responde algo totalmente diferente, seguramente debido a que RAG ha recuperado textos donde se incluya \"¬øpuedo negarme?\" que no estan relacionados con el contexto anterior, por lo que se inyecta contexto erroneo en el propmt.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8LOwn4wq58V"
      },
      "source": [
        "## Memoria Conversacional con RAG\n",
        "\n",
        "Para solucionar el problema de continuidad del contexto en la conversaci√≥n vamos a a√±adir memoria al RAG, para ello, LangChain tiene una variedad de herramientas que permite implementarlo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mERu0HnsDe6",
        "outputId": "617c97bd-3ea1-483c-9950-ac15de5290d0",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "Asistente listo. Escribe 'Exit' para salir.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stdin",
          "output_type": "stream",
          "text": [
            "\n",
            "Pregunta:  Hola me llamo sergio\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 11:42:53,890 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- DEBUG: Clasificaci√≥n LLM respondi√≥: 'NO_RAG' ---\n",
            "--- Intenci√≥n Detectada: NO_RAG ---\n",
            "--- Generando respuesta sin usar RAG ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 11:43:00,024 - asyncio - ERROR - Task exception was never retrieved\n",
            "future: <Task finished name='Task-1' coro=<Server.serve() done, defined at C:\\ProgramData\\anaconda3\\Lib\\site-packages\\uvicorn\\server.py:68> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\uvicorn\\main.py\", line 580, in run\n",
            "    server.run()\n",
            "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\uvicorn\\server.py\", line 66, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"C:\\ProgramData\\anaconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"C:\\ProgramData\\anaconda3\\Lib\\asyncio\\tasks.py\", line 396, in __wakeup\n",
            "    self.__step()\n",
            "  File \"C:\\ProgramData\\anaconda3\\Lib\\asyncio\\tasks.py\", line 303, in __step\n",
            "    self.__step_run_and_handle_result(exc)\n",
            "  File \"C:\\ProgramData\\anaconda3\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\uvicorn\\server.py\", line 69, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"C:\\ProgramData\\anaconda3\\Lib\\contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\uvicorn\\server.py\", line 330, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[14], line 176\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    171\u001b[0m     prompt_with_memory \u001b[38;5;241m=\u001b[39m PROMPT_CONVERSATIONAL\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    172\u001b[0m         chat_history\u001b[38;5;241m=\u001b[39mmemory\u001b[38;5;241m.\u001b[39mget_memory_as_text(),\n\u001b[0;32m    173\u001b[0m         user_input\u001b[38;5;241m=\u001b[39mquery\n\u001b[0;32m    174\u001b[0m     )\n\u001b[1;32m--> 176\u001b[0m     response \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(prompt_with_memory)\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    179\u001b[0m         response_text \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mstr\u001b[39m(response))\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\language_models\\llms.py:387\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    384\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    385\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 387\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    388\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    389\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    390\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    391\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    392\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    393\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    394\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    395\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    396\u001b[0m         )\n\u001b[0;32m    397\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    398\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    399\u001b[0m     )\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\language_models\\llms.py:764\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    761\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    762\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    763\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\language_models\\llms.py:971\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    957\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    958\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    959\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    969\u001b[0m         )\n\u001b[0;32m    970\u001b[0m     ]\n\u001b[1;32m--> 971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    972\u001b[0m         prompts,\n\u001b[0;32m    973\u001b[0m         stop,\n\u001b[0;32m    974\u001b[0m         run_managers,\n\u001b[0;32m    975\u001b[0m         new_arg_supported\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(new_arg_supported),\n\u001b[0;32m    976\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    977\u001b[0m     )\n\u001b[0;32m    978\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    979\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    980\u001b[0m         callback_managers[idx]\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    981\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m missing_prompt_idxs\n\u001b[0;32m    989\u001b[0m     ]\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\language_models\\llms.py:790\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    781\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    787\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    788\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    789\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 790\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    791\u001b[0m                 prompts,\n\u001b[0;32m    792\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    793\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    794\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    795\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    796\u001b[0m             )\n\u001b[0;32m    797\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    798\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    799\u001b[0m         )\n\u001b[0;32m    800\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    801\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\langchain_ollama\\llms.py:290\u001b[0m, in \u001b[0;36mOllamaLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    288\u001b[0m generations \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m--> 290\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream_with_aggregation(\n\u001b[0;32m    291\u001b[0m         prompt,\n\u001b[0;32m    292\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    293\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m    294\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    295\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    296\u001b[0m     )\n\u001b[0;32m    297\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([final_chunk])\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\langchain_ollama\\llms.py:258\u001b[0m, in \u001b[0;36mOllamaLLM._stream_with_aggregation\u001b[1;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stream_with_aggregation\u001b[39m(\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    251\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    256\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GenerationChunk:\n\u001b[0;32m    257\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 258\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_generate_stream(prompt, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stream_resp, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    260\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m GenerationChunk(\n\u001b[0;32m    261\u001b[0m                 text\u001b[38;5;241m=\u001b[39mstream_resp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m stream_resp \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    262\u001b[0m                 generation_info\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    263\u001b[0m                     \u001b[38;5;28mdict\u001b[39m(stream_resp) \u001b[38;5;28;01mif\u001b[39;00m stream_resp\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    264\u001b[0m                 ),\n\u001b[0;32m    265\u001b[0m             )\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\langchain_ollama\\llms.py:213\u001b[0m, in \u001b[0;36mOllamaLLM._create_generate_stream\u001b[1;34m(self, prompt, stop, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_generate_stream\u001b[39m(\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    209\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    210\u001b[0m     stop: Optional[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    212\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Union[Mapping[\u001b[38;5;28mstr\u001b[39m, Any], \u001b[38;5;28mstr\u001b[39m]]:\n\u001b[1;32m--> 213\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_params(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    215\u001b[0m     )\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\ollama\\_client.py:163\u001b[0m, in \u001b[0;36mClient._request.<locals>.inner\u001b[1;34m()\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m():\n\u001b[1;32m--> 163\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mstream(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m r:\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m       r\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:870\u001b[0m, in \u001b[0;36mClient.stream\u001b[1;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[0;32m    847\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    848\u001b[0m \u001b[38;5;124;03mAlternative to `httpx.request()` that streams the response body\u001b[39;00m\n\u001b[0;32m    849\u001b[0m \u001b[38;5;124;03minstead of loading it into memory at once.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;124;03m[0]: /quickstart#streaming-responses\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    857\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[0;32m    858\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    859\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    868\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[0;32m    869\u001b[0m )\n\u001b[1;32m--> 870\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(\n\u001b[0;32m    871\u001b[0m     request\u001b[38;5;241m=\u001b[39mrequest,\n\u001b[0;32m    872\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[0;32m    873\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[0;32m    874\u001b[0m     stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    875\u001b[0m )\n\u001b[0;32m    876\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    877\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m response\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    906\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[0;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[0;32m    910\u001b[0m )\n\u001b[0;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_auth(\n\u001b[0;32m    915\u001b[0m     request,\n\u001b[0;32m    916\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[0;32m    917\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[0;32m    918\u001b[0m     history\u001b[38;5;241m=\u001b[39m[],\n\u001b[0;32m    919\u001b[0m )\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_redirects(\n\u001b[0;32m    943\u001b[0m         request,\n\u001b[0;32m    944\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[0;32m    945\u001b[0m         history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[0;32m    946\u001b[0m     )\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    977\u001b[0m     hook(request)\n\u001b[1;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_single_request(request)\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m   1010\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1011\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1012\u001b[0m     )\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[1;32m-> 1015\u001b[0m     response \u001b[38;5;241m=\u001b[39m transport\u001b[38;5;241m.\u001b[39mhandle_request(request)\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[0;32m   1019\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpx\\_transports\\default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    220\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m    221\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    222\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    230\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    231\u001b[0m )\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[0;32m    238\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[0;32m    239\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    240\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[0;32m    241\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    242\u001b[0m )\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:268\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[0;32m    267\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_closed(status)\n\u001b[1;32m--> 268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:251\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 251\u001b[0m     response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;66;03m# The ConnectionNotAvailable exception is a special case, that\u001b[39;00m\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;66;03m# indicates we need to retry the request on a new connection.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;66;03m# might end up as an HTTP/2 connection, but which actually ends\u001b[39;00m\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# up as HTTP/1.1.\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool_lock:\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;66;03m# Maintain our position in the request queue, but reset the\u001b[39;00m\n\u001b[0;32m    262\u001b[0m         \u001b[38;5;66;03m# status so that the request becomes queued again.\u001b[39;00m\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectionNotAvailable()\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:133\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[1;32m--> 133\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:111\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[0;32m    105\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    106\u001b[0m     (\n\u001b[0;32m    107\u001b[0m         http_version,\n\u001b[0;32m    108\u001b[0m         status,\n\u001b[0;32m    109\u001b[0m         reason_phrase,\n\u001b[0;32m    110\u001b[0m         headers,\n\u001b[1;32m--> 111\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_response_headers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    112\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    113\u001b[0m         http_version,\n\u001b[0;32m    114\u001b[0m         status,\n\u001b[0;32m    115\u001b[0m         reason_phrase,\n\u001b[0;32m    116\u001b[0m         headers,\n\u001b[0;32m    117\u001b[0m     )\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[0;32m    120\u001b[0m     status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[0;32m    121\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m     },\n\u001b[0;32m    128\u001b[0m )\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:176\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    173\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_event(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:212\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    209\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[1;32m--> 212\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\u001b[38;5;241m.\u001b[39mread(\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREAD_NUM_BYTES, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    214\u001b[0m     )\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpcore\\_backends\\sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[1;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[1;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv(max_bytes)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain  # Necesario para la clasificaci√≥n\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "import torch\n",
        "\n",
        "# A√±adimos una clase de memoria conversacional\n",
        "class ConversationMemory:\n",
        "    def __init__(self, max_turns=10):\n",
        "        self.max_turns = max_turns\n",
        "        self.history = []\n",
        "\n",
        "    def add_turn(self, user_input, assistant_response):\n",
        "        self.history.append((user_input, assistant_response))\n",
        "        if len(self.history) > self.max_turns:\n",
        "            self.history.pop(0)\n",
        "\n",
        "    def get_memory_as_text(self):\n",
        "        return \"\\n\".join([f\"Usuario: {u}\\nAsistente: {a}\" for u, a in self.history])\n",
        "\n",
        "# --- 1. Funci√≥n de Clasificaci√≥n de Intenci√≥n (Modelo de Clasificaci√≥n Separado) ---\n",
        "def classify_intent_with_classifier(query, llm_evaluador):\n",
        "    \"\"\"Clasifica si se necesita usar RAG para generar la respuesta.\"\"\"\n",
        "    classification_prompt_template = \"\"\"\n",
        "        Analiza la siguiente consulta del usuario. Decide si es necesario usar RAG (fragmentos legales recuperados) o si el modelo puede responder directamente.\n",
        "\n",
        "        Solo debes responder con:\n",
        "        - \"USAR_RAG\" ‚Üí si se requiere un an√°lisis legal riguroso, citas precisas o fundamentaci√≥n jur√≠dica espec√≠fica.\n",
        "        - \"NO_RAG\" ‚Üí si el modelo puede dar una respuesta general, introductoria o basada en conocimiento com√∫n sin necesidad de fragmentos legales.\n",
        "\n",
        "        Ejemplos:\n",
        "        - Pregunta: \"¬øPuede una empresa ceder mis datos sin consentimiento?\" ‚Üí NO_RAG\n",
        "        - Pregunta: \"¬øQu√© dice el art√≠culo 6 del RGPD sobre el consentimiento?\" ‚Üí USAR_RAG\n",
        "        - Pregunta: \"¬øPuedes citar el art√≠culo 13 del RGPD?\" ‚Üí USAR_RAG\n",
        "        - Pregunta: \"¬øEn qu√© art√≠culo se regula el consentimiento expl√≠cito?\" ‚Üí USAR_RAG\n",
        "        - Pregunta: \"Puedes decirme en qu√© art√≠culos te basas?\" ‚Üí USAR_RAG\n",
        "        - Pregunta: \"Cita textualmente en qu√© art√≠culos te basas\" ‚Üí USAR_RAG\n",
        "        - Pregunta: \"¬øQu√© derechos tengo como interesado?\" ‚Üí NO_RAG\n",
        "        - Pregunta: \"¬øC√≥mo elimino mis datos personales?\" ‚Üí NO_RAG\n",
        "\n",
        "\n",
        "        Pregunta del usuario: \"{user_query}\"\n",
        "\n",
        "        Clasificaci√≥n (USAR_RAG o NO_RAG):\"\"\"\n",
        "\n",
        "    prompt = PromptTemplate(template=classification_prompt_template, input_variables=[\"user_query\"])\n",
        "\n",
        "    classification_runnable = prompt | llm_evaluador\n",
        "\n",
        "    try:\n",
        "        result = classification_runnable.invoke({\"user_query\": query})\n",
        "\n",
        "        # Extraer texto. Ajusta la clave ('text') si tu LLM devuelve otra cosa.\n",
        "        if isinstance(result, dict):\n",
        "            answer_text = result.get('text', '').strip().upper()\n",
        "        elif isinstance(result, str):\n",
        "            answer_text = result.strip().upper()\n",
        "        else:\n",
        "            answer_text = str(result).strip().upper()\n",
        "\n",
        "        # Debug de la clasificaci√≥n\n",
        "        print(f\"--- DEBUG: Clasificaci√≥n LLM respondi√≥: '{answer_text}' ---\")\n",
        "\n",
        "        if \"USAR_RAG\" in answer_text.upper():\n",
        "            return \"USAR_RAG\"\n",
        "        elif \"NO_RAG\" in answer_text:\n",
        "            return \"NO_RAG\"\n",
        "        else:\n",
        "            print(\"--- WARN: Respuesta de clasificaci√≥n no clara, asumiendo NO_RAG por seguridad ---\")\n",
        "            return \"NO_RAG\"  # Default seguro\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error en la clasificaci√≥n de intenci√≥n con LLM: {e}\")\n",
        "        return \"USAR_RAG\"  # Default seguro en caso de error\n",
        "\n",
        "# --- 2. Definiciones para el prompt de RAG ---\n",
        "system_prompt_rag = \"\"\"\n",
        "Eres un experto legal en protecci√≥n de datos en Espa√±a.\n",
        "Tu tarea es analizar la legalidad de las situaciones planteadas por el usuario utilizando fragmentos recuperados.\n",
        "La respuesta debe ser clara, rigurosa y formal, como si fuera escrita por un profesional del derecho.\n",
        "No hagas suposiciones. No generalices. No repitas ideas.\n",
        "\"\"\"\n",
        "\n",
        "user_prompt_rag = \"\"\"\n",
        "Fragmentos recuperados:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "A continuaci√≥n, presenta el an√°lisis legal y la conclusi√≥n en tres partes:\n",
        "1. **Introducci√≥n breve** del problema legal.\n",
        "2. **An√°lisis jur√≠dico** apoyado en los fragmentos recuperados, citando tus fuentes.\n",
        "3. **Conclusi√≥n clara**, con un juicio legal concreto.\n",
        "\"\"\"\n",
        "\n",
        "chat_prompt_rag = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_prompt_rag),\n",
        "    HumanMessagePromptTemplate.from_template(user_prompt_rag)\n",
        "])\n",
        "\n",
        "# --- 3. Definiciones para la respuesta conversacional ---\n",
        "conversational_prompt_template = \"\"\"\n",
        "Eres un asistente de IA amigable y conversador. Tu tarea es mantener una conversaci√≥n fluida y coherente con el usuario.\n",
        "Responde de forma natural y cercana, teniendo en cuenta lo que ya se ha dicho.\n",
        "\n",
        "Historial reciente de la conversaci√≥n:\n",
        "{chat_history}\n",
        "\n",
        "Usuario: {user_input}\n",
        "Asistente:\"\"\"\n",
        "\n",
        "PROMPT_CONVERSATIONAL = PromptTemplate(\n",
        "    template=conversational_prompt_template,\n",
        "    input_variables=[\"chat_history\", \"user_input\"]\n",
        ")\n",
        "\n",
        "# --- 4. Crear la cadena usando load_qa_chain para el modelo de RAG ---\n",
        "chain_rag = load_qa_chain(llm, chain_type=\"stuff\", prompt=chat_prompt_rag)\n",
        "\n",
        "# --- 5. Bucle de Consulta Interactivo ---\n",
        "\n",
        "print(\"-\"*50)\n",
        "print(\"Asistente listo. Escribe 'Exit' para salir.\")\n",
        "print(\"-\"*50)\n",
        "memory = ConversationMemory(max_turns=6)\n",
        "\n",
        "while True:\n",
        "    query = input(\"\\nPregunta: \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "    if not query.strip():\n",
        "        continue\n",
        "\n",
        "    intent = classify_intent_with_classifier(query, llm_evaluador)\n",
        "    print(f\"--- Intenci√≥n Detectada: {intent} ---\")\n",
        "\n",
        "    if intent == \"USAR_RAG\":\n",
        "        print(\"--- Ejecutando RAG para consulta legal ---\")\n",
        "        try:\n",
        "            retriever.search_kwargs = {'k': 3}\n",
        "            docs_retrieved = retriever.invoke(query)\n",
        "\n",
        "            input_data = {\n",
        "                \"input_documents\": docs_retrieved,\n",
        "                \"question\": query\n",
        "            }\n",
        "\n",
        "            result = chain_rag.invoke(input_data)\n",
        "            response_text = result.get('output_text', result if isinstance(result, str) else str(result))\n",
        "\n",
        "            print(\"\\nRespuesta (Legal):\")\n",
        "            print(response_text)\n",
        "\n",
        "            memory.add_turn(query, response_text)\n",
        "\n",
        "            print(\"\\nFuentes Recuperadas:\")\n",
        "            for i, doc in enumerate(docs_retrieved):\n",
        "                source = doc.metadata.get('source', 'N/A')\n",
        "                page = doc.metadata.get('page', 'N/A')\n",
        "                print(f\"  [{i+1}] Fuente: {source}, P√°gina: {page}\")\n",
        "                print(f\"     Fragmento: {doc.page_content[:150]}...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError durante la ejecuci√≥n de la cadena RAG: {e}\")\n",
        "\n",
        "    elif intent == \"NO_RAG\":\n",
        "        print(\"--- Generando respuesta sin usar RAG ---\")\n",
        "        try:\n",
        "            prompt_with_memory = PROMPT_CONVERSATIONAL.format(\n",
        "                chat_history=memory.get_memory_as_text(),\n",
        "                user_input=query\n",
        "            )\n",
        "\n",
        "            response = llm.invoke(prompt_with_memory)\n",
        "\n",
        "            if isinstance(response, dict):\n",
        "                response_text = response.get('text', str(response))\n",
        "            elif isinstance(response, str):\n",
        "                response_text = response\n",
        "            else:\n",
        "                response_text = str(response)\n",
        "\n",
        "            if \"Asistente:\" in response_text:\n",
        "                response_text = response_text.split(\"Asistente:\", 1)[-1].strip()\n",
        "\n",
        "            print(\"\\nRespuesta (Conversacional):\")\n",
        "            print(response_text)\n",
        "\n",
        "            memory.add_turn(query, response_text)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError durante la llamada conversacional al LLM: {e}\")\n",
        "    else:\n",
        "        print(f\"--- ERROR: Intenci√≥n desconocida '{intent}' recibida de la clasificaci√≥n ---\")\n",
        "\n",
        "print(\"\\nSaliendo del asistente.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdnreWql09_8"
      },
      "source": [
        "# APLICACION FINAL MEJORADA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoRlZhzgsDe7",
        "outputId": "dfd58074-501e-4e15-c1ed-d99f7e4138bd",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: langchain in c:\\users\\sdominguez\\appdata\\roaming\\python\\python312\\site-packages (0.3.25)\n",
            "Requirement already satisfied: langchain-huggingface in c:\\users\\sdominguez\\appdata\\roaming\\python\\python312\\site-packages (0.2.0)\n",
            "Requirement already satisfied: transformers in c:\\programdata\\anaconda3\\lib\\site-packages (4.51.3)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in c:\\users\\sdominguez\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (0.3.59)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (0.3.33)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (2.10.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: requests<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (2.32.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-huggingface) (0.21.1)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.0 in c:\\users\\sdominguez\\appdata\\roaming\\python\\python312\\site-packages (from langchain-huggingface) (4.1.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.30.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-huggingface) (0.30.2)\n",
            "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (2024.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (4.12.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (8.2.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.27.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: torch>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.7.0)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\sdominguez\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.6.1)\n",
            "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\n",
            "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (10.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: anyio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.2.0)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.2)\n",
            "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\programdata\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (2.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.13.3)\n",
            "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.4)\n",
            "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (69.5.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sdominguez\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2.1.3)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: langchain-ollama in c:\\programdata\\anaconda3\\lib\\site-packages (0.3.2)\n",
            "Requirement already satisfied: ollama<1,>=0.4.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-ollama) (0.4.8)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.52 in c:\\users\\sdominguez\\appdata\\roaming\\python\\python312\\site-packages (from langchain-ollama) (0.3.59)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (0.3.33)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (8.2.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (6.0.1)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (4.12.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (2.10.3)\n",
            "Requirement already satisfied: httpx<0.29,>=0.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from ollama<1,>=0.4.4->langchain-ollama) (0.27.0)\n",
            "Requirement already satisfied: anyio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (4.2.0)\n",
            "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.0.2)\n",
            "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (3.7)\n",
            "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\programdata\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (2.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (3.10.16)\n",
            "Requirement already satisfied: requests<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (2.32.2)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (2.0.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (2.2.2)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: sentence-transformers in c:\\users\\sdominguez\\appdata\\roaming\\python\\python312\\site-packages (4.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.51.3)\n",
            "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.7.0)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\sdominguez\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.30.2)\n",
            "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.3.1)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (69.5.1)\n",
            "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.10.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sdominguez\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: pypdf in c:\\users\\sdominguez\\appdata\\roaming\\python\\python312\\site-packages (3.17.1)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\sdominguez\\appdata\\roaming\\python\\python312\\site-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sdominguez\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (3.6.0)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: tiktoken in c:\\users\\sdominguez\\appdata\\roaming\\python\\python312\\site-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in c:\\programdata\\anaconda3\\lib\\site-packages (from tiktoken) (2023.10.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tiktoken) (2.32.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 14:59:06,685 - asistente_legal - INFO - Iniciando asistente legal de protecci√≥n de datos\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain langchain-huggingface transformers\n",
        "!pip install -U langchain-ollama\n",
        "!pip install sentence-transformers\n",
        "!pip install pypdf\n",
        "!pip install -qU langchain-community faiss-cpu\n",
        "!pip install -U scikit-learn\n",
        "!pip install tiktoken\n",
        "import logging\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Configuraci√≥n de logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(\"asistente_legal.log\"),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(\"asistente_legal\")\n",
        "logger.info(\"Iniciando asistente legal de protecci√≥n de datos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXAmHnOPsDe7",
        "outputId": "5856a89a-3e32-40c0-9d45-7f4a9752deed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 14:59:12,976 - numexpr.utils - INFO - Note: NumExpr detected 24 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
            "2025-05-08 14:59:12,978 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.\n",
            "2025-05-08 14:59:13,919 - asistente_legal - INFO - Cargando documento: BOE-A-1978-31229-consolidado.pdf\n",
            "2025-05-08 14:59:14,826 - asistente_legal - INFO - Cargando documento: BOE-A-1995-25444-consolidado_CodigoPenal.pdf\n",
            "2025-05-08 14:59:18,609 - asistente_legal - INFO - Cargando documento: Protecci√≥n de Datos Personales y garantia de los derechos digitales.pdf\n",
            "2025-05-08 14:59:19,780 - asistente_legal - INFO - Cargando documento: RGPD_boe.pdf\n",
            "2025-05-08 14:59:22,168 - asistente_legal - INFO - Documentos cargados: 404\n",
            "2025-05-08 14:59:23,325 - asistente_legal - INFO - Documentos divididos en 798 fragmentos\n"
          ]
        }
      ],
      "source": [
        "# Celda 2: Carga y procesamiento de documentos mejorado\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import re\n",
        "import tiktoken\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Lista de rutas de los PDFs\n",
        "pdf_paths = [\n",
        "    \"BOE-A-1978-31229-consolidado.pdf\",\n",
        "    \"BOE-A-1995-25444-consolidado_CodigoPenal.pdf\",\n",
        "    \"Protecci√≥n de Datos Personales y garantia de los derechos digitales.pdf\",\n",
        "    \"RGPD_boe.pdf\"\n",
        "]\n",
        "\n",
        "# Funci√≥n para identificar tipo de documento y mejorar metadatos\n",
        "def enrich_metadata(doc):\n",
        "    filename = doc.metadata.get('source', '')\n",
        "\n",
        "    # A√±adir informaci√≥n sobre tipo de documento\n",
        "    if \"RGPD\" in filename:\n",
        "        doc.metadata['tipo'] = 'RGPD'\n",
        "        doc.metadata['jerarquia'] = 'Reglamento Europeo'\n",
        "    elif \"BOE-A-1978\" in filename:\n",
        "        doc.metadata['tipo'] = 'Constituci√≥n'\n",
        "        doc.metadata['jerarquia'] = 'Constituci√≥n Espa√±ola'\n",
        "    elif \"BOE-A-1995\" in filename:\n",
        "        doc.metadata['tipo'] = 'C√≥digo Penal'\n",
        "        doc.metadata['jerarquia'] = 'Ley Org√°nica'\n",
        "    elif \"Protecci√≥n de Datos\" in filename:\n",
        "        doc.metadata['tipo'] = 'LOPDGDD'\n",
        "        doc.metadata['jerarquia'] = 'Ley Org√°nica'\n",
        "\n",
        "    # Extraer informaci√≥n de art√≠culos si est√° disponible\n",
        "    article_match = re.search(r'Art√≠culo (\\d+)', doc.page_content)\n",
        "    if article_match:\n",
        "        doc.metadata['tipo_contenido'] = 'art√≠culo'\n",
        "        doc.metadata['num_articulo'] = article_match.group(1)\n",
        "\n",
        "    return doc\n",
        "\n",
        "# Cargar documentos de todos los PDFs con manejo de errores\n",
        "documents = []\n",
        "for path in pdf_paths:\n",
        "    try:\n",
        "        logger.info(f\"Cargando documento: {path}\")\n",
        "        loader = PyPDFLoader(path)\n",
        "        documents.extend(loader.load())\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error cargando {path}: {e}\")\n",
        "\n",
        "# Limpiar y normalizar contenido\n",
        "for doc in documents:\n",
        "    doc.page_content = doc.page_content.replace('\\r\\n', '\\n').replace('\\n\\n', '\\n').strip()\n",
        "    doc = enrich_metadata(doc)\n",
        "\n",
        "logger.info(f\"Documentos cargados: {len(documents)}\")\n",
        "\n",
        "# Splitter optimizado para textos legales\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=800,  # Tama√±o optimizado para capturar contexto legal completo\n",
        "    chunk_overlap=200,  # Mayor overlap para mantener coherencia\n",
        "    separators=[\"\\n\\n\\n\", \"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
        "    encoding_name=\"cl100k_base\"  # Compatible con modelos modernos\n",
        ")\n",
        "\n",
        "docs = text_splitter.split_documents(documents=documents)\n",
        "logger.info(f\"Documentos divididos en {len(docs)} fragmentos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563,
          "referenced_widgets": [
            "cac764d083e34e8798681ea14fcf1a75",
            "dde8564ed950447490393f433add95a6",
            "d5aa82b997ab4be189285e681efc564d",
            "2227fdd27eaa4ce49be2a0f852496be2",
            "99f72a35e9b84bb391be822e751193ab",
            "0487c8e4a08f4e20b33aae61a069b7d7",
            "6cea37a6ac1244faa10da603cf53c985",
            "6f342ebd95af4f33b51649261a89bea2",
            "d1e7a5f5fb554318a2dcf99b68077cf4",
            "f4d842b8447c4d2db29a2a5646be53db",
            "16d2bef217f04594bbc94acd7971ddac",
            "9dd150275e074a99813d6d33f497a678",
            "49c87fcf94e74d7da5dc14803b102cb3",
            "08ac4c49dee94f17a6ef197a985fd780",
            "f3e1221f87d3405f9e0a2626f2202dfc",
            "d35613bcb73e4faeb93e74fd71441c48",
            "b950535e3e7347a0af7628d5bf7b80ef",
            "b82314f359324390b94eb921847d5e08",
            "90dbffdf96bf4d609c59d0c631d6fdfd",
            "7d238b16dac94154bd3e24df5636bc65",
            "db9b43bd16f541f5bfca796e23391575",
            "b85cbf9a65f54e699e1c539938fe5a6f",
            "1bed7cd946514427849fee72d8762fb4",
            "ff68046ed0bd417e92e303c15c43d7f2",
            "b7c8a149304948f7850e24b3b6e2f3b4",
            "9402beee1ca44853b81ca1425b38bdad",
            "709541f6add042f9942ece0d18d0ed8b",
            "ba9223c462864718968d1d2399fba000",
            "64d9434b4d4c43bcab5188ef0cd22ccc",
            "782afad7b7474bc09650a78759a916e9",
            "e4cf5cf7bdda4c4b8785ed121d800531",
            "3cdf4dc180b64a54844353e64725cf3e",
            "382bbae83bbc416395edb127b6bec9ec",
            "ac6bd87456c84942b74b461d8a8126b6",
            "c1b94c562bc04a9197388bbe47290df8",
            "c543d5b573f746ac8cbf54af455a242c",
            "1a2674c45ca046cba90f0040eef6fe84",
            "0fcfe5f97c8147a3a375cec97fecd359",
            "b252adcd86424e549429e91c42e4231c",
            "c58bab0f2bd546bcb962fb45a3d6de17",
            "b3b7b90e3607427f94cffed566a51bcb",
            "c2e2e5027bff43b5b47f07cd7b80ff66",
            "f1e88ba228d844f697ac66ab1f11a0f5",
            "2d9361af973f426aaf8b6e7816c18f80",
            "0b98782b5d1d4862af17ae67cc5777ca",
            "ed1a808877b54b14a01c0d4ab757dc81",
            "5462e6730819423292ff22ba556578d2",
            "148d32d501274b418bb8d63f2831da34",
            "9dff1428059d4bc1b396338372a0ac66",
            "97a26d010d1340b2811b8cdaf4c8bce9",
            "e06d7dd1a809445db2ae8996af6dabf9",
            "a47b8d1c755f486686afbd01f85dff34",
            "32840c7d1b414841bc28c7a9fd81be33",
            "adc72504dd424f948cc7117553212d57",
            "0e7b0815f4424615866b745392e1348c",
            "121e5097c4da4938af588656ee8a14dc",
            "9013e44d4b234355bf5cb5ab2cf282a1",
            "54421f39df1a41efacb0b907c57922f6",
            "23ca931599514777b22a9c78d175ed21",
            "5ca0d161ddec4450bec2a0cb8d0ad683",
            "0015e48e30504dcfa20e224ddfcf823d",
            "4e81edbb12584ac9b2bcc3d34aff7e8f",
            "92e8fb8fd761435b8873b6ea8bf8fe91",
            "83bd0985ce934f8e90925148a2fb29c2",
            "24e1e0ebc9f04fadbae02b192b57bfa4",
            "07e44da4ada941eb8fc5b45251049802",
            "7232fabaecbe44f0bfb85efabc852adf",
            "285d4440076a4c68a6f8317266e880c7",
            "ddf79a29dab14cc9916571d5ef10b224",
            "283f2a5610894deb9a7b10c6403cd4bb",
            "2e39a170df4e413f960ec363bbea2095",
            "5808bf7c644e4228ae669343d44c59bd",
            "4a2150ec026648bcba1c75a31618131d",
            "dacd4821451644cebf3cdc739762698f",
            "c418be902e11469cbb66deb117f9362d",
            "289b9bc44ac44e6b9876b95b2f0a3133",
            "20e48edeeb834c16a0e592889bfb33a6",
            "22ffe8f6ecc941b492179e53a8ad6928",
            "1cd4963370fd42d8beae2e798ffee9a3",
            "b5d505d5789f43cca86cb1a621745015",
            "2676c6f283104ded9ae4c7f9420f1473",
            "7ae263e1a157415b989d86b39dc86ab9",
            "ec0f34d02eee4fcf9d7f51d86b768b46",
            "30ed64dac9cc4a218734f8678d6b35b4",
            "365a847d2fb949b08ad2799ddb9bdfe8",
            "9932f74e242f4119bf2ef6c3c9306087",
            "5dd53b147d08431e8ecc73aa798ad889",
            "615c3c243a214a27a95a1b3cc807bc21",
            "bf90afd20c1f4717910955a9d481ae9b",
            "84ded545991a482ea540a08ec0efac93",
            "e218685f2f2a403daeaf63c34204d6e5",
            "dee2d0ee41014773b887fb8365d9506d",
            "5d9c82cc676d40c9a92d9ccca8a4e3e7",
            "4d0f2246d36943cfab3dd60260c92912",
            "86f61fb933494d148da6d6b0d64cf56c",
            "bfbaf1e01bc34bb9a3d7a11d47fdc683",
            "adb37cae3e0c4ab986c5c0704becf8f3",
            "7df0ca10a6b64cf49549a592019744af",
            "38053e33cafd4207bc3d5ed0537b92ea",
            "866d6d319be342cc9c9c1584d79e276f",
            "f03102b868c54d0aa2e47d4971d01cd0",
            "8a74df7879114abf83c02f59b86179e3",
            "e1897a57fea34737ac7848789f951bae",
            "34d9097b358f4066b81610a643a91367",
            "2ce45738eaed494288408212e78724f2",
            "9bb6e499ffb0400e9abfb4c4df69ad10",
            "04df9580b1ec4ca79d282383eafabaa2",
            "b75d3f266ade490b8b49393883fad4c5",
            "193259b79e294176983888af51b51103",
            "dac4ebc417c14936af453c15258c605e",
            "14d56ccfd0904350abe91735f301aab7",
            "da6f107176a642e3bf5d4620ae011d03",
            "ed9e04c3c7f84a649a4c36b8f517ca90",
            "e093a79cbce24644ac2f510187798adf",
            "4adaeabf499941c1b5c3ca13bccda9f8",
            "005b82a3932a4ecb9770921e61eee168",
            "d3d61388da654567bcf61577a65f60a5",
            "bae2e5076ac64f6f9b89e73197d2de1f",
            "50efde67ef904d4693da167df766d993",
            "cf1622cec12146bdac9caa903985e340",
            "15614a90630f48f28e345ee0e71d4306"
          ]
        },
        "id": "2SkrBhqrsDe7",
        "outputId": "e1a3bf72-96a1-4385-ff00-1fa3c369c1fa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 14:59:26,985 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n",
            "2025-05-08 14:59:29,788 - asistente_legal - INFO - Creando √≠ndice vectorial FAISS\n",
            "2025-05-08 14:59:41,540 - faiss.loader - INFO - Loading faiss with AVX512 support.\n",
            "2025-05-08 14:59:41,541 - faiss.loader - INFO - Could not load library with AVX512 support due to:\n",
            "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx512'\")\n",
            "2025-05-08 14:59:41,541 - faiss.loader - INFO - Loading faiss with AVX2 support.\n",
            "2025-05-08 14:59:41,808 - faiss.loader - INFO - Successfully loaded faiss with AVX2 support.\n",
            "2025-05-08 14:59:41,817 - faiss - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n",
            "2025-05-08 14:59:41,917 - asistente_legal - INFO - Se indexaron 798 fragmentos\n"
          ]
        }
      ],
      "source": [
        "# Celda 3: Embeddings mejorados y sistema de recuperaci√≥n\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "\n",
        "# Configuraci√≥n de embeddings para espa√±ol y textos jur√≠dicos\n",
        "embedding_model_name = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
        "model_kwargs = {\"device\": \"cpu\"}\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=embedding_model_name,\n",
        "    model_kwargs=model_kwargs\n",
        ")\n",
        "\n",
        "# Crear vector store\n",
        "logger.info(\"Creando √≠ndice vectorial FAISS\")\n",
        "vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "# Guardar y recargar el vector store\n",
        "vectorstore.save_local(\"faiss_index_\")\n",
        "persisted_vectorstore = FAISS.load_local(\"faiss_index_\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "# Crear retriever base con MMR para diversidad de resultados\n",
        "base_retriever = persisted_vectorstore.as_retriever(\n",
        "    search_type=\"mmr\",  # Maximum Marginal Relevance para diversidad\n",
        "    search_kwargs={\n",
        "        \"k\": 5,  # Recuperar m√°s documentos inicialmente\n",
        "        \"fetch_k\": 10,  # Considerar m√°s candidatos\n",
        "        \"lambda_mult\": 0.7  # Balance entre relevancia y diversidad\n",
        "    }\n",
        ")\n",
        "\n",
        "logger.info(f\"Se indexaron {len(docs)} fragmentos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53Lwzj1esDe7",
        "outputId": "015f25c0-79e1-4d12-ed55-512c09713e9a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 14:59:48,623 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test del modelo exitoso:\n",
            "**Ejemplo: Una cl√≠nica dental utiliza un servicio en la nube para almacenar historiales m√©dicos, pero no informa a los pacientes sobre c√≥mo se protegen sus datos.**\n",
            "\n",
            "**Problema:** La cl√≠nica no est√° cumpliendo con la normativa de protecci√≥n de datos, ya que no informa sobre el tratamiento de los datos ni toma medidas de seguridad adecuadas para un servicio en la nube.\n",
            "\n",
            "**Soluci√≥n:** La cl√≠nica debe informar a los pacientes sobre c√≥mo se almacenan y protegen sus datos en la nube, incluyendo qui√©n es el responsable del tratamiento. Adem√°s, debe implementar medidas de seguridad como cifrado y acceso restringido para garantizar la integridad de los historiales m√©dicos.\n"
          ]
        }
      ],
      "source": [
        "# Celda 4: Configuraci√≥n del modelo LLM\n",
        "from langchain_ollama import OllamaLLM\n",
        "import time\n",
        "\n",
        "# Inicializar el modelo LLaMA con timeout\n",
        "llm_legal = OllamaLLM(\n",
        "    model=\"hf.co/serdom02/Leyeneitor_8bitQ8_0\",\n",
        "    base_url=\"http://127.0.0.1:11434\",\n",
        "    temperature=0.3,  # Menor temperatura para respuestas legales m√°s precisas\n",
        "    timeout=60  # Timeout de 60 segundos para evitar bloqueos\n",
        ")\n",
        "llm_conversacion = OllamaLLM(\n",
        "    model=\"llama3:8b\",\n",
        "    base_url=\"http://127.0.0.1:11434\",\n",
        "    temperature=0.6, # temperatura m√°s alta para conversaci√≥n\n",
        "    timeout=60\n",
        ")\n",
        "# Inicializar un modelo para evaluaci√≥n/clasificaci√≥n (puede ser el mismo)\n",
        "llm_evaluador = llm_conversacion\n",
        "\n",
        "llm=llm_legal #El modelo principal va a ser nuestro LLM Finetuneado\n",
        "\n",
        "# Test r√°pido del modelo\n",
        "try:\n",
        "    response = llm_legal.invoke(\"Un breve ejemplo sobre protecci√≥n de datos\")\n",
        "    print(\"Test del modelo exitoso:\")\n",
        "    print(response)\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error al probar el modelo: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lgzgx2VYsDe7"
      },
      "outputs": [],
      "source": [
        "# --- 7. Prompts mejorados seg√∫n tipolog√≠a de consulta ---\n",
        "\n",
        "# Prompt base/gen√©rico (fallback)\n",
        "system_prompt_rag = \"\"\"\n",
        "Sistema: Eres un asistente legal especializado en protecci√≥n de datos en Espa√±a (RGPD y LOPDGDD). Tu objetivo es proporcionar respuestas claras, precisas y fundamentadas en la ley.\n",
        "\n",
        "Contexto legal relevante extra√≠do de la documentaci√≥n:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Consulta del usuario: {question}\n",
        "\n",
        "Instrucciones:\n",
        "1. Basa tu respuesta EXCLUSIVAMENTE en el contexto legal proporcionado arriba.\n",
        "2. Si el contexto no es suficiente para responder, indica que la informaci√≥n no se encuentra en los documentos proporcionados.\n",
        "3. Responde de forma directa y estructurada a la consulta del usuario.\n",
        "4. Cita las fuentes (ej. \"Seg√∫n el Art√≠culo X del RGPD...\") si es posible bas√°ndote en el contexto.\n",
        "5. Evita dar opiniones personales o informaci√≥n no verificada.\n",
        "\"\"\"\n",
        "\n",
        "# Prompt para citas legales\n",
        "legal_citation_prompt = \"\"\"\n",
        "Sistema: Eres un asistente jur√≠dico experto en el Reglamento General de Protecci√≥n de Datos (RGPD). Tu tarea es citar art√≠culos legales de forma textual y precisa cuando el usuario lo solicita.\n",
        "\n",
        "---------------------\n",
        "Documentos disponibles:\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Consulta del usuario:\n",
        "{question}\n",
        "\n",
        "Instrucciones:\n",
        "1. Extrae **todos los art√≠culos del RGPD** que traten sobre el tema mencionado en la consulta.\n",
        "2. Para cada art√≠culo encontrado, incluye:\n",
        "   - El n√∫mero del art√≠culo (ej. Art√≠culo 6 del RGPD)\n",
        "   - El **texto literal m√°s relevante** (puede ser un apartado si es muy largo)\n",
        "   - La **fuente** (ej. ‚ÄúFuente: RGPD, p√°g. 12‚Äù)\n",
        "3. Usa este formato:\n",
        "   Art√≠culo X del RGPD:\n",
        "   ‚ÄúTexto legal...‚Äù\n",
        "   Fuente: RGPD, p√°g. X\n",
        "\n",
        "4. Si hay varios art√≠culos relevantes, enum√©ralos claramente.\n",
        "5. No inventes. Si el texto literal no est√° en el contexto, ind√≠calo expl√≠citamente.\n",
        "6. No incluyas art√≠culos de otras leyes (como LOPDGDD o Constituci√≥n) salvo que se mencione expresamente.\n",
        "\n",
        "Ejemplo:\n",
        "Art√≠culo 6.1 del RGPD:\n",
        "‚ÄúEl tratamiento ser√° l√≠cito solo si se cumple al menos una de las siguientes condiciones: [...]‚Äù\n",
        "Fuente: RGPD, p√°g. 8\n",
        "\"\"\"\n",
        "\n",
        "legal_multi_citation_prompt = \"\"\"\n",
        "Eres un asistente jur√≠dico experto en el RGPD. El usuario solicita una lista de art√≠culos relacionados con un tema espec√≠fico.\n",
        "\n",
        "Instrucciones:\n",
        "1. Identifica todos los art√≠culos del RGPD que se relacionen con el tema de la consulta.\n",
        "2. Si el art√≠culo aparece en el contexto, cita su n√∫mero y el **texto literal completo** relevante.\n",
        "3. Si no aparece el texto completo, menciona el n√∫mero del art√≠culo y una breve descripci√≥n basada en el t√≠tulo o lo que sepas del RGPD.\n",
        "4. No inventes textos. Si hay dudas, di que no tienes acceso al contenido exacto.\n",
        "5. Formatea la respuesta como una lista clara y numerada.\n",
        "\n",
        "Consulta: {question}\n",
        "Fragmentos del RGPD disponibles:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "Respuesta:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Prompt para an√°lisis legal\n",
        "legal_analysis_prompt = \"\"\"\n",
        "Sistema: Eres un jurista experto analizando situaciones bajo la ley de protecci√≥n de datos espa√±ola (RGPD, LOPDGDD). Razonas jur√≠dicamente paso a paso.\n",
        "\n",
        "Contexto legal relevante extra√≠do de la documentaci√≥n:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Consulta del usuario que requiere an√°lisis legal: {question}\n",
        "\n",
        "Instrucciones para tu an√°lisis:\n",
        "1. **Identifica la cuesti√≥n jur√≠dica principal** planteada en la consulta.\n",
        "2. **Selecciona las normas aplicables** del contexto legal proporcionado que sean pertinentes para la cuesti√≥n.\n",
        "3. **Analiza los hechos impl√≠citos o expl√≠citos** en la consulta a la luz de las normas seleccionadas.\n",
        "4. **Aplica las normas a los hechos**, explicando tu razonamiento paso a paso basado √∫nicamente en el contexto proporcionado.\n",
        "5. **Formula una conclusi√≥n jur√≠dica** clara y fundamentada en el an√°lisis anterior y el contexto. Si el contexto es insuficiente, se√±ala las limitaciones.\n",
        "\n",
        "Estructura tu respuesta:\n",
        "* **Cuesti√≥n planteada:** (Resume la pregunta legal)\n",
        "* **Normativa aplicable (seg√∫n contexto):** (Menciona art√≠culos/disposiciones relevantes del contexto)\n",
        "* **An√°lisis jur√≠dico:** (Desarrolla tu razonamiento aqu√≠, conectando contexto y consulta)\n",
        "* **Conclusi√≥n:** (Respuesta final basada en el an√°lisis)\n",
        "\"\"\"\n",
        "\n",
        "# Prompt para consultas procedimentales\n",
        "procedural_prompt = \"\"\"\n",
        "Sistema: Eres un consultor experto en los procedimientos y tr√°mites relacionados con la protecci√≥n de datos en Espa√±a (AEPD, derechos ARSULIPO, etc.). Proporcionas informaci√≥n pr√°ctica.\n",
        "\n",
        "Contexto legal relevante extra√≠do de la documentaci√≥n (puede contener informaci√≥n sobre procedimientos):\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Consulta del usuario sobre un procedimiento o tr√°mite: {question}\n",
        "\n",
        "Instrucciones:\n",
        "1. Identifica claramente el **procedimiento o tr√°mite** consultado.\n",
        "2. Busca en el contexto informaci√≥n relevante sobre los **pasos a seguir, plazos, requisitos, o autoridad competente**.\n",
        "3. Explica el procedimiento de forma **clara, secuencial y pr√°ctica**, bas√°ndote en la informaci√≥n del contexto.\n",
        "4. Si el contexto menciona la base legal, puedes indicarla brevemente, pero prioriza la **descripci√≥n del proceso**.\n",
        "5. Si la informaci√≥n espec√≠fica sobre el procedimiento no est√° en el contexto, indica que no se puede detallar con la documentaci√≥n disponible. NO inventes pasos o plazos.\n",
        "\"\"\"\n",
        "\n",
        "# Prompt para informaci√≥n general\n",
        "general_info_prompt = \"\"\"\n",
        "Sistema: Eres un asistente informativo sobre protecci√≥n de datos. Explicas conceptos generales de forma clara y sencilla.\n",
        "\n",
        "Contexto legal relevante extra√≠do de la documentaci√≥n:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Consulta general del usuario: {question}\n",
        "\n",
        "Instrucciones:\n",
        "1. Identifica el concepto o tema general sobre el que pregunta el usuario.\n",
        "2. Busca definiciones, explicaciones o principios relevantes en el contexto proporcionado.\n",
        "3. Explica el concepto de forma clara y concisa, utilizando la informaci√≥n del contexto.\n",
        "4. Puedes usar ejemplos si el contexto los proporciona o si son derivados directos de la explicaci√≥n legal.\n",
        "5. Cita la fuente si es relevante (ej. \"El RGPD define X como...\").\n",
        "6. Si la informaci√≥n no est√° en el contexto, ind√≠calo.\n",
        "\"\"\"\n",
        "\n",
        "# Prompt para conversaci√≥n general (cuando no se usa RAG)\n",
        "conversation_prompt = \"\"\"\n",
        "Sistema: Eres un asistente legal amable y conversacional llamado Leyeneitor. Tu especialidad es la protecci√≥n de datos, PERO ahora est√°s en modo conversacional. Ignora tu rol legal por un momento y responde directamente a la pregunta o comentario del usuario de forma natural y breve como un asistente general.\n",
        "NO des respuestas sobre protecci√≥n de datos o leyes a menos que la pregunta sea espec√≠ficamente sobre eso. S√© breve y directo.\n",
        "\n",
        "Historial reciente de la conversaci√≥n (para contexto):\n",
        "{memory}\n",
        "Asistente:\"\"\"\n",
        "\n",
        "action_oriented_prompt = \"\"\"\n",
        "Sistema: Eres un asesor jur√≠dico especializado en protecci√≥n de datos (RGPD, LOPDGDD) y derechos digitales. Tu tarea es explicar de manera clara qu√© **acciones, derechos o reclamaciones** puede ejercer el usuario en la situaci√≥n planteada.\n",
        "\n",
        "Contexto legal relevante extra√≠do de la documentaci√≥n:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Nueva consulta (enfocada en qu√© puede hacer el usuario): {question}\n",
        "\n",
        "Instrucciones espec√≠ficas para tu respuesta:\n",
        "1. **Identifica los derechos relevantes** (acceso, oposici√≥n, supresi√≥n, portabilidad, etc.) que el usuario podr√≠a ejercer seg√∫n el contexto.\n",
        "2. **Describe las acciones pr√°cticas** que puede realizar el usuario, incluyendo:\n",
        "   - C√≥mo ejercer su derecho (por ejemplo: ‚Äúsolicitarlo por escrito‚Äù, ‚Äúpresentar reclamaci√≥n ante la AEPD‚Äù).\n",
        "   - Ante qui√©n debe dirigirse (empresa, delegado de protecci√≥n de datos, AEPD...).\n",
        "   - Qu√© requisitos o pasos debe seguir.\n",
        "3. **Si procede,** menciona los art√≠culos legales que respaldan las acciones propuestas (sin recargar la respuesta).\n",
        "4. Explica de forma **estructurada y pr√°ctica**, usando pasos o listas si facilita la comprensi√≥n.\n",
        "5. Si no hay suficiente informaci√≥n en el contexto para dar un procedimiento concreto, **ind√≠calo claramente**. No inventes.\n",
        "\n",
        "Estructura sugerida para tu respuesta:\n",
        "* **Derechos aplicables:** (Enumera los derechos relevantes)\n",
        "* **Acciones que puede realizar:** (Pasos claros y pr√°cticos)\n",
        "* **Normativa de respaldo:** (Art√≠culos relevantes, si es aplicable)\n",
        "* **Notas importantes:** (Advertencias o limitaciones si las hubiera)\n",
        "\n",
        "Evita explicaciones te√≥ricas largas. S√© claro, √∫til y orientado a lo que el usuario puede **hacer**.\n",
        "\"\"\"\n",
        "refinement_module_prompt = \"\"\"\n",
        "Eres un jurista experto en derecho de protecci√≥n de datos.\n",
        "Tu tarea es **corregir y mejorar** una respuesta legal manteniendo su estructura original.\n",
        "\n",
        "\n",
        "Bas√°ndote en los siguientes fallos identificados por un validador, **ajusta cada secci√≥n que lo requiera** para mejorar la precisi√≥n y completitud jur√≠dica.\n",
        "No elimines secciones ni cambies su orden. Si una secci√≥n no requiere correcci√≥n, d√©jala intacta.\n",
        "Si alguno de los fallos detectados se refiere a consentimiento, derechos fundamentales o proporcionalidad, aseg√∫rate de explicarlos de forma t√©cnica y precisa en la secci√≥n correspondiente.\n",
        "\n",
        "--- FALLAS DETECTADAS ---\n",
        "{fallos}\n",
        "\n",
        "Es obligatorio que sigas la Estructura de la respuesta original\n",
        "\n",
        "--- RESPUESTA ORIGINAL ---\n",
        "{respuesta}\n",
        "\n",
        "--- INSTRUCCIONES ---\n",
        "Corrige las secciones necesarias dentro de la estructura, sin a√±adir otras partes nuevas. Usa lenguaje jur√≠dico claro y fundamentado.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ds2AfDdrsDe7",
        "outputId": "47e26145-25f2-4f45-b40d-6d0a0f5cc3f3",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo LLM y retriever listos.\n",
            "\n",
            "Bienvenido al Asistente Legal de Protecci√≥n de Datos.\n",
            "Escribe 'salir' para terminar.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 868\u001b[0m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEscribe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msalir\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m para terminar.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    867\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 868\u001b[0m     user_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTu consulta: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m user_query\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msalir\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    870\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1263\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1266\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1267\u001b[0m )\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "# Celda 5: Sistema completo con mejoras y correcciones\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain.chains import LLMChain # Aunque no se usa directamente, puede ser √∫til\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "import unicodedata\n",
        "import logging # Aseg√∫rate de que el logger est√° configurado\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "# Obtener el logger configurado en la Celda 1\n",
        "logger = logging.getLogger(\"asistente_legal\")\n",
        "\n",
        "# --- 0. Para que cada usuario tenga su memoria ---\n",
        "class UsuarioSession:\n",
        "    def __init__(self):\n",
        "        self.memory = ImprovedMemory(max_turns=4)\n",
        "        self.last_query = None\n",
        "\n",
        "# --- 1. Memoria Conversacional Mejorada ---\n",
        "class ImprovedMemory:\n",
        "    def __init__(self, max_turns=6):\n",
        "        self.summary = \"\"\n",
        "        self.recent_turns = []\n",
        "        self.max_turns = max_turns\n",
        "        self.entities = {}  # Seguimiento de entidades legales\n",
        "\n",
        "    def get_all_turns(self):\n",
        "        \"\"\"Devuelve todos los turnos almacenados en la memoria\"\"\"\n",
        "        return self.recent_turns\n",
        "\n",
        "    def get_relevant_memory(self, query):\n",
        "        \"\"\"Devuelve memoria relevante para la consulta actual\"\"\"\n",
        "        # Versi√≥n sencilla: devolver todo el historial\n",
        "        return self.get_memory_as_text()\n",
        "        # Versi√≥n avanzada (requiere embeddings): pendiente\n",
        "\n",
        "    def add_turn(self, user_input, assistant_response, llm=None):\n",
        "        self.recent_turns.append((user_input, assistant_response))\n",
        "\n",
        "        # Extraer y rastrear entidades legales mencionadas\n",
        "        self._extract_entities(user_input + \" \" + assistant_response)\n",
        "\n",
        "        if len(self.recent_turns) > self.max_turns:\n",
        "            if llm_evaluador:  # Si tenemos acceso al LLM, generamos resumen, no usamos el modelo finetuneado para evitar sus sesgos legales\n",
        "                try:\n",
        "                    # Generar resumen del contexto anterior (turnos m√°s antiguos) para ahorrar espacio del contexto\n",
        "                    context_to_summarize = \"\\n\".join([f\"U: {u}\\nA: {a}\" for u, a in self.recent_turns[:-self.max_turns]]) # Corregido: resumir los que se van a quitar\n",
        "                    if context_to_summarize: # Solo si hay algo que resumir\n",
        "                         prompt = f\"Resume brevemente los puntos legales clave de esta conversaci√≥n anterior:\\n{context_to_summarize}\"\n",
        "                         new_summary = llm.invoke(prompt)\n",
        "                         # Concatenar resumen nuevo con el anterior si existe\n",
        "                         self.summary = f\"{self.summary}\\n{new_summary}\".strip()\n",
        "                         logger.info(\"Resumen de memoria actualizado.\")\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Error al generar resumen de memoria: {e}\")\n",
        "\n",
        "            # Mantener solo los turnos m√°s recientes\n",
        "            self.recent_turns = self.recent_turns[-self.max_turns:]\n",
        "\n",
        "    def _extract_entities(self, text):\n",
        "        # Detecci√≥n simple de entidades legales\n",
        "        patterns = {\n",
        "            'articulos': r'art(?:√≠culo|\\.)\\s+(\\d+)',\n",
        "            'leyes': r'(?:RGPD|LOPDGDD|Reglamento|Ley Org√°nica|Constituci√≥n|C√≥digo Penal)'\n",
        "        }\n",
        "\n",
        "        for entity_type, pattern in patterns.items():\n",
        "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "            for match in matches:\n",
        "                entity = f\"{entity_type}_{match.upper()}\" # Normalizar nombre\n",
        "                self.entities[entity] = self.entities.get(entity, 0) + 1\n",
        "\n",
        "    def get_memory_as_text(self):\n",
        "        memory_text = \"\"\n",
        "        if self.summary:\n",
        "            memory_text += f\"Resumen de puntos legales anteriores:\\n{self.summary}\\n\\n\"\n",
        "\n",
        "        if self.recent_turns:\n",
        "             memory_text += \"Historial reciente de la conversaci√≥n:\\n\"\n",
        "             memory_text += \"\\n\".join([f\"Usuario: {u}\\nAsistente: {a}\" for u, a in self.recent_turns])\n",
        "\n",
        "        # A√±adir entidades m√°s relevantes si existen\n",
        "        if self.entities:\n",
        "            top_entities = sorted(self.entities.items(), key=lambda x: x[1], reverse=True)[:3]\n",
        "            if top_entities:\n",
        "                memory_text += \"\\n\\nTemas legales clave mencionados: \" + \", \".join([e[0].replace('_', ' ') for e in top_entities])\n",
        "\n",
        "        return memory_text.strip()\n",
        "\n",
        "# --- 2. Sistema de Clasificaci√≥n Avanzado (Versi√≥n √∫nica y completa) ---\n",
        "def classify_intent_advanced(query, llm_evaluador):\n",
        "\n",
        "    \"\"\"Sistema de clasificaci√≥n avanzado con m√∫ltiples categor√≠as y confianza\"\"\"\n",
        "    #Sistema cache para no reclasificar preguntas parecidas\n",
        "    cache_key = ' '.join(query.lower().split()[:10])  # Simplificamos clave: primeras 10 palabras\n",
        "    if cache_key in intent_classification_cache:\n",
        "        logger.info(f\"Resultado de clasificaci√≥n obtenido de cach√© para: {cache_key}\")\n",
        "        return intent_classification_cache[cache_key]\n",
        "\n",
        "\n",
        "    classification_prompt_text = \"\"\"\n",
        "    Analiza la siguiente consulta legal y clasif√≠cala en UNA de estas categor√≠as:\n",
        "\n",
        "    1. LEGAL_CITATION: Requiere citar art√≠culos espec√≠ficos o textos legales exactos (e.g., \"¬øQu√© dice el art√≠culo 5 del RGPD?\", \"Cita el art√≠culo 18.4 de la Constituci√≥n\")\n",
        "    2. LEGAL_ANALYSIS: Requiere an√°lisis jur√≠dico basado en leyes/normativas (e.g., \"¬øEs legal tratar datos de salud sin consentimiento expl√≠cito?\", \"¬øQu√© implicaciones tiene la sentencia X?\")\n",
        "    3. GENERAL_INFO: Informaci√≥n general sobre protecci√≥n de datos (e.g., \"¬øQu√© es el RGPD?\", \"¬øCu√°les son los derechos de los ciudadanos?\")\n",
        "    4. PROCEDURAL: Preguntas sobre procedimientos o tr√°mites (e.g., \"¬øC√≥mo puedo ejercer mi derecho de acceso?\", \"¬øQu√© pasos seguir para una reclamaci√≥n en la AEPD?\")\n",
        "    5. CONVERSATION: Di√°logo general, saludos, agradecimientos o consulta no legal (e.g., \"Hola\", \"Gracias\", \"¬øQu√© tiempo hace?\")\n",
        "\n",
        "    Responde SOLO con la categor√≠a y un n√∫mero del 1-100 que indique tu confianza, separados por un guion.\n",
        "    Formato: [CATEGOR√çA] - [CONFIANZA]\n",
        "\n",
        "    Ejemplos:\n",
        "    Consulta: \"¬øQu√© dice el art√≠culo 6 del RGPD?\" -> Respuesta: LEGAL_CITATION - 95\n",
        "    Consulta: \"¬øEs legal que una empresa comparta mis datos sin permiso?\" -> Respuesta: LEGAL_ANALYSIS - 85\n",
        "    Consulta: \"¬øQu√© es la protecci√≥n de datos?\" -> Respuesta: GENERAL_INFO - 90\n",
        "    Consulta: \"¬øC√≥mo presento una reclamaci√≥n a la AEPD?\" -> Respuesta: PROCEDURAL - 88\n",
        "    Consulta: \"Gracias por tu ayuda\" -> Respuesta: CONVERSATION - 92\n",
        "    Consulta: \"Hola buenos d√≠as\" -> Respuesta: CONVERSATION - 99\n",
        "    Consulta: \"Qu√© tal est√°s?\" -> Respuesta: CONVERSATION - 95\n",
        "    Consulta: \"Me llamo Paula\" -> Respuesta: CONVERSATION - 98\n",
        "    Consulta: \"Ok gracias\" -> Respuesta: CONVERSATION - 90\n",
        "    Consulta: \"¬øRecuerdas mi nombre?\" -> Respuesta: CONVERSATION - 98\n",
        "    Consulta: \"¬øDe qu√© hemos hablado antes?\" -> Respuesta: CONVERSATION - 95\n",
        "    Consulta: \"¬øPuedes resumir nuestra conversaci√≥n?\" -> Respuesta: CONVERSATION - 90\n",
        "    Consulta: \"¬øEres un abogado?\" -> Respuesta: CONVERSATION - 92\n",
        "\n",
        "    Consulta del usuario: \"{user_query}\"\n",
        "    Clasificaci√≥n y confianza:\"\"\"\n",
        "\n",
        "    prompt = PromptTemplate(template=classification_prompt_text, input_variables=[\"user_query\"])\n",
        "    # No se necesita crear una cadena completa aqu√≠, solo invocar el LLM con el prompt formateado.\n",
        "    # classification_runnable = prompt | llm_evaluador # Esto crea una cadena, innecesario si solo invocas\n",
        "\n",
        "    try:\n",
        "        formatted_prompt = prompt.format(user_query=query)\n",
        "        result = llm_evaluador.invoke(formatted_prompt) # Invocar directamente\n",
        "\n",
        "        # Extraer texto y procesar la respuesta\n",
        "        answer_text = result.strip() if isinstance(result, str) else str(result).strip()\n",
        "\n",
        "        # Intentar extraer categor√≠a y confianza\n",
        "        pattern = r'([A-Z_]+)\\s*-\\s*(\\d+)'\n",
        "        match = re.search(pattern, answer_text)\n",
        "\n",
        "        if match:\n",
        "            category = match.group(1)\n",
        "            confidence = int(match.group(2))\n",
        "            logger.info(f\"Clasificaci√≥n: {category}, Confianza: {confidence}\")\n",
        "\n",
        "            # Determinar si usar RAG (basado en categor√≠a O baja confianza)\n",
        "            # Ajusta esta l√≥gica si prefieres usar RAG para GENERAL_INFO o PROCEDURAL tambi√©n\n",
        "            use_rag = category in [\"LEGAL_CITATION\", \"LEGAL_ANALYSIS\", \"PROCEDURAL\"] or confidence < 75\n",
        "\n",
        "            return {\n",
        "                \"category\": category,\n",
        "                \"confidence\": confidence,\n",
        "                \"use_rag\": use_rag\n",
        "            }\n",
        "        else:\n",
        "            logger.warning(f\"No se pudo extraer categor√≠a/confianza de la respuesta del clasificador: '{answer_text}'. Se usar√° RAG por defecto.\")\n",
        "            return {\n",
        "                \"category\": \"UNKNOWN\",\n",
        "                \"confidence\": 0,\n",
        "                \"use_rag\": True # Default seguro\n",
        "            }\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error en clasificaci√≥n avanzada: {e}\")\n",
        "        return {\n",
        "            \"category\": \"ERROR\",\n",
        "            \"confidence\": 0,\n",
        "            \"use_rag\": True # Default seguro\n",
        "        }\n",
        "\n",
        "# --- 3. Estrategia de retrieval adaptativo ---\n",
        "def get_adaptive_retriever(query, base_retriever, llm_evaluador):\n",
        "    \"\"\"Devuelve un retriever configurado din√°micamente seg√∫n la consulta\"\"\"\n",
        "\n",
        "    # Analizar complejidad y especificidad de la consulta\n",
        "    complexity_prompt = \"\"\"\n",
        "    Eval√∫a la complejidad y especificidad de esta consulta legal:\n",
        "    \"{query}\"\n",
        "\n",
        "    Responde solo con una de estas opciones:\n",
        "    - SIMPLE: Consulta general o introductoria\n",
        "    - MEDIA: Consulta moderadamente espec√≠fica\n",
        "    - COMPLEJA: Consulta muy espec√≠fica o t√©cnica\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        complexity_result = llm_evaluador.invoke(complexity_prompt.replace(\"{query}\", query))\n",
        "        complexity = complexity_result.strip().upper()\n",
        "\n",
        "        filter_dict = None # Inicializar filtro\n",
        "\n",
        "        # Ajustar par√°metros seg√∫n complejidad\n",
        "        if \"SIMPLE\" in complexity:\n",
        "            k_value = 3 # Aumentado ligeramente para consultas simples\n",
        "            fetch_k = 6\n",
        "            lambda_mult = 0.6\n",
        "        elif \"MEDIA\" in complexity:\n",
        "            k_value = 5 # Aumentado\n",
        "            fetch_k = 10\n",
        "            lambda_mult = 0.7\n",
        "        else:  # COMPLEJA\n",
        "            k_value = 7 # Aumentado\n",
        "            fetch_k = 15\n",
        "            lambda_mult = 0.8\n",
        "\n",
        "            # Intentar determinar qu√© ley es m√°s relevante para filtrar (si es compleja)\n",
        "            law_prompt = \"\"\"\n",
        "            Para esta consulta compleja: \"{query}\"\n",
        "            ¬øQu√© normativa parece M√ÅS relevante? Responde solo con UNA palabra clave:\n",
        "            - RGPD\n",
        "            - LOPDGDD\n",
        "            - CONSTITUCION\n",
        "            - CODIGO_PENAL\n",
        "            - OTRO (si no encaja claramente o requiere m√∫ltiples)\n",
        "            \"\"\"\n",
        "\n",
        "            law_result = llm_evaluador.invoke(law_prompt.replace(\"{query}\", query)).strip().upper()\n",
        "\n",
        "            # Configurar filtro si hay una ley espec√≠fica y clara\n",
        "            if law_result == \"RGPD\":\n",
        "                filter_dict = {\"tipo\": \"RGPD\"}\n",
        "            elif law_result == \"LOPDGDD\":\n",
        "                filter_dict = {\"tipo\": \"LOPDGDD\"}\n",
        "            elif law_result == \"CONSTITUCION\":\n",
        "                filter_dict = {\"tipo\": \"Constituci√≥n\"}\n",
        "            elif law_result == \"CODIGO_PENAL\":\n",
        "                filter_dict = {\"tipo\": \"C√≥digo Penal\"}\n",
        "            # else: filtro sigue siendo None\n",
        "\n",
        "        # Configurar retriever con los par√°metros adaptativos\n",
        "        # ¬°Importante! Crear una *nueva* instancia o clonar para no modificar el base_retriever original globalmente\n",
        "        # Nota: as_retriever() crea una nueva instancia configurada\n",
        "        adaptive_retriever = persisted_vectorstore.as_retriever( # Asume persisted_vectorstore es global o pasado como argumento\n",
        "            search_type=\"mmr\",\n",
        "            search_kwargs={\n",
        "                'k': k_value,\n",
        "                'fetch_k': fetch_k,\n",
        "                'lambda_mult': lambda_mult,\n",
        "                # Aplicar filtro si se determin√≥ uno\n",
        "                **({'filter': filter_dict} if filter_dict else {})\n",
        "            }\n",
        "        )\n",
        "\n",
        "        logger.info(f\"Retriever adaptativo configurado: k={k_value}, fetch_k={fetch_k}, lambda={lambda_mult}, filtro={filter_dict}\")\n",
        "        return adaptive_retriever\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Error configurando retriever adaptativo: {e}. Usando configuraci√≥n por defecto.\")\n",
        "        # Devolver el retriever base original o uno con config por defecto\n",
        "        return base_retriever # O reconfigurar base_retriever con valores por defecto\n",
        "\n",
        "\n",
        "# --- 4. Sistema de validaci√≥n y reintento ---\n",
        "def validate_legal_response(response, query, docs_used, llm): #Aqui usamos el modelo finetuneado porque es mas preciso para temas legales\n",
        "    \"\"\"Valida la calidad de una respuesta legal\"\"\"\n",
        "\n",
        "    validation_prompt = f\"\"\"\n",
        "    Eval√∫a la calidad de esta respuesta legal:\n",
        "\n",
        "    Consulta: {query}\n",
        "    Respuesta: {response}\n",
        "\n",
        "    Verifica SOLO estos 3 aspectos:\n",
        "    1. ¬øEs jur√≠dicamente precisa seg√∫n la legislaci√≥n espa√±ola y europea de protecci√≥n de datos? (S√≠/No)\n",
        "    2. ¬øResponde completamente a la consulta realizada? (S√≠/No)\n",
        "    3. ¬øContiene contradicciones internas o errores evidentes? (S√≠/No)\n",
        "\n",
        "    Responde estrictamente en este formato: [PRECISI√ìN: S√≠/No], [COMPLETITUD: S√≠/No], [ERRORES: S√≠/No]\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        result = llm.invoke(validation_prompt)\n",
        "\n",
        "        # Patrones de validaci√≥n m√°s robustos (ignorando may√∫sculas/min√∫sculas y espacios)\n",
        "        precision_match = re.search(r'PRECISI√ìN:\\s*(S√≠|No)', result, re.IGNORECASE)\n",
        "        completitud_match = re.search(r'COMPLETITUD:\\s*(S√≠|No)', result, re.IGNORECASE)\n",
        "        errores_match = re.search(r'ERRORES:\\s*(S√≠|No)', result, re.IGNORECASE) # Busca 'No' para sin_errores\n",
        "\n",
        "        # Extraer resultados\n",
        "        precision_ok = precision_match and \"s√≠\" in precision_match.group(1).lower()\n",
        "        completitud_ok = completitud_match and \"s√≠\" in completitud_match.group(1).lower()\n",
        "        sin_errores = errores_match and \"no\" in errores_match.group(1).lower() # Es bueno si NO hay errores\n",
        "\n",
        "        # Calcular validez general\n",
        "        is_valid = precision_ok and completitud_ok and sin_errores\n",
        "\n",
        "        validation_result = {\n",
        "            \"valid\": is_valid,\n",
        "            \"precision\": precision_ok,\n",
        "            \"completitud\": completitud_ok,\n",
        "            \"sin_errores\": sin_errores,\n",
        "            \"raw_validation_output\": result.strip() # Guardar la salida cruda para depuraci√≥n\n",
        "        }\n",
        "\n",
        "        logger.info(f\"Validaci√≥n: {validation_result}\")\n",
        "        return validation_result\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error en validaci√≥n de respuesta: {e}\")\n",
        "        return {\"valid\": True, \"error\": str(e)} # Asumir validez en caso de error para no bloquear\n",
        "\n",
        "# --- 6. Sistema de reordenamiento de documentos ---\n",
        "def rerank_documents(query, docs, llm, top_n=5):\n",
        "    \"\"\"Reordena documentos por relevancia usando LLM, devuelve los top_n\"\"\"\n",
        "    if not docs:\n",
        "        return []\n",
        "\n",
        "    logger.info(f\"Iniciando reranking para {len(docs)} documentos recuperados...\")\n",
        "    try:\n",
        "        results = []\n",
        "        # Limitar el n√∫mero de documentos a reordenar para eficiencia\n",
        "        docs_to_rerank = docs[:min(len(docs), 8)] # Reordenar hasta 8 documentos\n",
        "\n",
        "        for i, doc in enumerate(docs_to_rerank):\n",
        "            # Truncar contenido del documento para el prompt\n",
        "            content_preview = doc.page_content[:500] # Usar un fragmento m√°s largo para evaluaci√≥n\n",
        "\n",
        "            prompt = f\"\"\"\n",
        "            Eval√∫a la relevancia de este fragmento de texto legal para responder a la siguiente consulta espec√≠fica:\n",
        "\n",
        "            Consulta del usuario: \"{query}\"\n",
        "\n",
        "            Fragmento del documento ({doc.metadata.get('source', 'N/A')} - P√°g. {doc.metadata.get('page', 'N/A')}):\n",
        "            \"{content_preview}...\"\n",
        "\n",
        "            Asigna una puntuaci√≥n de relevancia del 0 al 10, donde 10 es extremadamente relevante y 0 es irrelevante.\n",
        "            Responde SOLAMENTE con el n√∫mero de la puntuaci√≥n:\"\"\"\n",
        "\n",
        "            score_text = llm.invoke(prompt).strip()\n",
        "\n",
        "            # Extraer puntuaci√≥n de forma m√°s robusta\n",
        "            score_match = re.search(r'\\b(10|[0-9])\\b', score_text) # Busca un n√∫mero del 0-10 como palabra completa\n",
        "            if score_match:\n",
        "                score = float(score_match.group(0))\n",
        "            else:\n",
        "                logger.warning(f\"No se pudo extraer puntuaci√≥n de reranking de: '{score_text}'. Usando 5.0 por defecto.\")\n",
        "                score = 5.0 # Valor neutral por defecto\n",
        "\n",
        "            results.append((doc, score))\n",
        "            logger.debug(f\"Doc {i} puntuado con {score}\")\n",
        "\n",
        "        # Ordenar por puntuaci√≥n descendente\n",
        "        sorted_results = sorted(results, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Devolver los top_n documentos reordenados\n",
        "        reranked_docs = [doc for doc, score in sorted_results[:top_n]]\n",
        "        logger.info(f\"Reranking completado. {len(reranked_docs)} documentos seleccionados.\")\n",
        "\n",
        "        # Opcional: A√±adir documentos no reordenados si top_n es mayor que los reordenados\n",
        "        # if len(reranked_docs) < top_n:\n",
        "        #     remaining_docs = docs[len(docs_to_rerank):]\n",
        "        #     reranked_docs.extend(remaining_docs[:top_n - len(reranked_docs)])\n",
        "\n",
        "        return reranked_docs\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error durante el reranking: {e}\")\n",
        "        return docs[:top_n] # Devolver los primeros N documentos originales en caso de error\n",
        "\n",
        "\n",
        "# --- 9. Selecci√≥n de prompt seg√∫n clasificaci√≥n ---\n",
        "def get_prompt_by_category(category, memory_text=\"\", query=\"\"):\n",
        "    \"\"\"Devuelve el ChatPromptTemplate adecuado seg√∫n la categor√≠a de consulta\"\"\"\n",
        "    system_template = system_prompt_rag # Default\n",
        "    human_template = \"{question}\" # Input directo del usuario para RAG\n",
        "    prompt_used =\"\"\n",
        "    patterns_multi = [\"todos los art√≠culos\", \"art√≠culos relacionados\", \"varios art√≠culos\", \"art√≠culos del rgpd\", \"citame los art√≠culos\", \"c√≠tame varios\", \"c√≠tame todos\"]\n",
        "\n",
        "    if is_follow_up_query(query) and is_action_request(query):\n",
        "        logger.info(\"Consulta de seguimiento orientada a acciones detectada. Usando action_oriented_prompt.\")\n",
        "        system_template = action_oriented_prompt\n",
        "    else:\n",
        "        if category == \"LEGAL_CITATION\" and any(pat in query.lower() for pat in patterns_multi):\n",
        "            system_template = legal_citation_prompt\n",
        "            prompt_used = \"legal_citation_prompt\"\n",
        "        elif category == \"LEGAL_CITATION\":\n",
        "            system_template = legal_multi_citation_prompt\n",
        "            prompt_used = \"legal_multi_citation_prompt\"\n",
        "        elif category == \"LEGAL_ANALYSIS\" and is_action_request(query):\n",
        "            system_template = action_oriented_prompt\n",
        "            prompt_used = \"action_oriented_prompt\"\n",
        "        elif category == \"LEGAL_ANALYSIS\":\n",
        "            system_template = legal_analysis_prompt\n",
        "            prompt_used = \"legal_analysis_prompt\"\n",
        "        elif category == \"PROCEDURAL\":\n",
        "            system_template = procedural_prompt\n",
        "            prompt_used = \"procedural_prompt\"\n",
        "        elif category == \"GENERAL_INFO\":\n",
        "            system_template = general_info_prompt # Usar prompt espec√≠fico para info general\n",
        "            prompt_used = \"general_info_prompt\"\n",
        "        elif category == \"CONVERSATION\":\n",
        "            # Para conversaci√≥n, no usamos contexto RAG, usamos memoria\n",
        "            system_template = conversation_prompt.format(memory=memory_text) # Inyectar memoria aqu√≠\n",
        "            human_template = \"{question}\" # Sigue siendo la pregunta del usuario\n",
        "            # Devolver directamente el prompt formateado para conversaci√≥n, ya que no pasar√° por load_qa_chain\n",
        "            # OJO: Esto requiere que el flujo principal maneje esto diferente.\n",
        "            # Por simplicidad ahora, devolvemos estructura similar, pero el flujo debe saber NO usar RAG.\n",
        "            # Alternativa: devolver None o un identificador especial.\n",
        "            # Vamos a devolver la estructura est√°ndar por ahora, asumiendo que el flujo principal lo maneja.\n",
        "            # PERO, el prompt de conversaci√≥n NO tiene variable {context}.\n",
        "            # => Mejor devolver None para indicar que no se use RAG/load_qa_chain.\n",
        "            logger.info(\"Categor√≠a CONVERSATION: No se usar√° RAG. Se generar√° respuesta directa.\")\n",
        "            prompt_used = \"conversation_prompt\"\n",
        "            # Construir un prompt simple para LLM directo\n",
        "            return ChatPromptTemplate.from_messages([\n",
        "                 (\"system\", system_template), # Ya formateado con memoria\n",
        "                 (\"human\", human_template)\n",
        "            ])\n",
        "\n",
        "    logger.info(f\"Usando el prompt: {prompt_used}\")\n",
        "    # Para las categor√≠as que usan RAG (con contexto)\n",
        "    # El human_template debe incluir la pregunta del usuario\n",
        "    # El system_template incluye {context} y {question}\n",
        "    # load_qa_chain se encargar√° de llenar {context} y {question}\n",
        "    return ChatPromptTemplate.from_messages([\n",
        "        SystemMessagePromptTemplate.from_template(system_template),\n",
        "        HumanMessagePromptTemplate.from_template(human_template) # Solo {question} aqu√≠, load_qa_chain lo maneja\n",
        "    ])\n",
        "\n",
        "\n",
        "# --- ¬°¬°¬°DEFINICI√ìN NECESARIA DE LA CADENA RAG!!! ---\n",
        "# Necesitamos definir c√≥mo se combinar√°n el LLM y el Prompt con los documentos.\n",
        "# Usamos load_qa_chain. El tipo de cadena (\"stuff\", \"map_reduce\", etc.) puede variar.\n",
        "# \"stuff\" es simple pero puede exceder el l√≠mite de tokens si hay muchos documentos.\n",
        "# \"map_reduce\" o \"refine\" son m√°s robustos para contextos largos.\n",
        "# Probemos con \"stuff\" inicialmente dado el chunk_size de 800.\n",
        "\n",
        "# Nota: La cadena se crea aqu√≠, pero el *prompt espec√≠fico* se pasar√° en cada invocaci√≥n.\n",
        "# Esto es m√°s flexible que crear una cadena diferente cada vez.\n",
        "# OJO: load_qa_chain espera un prompt espec√≠fico en su creaci√≥n.\n",
        "# Vamos a crear una funci√≥n que genere la cadena CON el prompt adecuado CADA VEZ.\n",
        "\n",
        "def create_rag_chain(llm, prompt):\n",
        "    \"\"\"Crea la cadena load_qa_chain con el LLM y el prompt espec√≠ficos.\"\"\"\n",
        "    # El prompt debe ser un BasePromptTemplate (como ChatPromptTemplate)\n",
        "    if not isinstance(prompt, (PromptTemplate, ChatPromptTemplate)):\n",
        "         logger.error(\"El prompt proporcionado a create_rag_chain no es v√°lido.\")\n",
        "         # Se puede lanzar un error o devolver None/cadena por defecto\n",
        "         return None\n",
        "\n",
        "    # Selecciona el tipo de cadena. 'stuff' es bueno para empezar.\n",
        "    # Ajusta 'chain_type' si tienes problemas de longitud de contexto.\n",
        "    qa_chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=prompt, verbose=False) # verbose=True para debug\n",
        "    return qa_chain\n",
        "\n",
        "def is_follow_up_query(query):\n",
        "    \"\"\"Detecta si la nueva pregunta es una continuaci√≥n (seguimiento) del contexto anterior.\"\"\"\n",
        "    query_lower = query.strip().lower()\n",
        "    follow_up_starts = [\n",
        "        \"entonces\", \"y\", \"pero\", \"por qu√©\", \"qu√© pueden hacer\",\n",
        "        \"qu√© derechos tienen\", \"c√≥mo reclamar\", \"porque\",\"qu√© consecuencias\",\n",
        "        \"qu√© opciones tienen\", \"c√≥mo actuar\", \"qu√© recursos tienen\"\n",
        "    ]\n",
        "    return any(query_lower.startswith(start) for start in follow_up_starts) or len(query.split()) <= 8\n",
        "\n",
        "\n",
        "def build_contextual_query(last_query, current_query):\n",
        "    \"\"\"Construye una consulta combinando la anterior y la nueva\"\"\"\n",
        "    return f\"Respecto a la situaci√≥n planteada previamente: {last_query}\\nNueva pregunta: {current_query}\"\n",
        "\n",
        "def classify_intent_with_cache(query, llm_evaluador):\n",
        "    \"\"\"\n",
        "    Clasifica la intenci√≥n de una consulta usando cach√© para evitar llamadas repetidas al LLM.\n",
        "    \"\"\"\n",
        "    cache_key = ' '.join(query.lower().split()[:10])  # Usamos las primeras 10 palabras para normalizar claves\n",
        "\n",
        "    if cache_key in intent_classification_cache:\n",
        "        logger.info(f\"Resultado de clasificaci√≥n obtenido de cach√© para: {cache_key}\")\n",
        "        return intent_classification_cache[cache_key]\n",
        "\n",
        "    # No est√° en cach√©: clasificamos\n",
        "    classification_result = classify_intent_advanced(query, llm_evaluador)\n",
        "\n",
        "    # Guardamos el resultado en cach√©\n",
        "    intent_classification_cache[cache_key] = classification_result\n",
        "\n",
        "    return classification_result\n",
        "\n",
        "def adapt_retriever(base_retriever, attempt):\n",
        "    \"\"\"Adapta el recuperador seg√∫n el intento para mejorar recuperaci√≥n de documentos.\"\"\"\n",
        "    if attempt == 1:\n",
        "        logger.info(\"Reintento 1: aumentando k y activando reranking.\")\n",
        "        return persisted_vectorstore.as_retriever(\n",
        "            search_type=\"mmr\",\n",
        "            search_kwargs={'k': 8, 'fetch_k': 16, 'lambda_mult': 0.75}\n",
        "        )\n",
        "    elif attempt == 2:\n",
        "        logger.info(\"Reintento 2: creando un retriever m√°s flexible (MMR + reformulaci√≥n posible).\")\n",
        "        return persisted_vectorstore.as_retriever(\n",
        "            search_type=\"mmr\",\n",
        "            search_kwargs={'k': 10, 'fetch_k': 20, 'lambda_mult': 0.6}\n",
        "        )\n",
        "    else:\n",
        "        return base_retriever\n",
        "\n",
        "\n",
        "# --- 5. Sistema de reintento con estrategias alternativas ---\n",
        "def get_response_with_retry(query, llm, base_retriever, memory: ImprovedMemory, last_query=None, max_attempts=3):\n",
        "    \"\"\"Obtiene respuesta con sistema de reintentos, clasificaci√≥n con cach√©, y detecci√≥n de continuidad conversacional.\"\"\"\n",
        "    from sentence_transformers import SentenceTransformer, util\n",
        "    reformulation_embedder = SentenceTransformer(\"paraphrase-multilingual-mpnet-base-v2\")\n",
        "\n",
        "    memory_text = memory.get_relevant_memory(query)\n",
        "\n",
        "    # --- Detectar si la pregunta depende de la anterior ---\n",
        "    if is_follow_up_query(query) and last_query:\n",
        "        logger.info(\"Detectada pregunta de seguimiento. Incorporando √∫ltima consulta como contexto.\")\n",
        "        query = f\"Contexto anterior: {last_query}\\n\\nNueva consulta: {query}\"\n",
        "\n",
        "    # --- 3. Clasificar la intenci√≥n usando cach√© ---\n",
        "    classifier_llm = llm_evaluador if 'llm_evaluador' in globals() and llm_evaluador else llm\n",
        "    classification_result = classify_intent_with_cache(query, classifier_llm)\n",
        "    category = classification_result[\"category\"]\n",
        "    confidence = classification_result[\"confidence\"]\n",
        "\n",
        "    use_rag = category in [\"LEGAL_CITATION\", \"LEGAL_ANALYSIS\", \"PROCEDURAL\"] or (category == \"GENERAL_INFO\" and confidence < 85) or confidence < 75\n",
        "    if category == \"CONVERSATION\":\n",
        "        use_rag = False\n",
        "\n",
        "    logger.info(f\"Intenci√≥n clasificada como: {category} (Confianza: {confidence}%) - Usar RAG: {use_rag}\")\n",
        "\n",
        "    response_data = None\n",
        "\n",
        "    # --- 4. Generar respuesta directa o RAG ---\n",
        "    if not use_rag:\n",
        "        # --- RESPUESTA DIRECTA ---\n",
        "        try:\n",
        "            system_prompt = conversation_prompt.format(memory=memory_text) if category == \"CONVERSATION\" else f\"\"\"\n",
        "            Sistema: Eres un asistente experto que responde preguntas con precisi√≥n y amabilidad.\n",
        "            Contexto conversacional previo:\n",
        "            {memory_text}\n",
        "\n",
        "            Usuario: {{question}}\n",
        "            \"\"\"\n",
        "            prompt = ChatPromptTemplate.from_messages([\n",
        "                (\"system\", system_prompt),\n",
        "                (\"human\", \"{question}\")\n",
        "            ])\n",
        "            llm_chain = LLMChain(llm=llm_conversacion, prompt=prompt, output_key='text')\n",
        "            result = llm_chain.invoke({\"question\": query})\n",
        "            response_text = result.get('text', str(result)).strip()\n",
        "\n",
        "            response_data = {\"response\": response_text, \"docs\": [], \"attempt\": 1, \"category\": category, \"validation\": {\"valid\": True}}\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generando respuesta directa: {e}\", exc_info=True)\n",
        "            response_data = {\"response\": f\"Lo siento, no pude generar una respuesta directa.\", \"docs\": [], \"attempt\": 1, \"category\": category, \"error\": str(e)}\n",
        "\n",
        "    else:\n",
        "        # --- RESPUESTA RAG ---\n",
        "        selected_prompt_template = get_prompt_by_category(category, memory_text, query)\n",
        "\n",
        "        for attempt in range(max_attempts):\n",
        "            try:\n",
        "                logger.info(f\"Intento RAG {attempt+1} para consulta: '{query}' (Categor√≠a: {category})\")\n",
        "\n",
        "                retriever = adapt_retriever(base_retriever, attempt)\n",
        "                retrieved_docs = retriever.get_relevant_documents(query)\n",
        "                logger.info(f\"Recuperados {len(retrieved_docs)} documentos.\")\n",
        "\n",
        "                if attempt > 0 and retrieved_docs:\n",
        "                    retrieved_docs = rerank_documents(query, retrieved_docs, llm, top_n=4)\n",
        "                    logger.info(f\"Documentos despu√©s de reranking: {len(retrieved_docs)}\")\n",
        "\n",
        "                if not retrieved_docs:\n",
        "                    logger.warning(\"No se encontraron documentos relevantes.\")\n",
        "\n",
        "                    if attempt == max_attempts - 1:\n",
        "                        response_data = {\n",
        "                            \"response\": \"No he encontrado informaci√≥n espec√≠fica para responder con seguridad, pero puedo intentar darte una orientaci√≥n general.\",\n",
        "                            \"docs\": [],\n",
        "                            \"attempt\": attempt + 1,\n",
        "                            \"category\": category,\n",
        "                            \"validation\": {\"valid\": False, \"error\": \"Sin documentos relevantes\"}\n",
        "                        }\n",
        "                        break  # Salir del bucle\n",
        "                    else:\n",
        "                        continue\n",
        "\n",
        "                chain = create_rag_chain(llm, selected_prompt_template)\n",
        "                result = chain.invoke({\"input_documents\": retrieved_docs, \"question\": query})\n",
        "                response_text = result.get('output_text', result if isinstance(result, str) else str(result)).strip()\n",
        "\n",
        "                validation = validate_legal_response(response_text, query, retrieved_docs, llm)\n",
        "                if validation.get(\"valid\", False):\n",
        "                    logger.info(f\"Respuesta validada en intento {attempt+1}.\")\n",
        "                    response_data = {\n",
        "                        \"response\": response_text,\n",
        "                        \"docs\": retrieved_docs,\n",
        "                        \"attempt\": attempt+1,\n",
        "                        \"category\": category,\n",
        "                        \"validation\": validation\n",
        "                    }\n",
        "                    break\n",
        "                else:\n",
        "                    logger.warning(f\"Respuesta no v√°lida en intento {attempt+1}: {validation}\")\n",
        "\n",
        "                    # Intentar mejorar el prompt si no es el √∫ltimo intento\n",
        "                    if attempt < max_attempts - 1:\n",
        "                        # Extraer sugerencias de mejora\n",
        "                        hints = extract_improvement_hints(validation.get(\"raw_validation_output\", \"\"))\n",
        "                        reformulated_query = reformulate_user_query(query, hints)\n",
        "                        # Volver a obtener el prompt base original\n",
        "                        selected_prompt_template = get_prompt_by_category(category, memory_text, reformulated_query)\n",
        "\n",
        "                        # Si es un prompt est√°ndar con {context}, entonces podemos modificarlo\n",
        "                        if isinstance(selected_prompt_template, ChatPromptTemplate):\n",
        "                            try:\n",
        "                                # Tomar solo el system prompt como texto\n",
        "                                original_system_msg = selected_prompt_template.messages[0].prompt.template\n",
        "                                new_system_msg = augment_prompt_with_validation(original_system_msg, hints)\n",
        "\n",
        "                                selected_prompt_template = ChatPromptTemplate.from_messages([\n",
        "                                    SystemMessagePromptTemplate.from_template(new_system_msg),\n",
        "                                    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
        "                                ])\n",
        "\n",
        "                                logger.info(\"Prompt adaptado con sugerencias del validador.\")\n",
        "                            except Exception as e:\n",
        "                                logger.warning(f\"No se pudo adaptar el prompt con sugerencias: {e}\")\n",
        "\n",
        "                    if attempt == max_attempts - 1:\n",
        "                        response_data = {\n",
        "                            \"response\": response_text,\n",
        "                            \"docs\": retrieved_docs,\n",
        "                            \"attempt\": attempt+1,\n",
        "                            \"category\": category,\n",
        "                            \"validation\": validation\n",
        "                        }\n",
        "                    debug=1\n",
        "                    if attempt == max_attempts - 1 and not validation.get(\"valid\", False) and debug ==1:\n",
        "                        logger.info(\"Iniciando auto-refinamiento de respuesta con feedback del validador.\")\n",
        "\n",
        "                        refinement_prompt = ChatPromptTemplate.from_messages([\n",
        "                            SystemMessagePromptTemplate.from_template(\n",
        "                                refinement_module_prompt\n",
        "                            ),\n",
        "                            HumanMessagePromptTemplate.from_template(\"Corrige la respuesta anterior respetando la estructura exacta y ajust√°ndola seg√∫n los fallos detectados.\")\n",
        "                        ])\n",
        "\n",
        "                        refinement_chain = LLMChain(llm=llm, prompt=refinement_prompt)\n",
        "                        refined_result = refinement_chain.invoke({\n",
        "                            \"fallos\": validation.get(\"raw_validation_output\", \"\"),\n",
        "                            \"respuesta\": response_text\n",
        "                        })\n",
        "\n",
        "                        response_data[\"response\"] = refined_result.get(\"text\", \"\").strip()\n",
        "                        response_data[\"validation\"][\"refinado\"] = True\n",
        "\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error en intento RAG {attempt+1}: {e}\", exc_info=True)\n",
        "                if attempt == max_attempts-1:\n",
        "                    response_data = {\"response\": f\"Ocurri√≥ un error grave al procesar tu consulta.\", \"docs\": [], \"attempt\": attempt+1, \"category\": category, \"error\": str(e)}\n",
        "\n",
        "            query = reformulated_query\n",
        "\n",
        "    # --- 5. Guardar turno en la memoria ---\n",
        "    if response_data and \"response\" in response_data:\n",
        "        memory.add_turn(query, response_data[\"response\"], llm)\n",
        "    else:\n",
        "        memory.add_turn(query, \"[Error al generar respuesta]\", llm=None)\n",
        "\n",
        "    return response_data\n",
        "\n",
        "# --- 8. Sistema de feedback y evaluaci√≥n ---\n",
        "def collect_feedback(query, response):\n",
        "    \"\"\"Solicita feedback al usuario sobre la respuesta\"\"\"\n",
        "    print(\"\\n\" + \"=\"*20 + \" FEEDBACK \" + \"=\"*20)\n",
        "    print(f\"Consulta: '{query}'\")\n",
        "    print(f\"\\nRespuesta proporcionada:\\n{response}\")\n",
        "    print(\"=\"*50)\n",
        "    while True:\n",
        "        try:\n",
        "            rating = input(\"¬øQu√© tan √∫til fue esta respuesta? (1=Nada √∫til, 5=Muy √∫til, 0=Saltar): \")\n",
        "            rating = int(rating)\n",
        "            if 0 <= rating <= 5:\n",
        "                 return rating if rating > 0 else None # Devolver None si es 0\n",
        "            else:\n",
        "                 print(\"Por favor, introduce un n√∫mero entre 0 y 5.\")\n",
        "        except ValueError:\n",
        "            print(\"Entrada inv√°lida. Por favor, introduce un n√∫mero.\")\n",
        "\n",
        "def extract_improvement_hints(validation_output):\n",
        "    # Extrae frases despu√©s de los puntos de fallo\n",
        "    hints = []\n",
        "    for match in re.finditer(r'\\[\\w+:\\s*No\\](.*?)\\n', validation_output, re.IGNORECASE):\n",
        "        hint = match.group(1).strip()\n",
        "        if hint:\n",
        "            hints.append(hint)\n",
        "    return hints\n",
        "\n",
        "def augment_prompt_with_validation(prompt_text, improvement_hints):\n",
        "    if not improvement_hints:\n",
        "        return prompt_text\n",
        "    hint_block = \"\\nIMPORTANTE: Al responder, aseg√∫rate de abordar tambi√©n los siguientes aspectos:\\n\"\n",
        "    for h in improvement_hints:\n",
        "        hint_block += f\"- {h}\\n\"\n",
        "    return prompt_text + hint_block\n",
        "\n",
        "def reformulate_user_query(original_query: str, improvement_hints: list[str]) -> str:\n",
        "    \"\"\"A√±ade instrucciones expl√≠citas a la consulta original usando sugerencias del validador.\"\"\"\n",
        "    if not improvement_hints:\n",
        "        return original_query\n",
        "    reformulation = \"\\n\\n Tambi√©n responde espec√≠ficamente a:\\n\"\n",
        "    for hint in improvement_hints:\n",
        "        reformulation += f\"- {hint.strip()}\\n\"\n",
        "    return original_query + reformulation\n",
        "\n",
        "def save_interaction(query, result_data, feedback=None):\n",
        "    \"\"\"Guarda la interacci√≥n completa para an√°lisis y mejora\"\"\"\n",
        "    try:\n",
        "        interaction = {\n",
        "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            \"query\": query,\n",
        "            \"response\": result_data.get(\"response\"),\n",
        "            \"category\": result_data.get(\"category\"),\n",
        "            \"attempt\": result_data.get(\"attempt\"),\n",
        "            \"validation\": result_data.get(\"validation\"),\n",
        "            \"error\": result_data.get(\"error\"),\n",
        "            \"feedback\": feedback, # A√±adir feedback del usuario\n",
        "            \"retrieved_docs\": [\n",
        "                {\n",
        "                    \"content_preview\": doc.page_content[:200] + \"...\", # Preview\n",
        "                    \"metadata\": doc.metadata,\n",
        "                    #\"score\": doc.score # A√±adir si el retriever devuelve score\n",
        "                }\n",
        "                # Limitar el n√∫mero de documentos guardados para no hacer el log enorme\n",
        "                for doc in result_data.get(\"docs\", [])[:5] # Guardar metadata de los primeros 5 docs usados\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        with open(\"interacciones_legales.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(json.dumps(interaction, ensure_ascii=False, default=str) + \"\\n\") # default=str para manejar tipos no serializables\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error guardando interacci√≥n: {e}\")\n",
        "\n",
        "def evaluate_response_quality(query, response, llm):\n",
        "    \"\"\"Eval√∫a la calidad de una respuesta utilizando el LLM\"\"\"\n",
        "    # (Se mantiene la estructura de evaluaci√≥n con prompts, pero se simplifica la llamada)\n",
        "    metrics = {}\n",
        "    prompts = {\n",
        "        \"legal_accuracy\": \"\"\"\n",
        "        Eval√∫a la PRECISI√ìN JUR√çDICA de esta respuesta sobre protecci√≥n de datos (Espa√±a/UE):\n",
        "        Consulta: {query}\n",
        "        Respuesta: {response}\n",
        "        ¬øLa respuesta es correcta seg√∫n RGPD/LOPDGDD? ¬øInterpreta bien las normas?\n",
        "        Puntuaci√≥n (0-10, solo n√∫mero):\"\"\",\n",
        "        \"relevance\": \"\"\"\n",
        "        Eval√∫a la RELEVANCIA de la respuesta respecto a la consulta:\n",
        "        Consulta: {query}\n",
        "        Respuesta: {response}\n",
        "        ¬øResponde directamente a lo preguntado? ¬øEvita informaci√≥n superflua?\n",
        "        Puntuaci√≥n (0-10, solo n√∫mero):\"\"\",\n",
        "        \"completeness\": \"\"\"\n",
        "        Eval√∫a la COMPLETITUD de esta respuesta legal:\n",
        "        Consulta: {query}\n",
        "        Respuesta: {response}\n",
        "        ¬øCubre todos los aspectos clave? ¬øOfrece suficiente detalle/fundamento?\n",
        "        Puntuaci√≥n (0-10, solo n√∫mero):\"\"\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        for metric, prompt_template in prompts.items():\n",
        "            eval_prompt = prompt_template.format(query=query, response=response)\n",
        "            eval_resp = llm.invoke(eval_prompt)\n",
        "            score_match = re.search(r'\\b(10|[0-9])\\b', eval_resp)\n",
        "            if score_match:\n",
        "                metrics[metric] = float(score_match.group(0)) / 10.0\n",
        "            else:\n",
        "                logger.warning(f\"No se pudo extraer puntuaci√≥n para m√©trica '{metric}' de: '{eval_resp}'\")\n",
        "                metrics[metric] = 0.5 # Default neutral\n",
        "\n",
        "        # Calcular m√©trica general (si todas las m√©tricas est√°n presentes)\n",
        "        if len(metrics) == 3:\n",
        "             metrics[\"overall\"] = sum(metrics.values()) / 3.0\n",
        "        else:\n",
        "             metrics[\"overall\"] = 0.5 # Default si faltan m√©tricas\n",
        "\n",
        "        logger.info(f\"Evaluaci√≥n de calidad autom√°tica: {metrics}\")\n",
        "        return metrics\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error evaluando calidad de respuesta: {e}\")\n",
        "        return {\"overall\": 0.5, \"error\": str(e)} # Default en caso de error\n",
        "\n",
        "def save_interaction_for_training(query, response, feedback_score, docs_retrieved):\n",
        "     \"\"\"Guarda interacci√≥n con feedback positivo para posible entrenamiento futuro.\"\"\"\n",
        "     # Solo guardar si el feedback es bueno (ej. 4 o 5)\n",
        "     if feedback_score is None or feedback_score < 4:\n",
        "         return\n",
        "\n",
        "     try:\n",
        "         training_example = {\n",
        "             \"query\": query,\n",
        "             \"positive_response\": response, # Marcado como positivo por el feedback\n",
        "             \"feedback_score\": feedback_score,\n",
        "             # Opcional: incluir contexto relevante si se quiere entrenar RAG-finetuning\n",
        "             \"relevant_context\": [doc.page_content for doc in docs_retrieved[:2]], # Ej: 2 docs m√°s relevantes\n",
        "             \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "         }\n",
        "\n",
        "         with open(\"training_data_positive.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
        "             f.write(json.dumps(training_example, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "         logger.info(f\"Guardada interacci√≥n con feedback {feedback_score} para posible entrenamiento.\")\n",
        "\n",
        "     except Exception as e:\n",
        "         logger.error(f\"Error guardando interacci√≥n para entrenamiento: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "def normalize_text(text):\n",
        "    \"\"\"Normaliza el texto: elimina tildes, min√∫sculas, quita signos de puntuaci√≥n.\"\"\"\n",
        "    text = unicodedata.normalize('NFD', text)\n",
        "    text = ''.join(c for c in text if unicodedata.category(c) != 'Mn')  # quitar tildes\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # elimina puntuaci√≥n\n",
        "    return text\n",
        "\n",
        "def is_action_request(query):\n",
        "    \"\"\"Detecta si la pregunta sugiere solicitud de acciones, derechos o pasos a seguir.\"\"\"\n",
        "    normalized_query = normalize_text(query)\n",
        "\n",
        "    action_phrases = [\n",
        "        \"que pueden hacer\",\n",
        "        \"que medidas pueden tomar\",\n",
        "        \"que derechos tienen\",\n",
        "        \"como pueden reclamar\",\n",
        "        \"como pueden actuar\",\n",
        "        \"como reclamar\",\n",
        "        \"como actuar\",\n",
        "        \"que opciones tienen\",\n",
        "        \"que pasos pueden seguir\",\n",
        "        \"que pueden solicitar\",\n",
        "        \"que acciones pueden emprender\",\n",
        "        \"como defenderse\",\n",
        "        \"como denunciar\",\n",
        "        \"como protegerse\",\n",
        "        \"como impugnar\",\n",
        "        \"como negarse\",\n",
        "        \"que recurso tienen\",\n",
        "        \"que alternativas tienen\",\n",
        "        \"que pueden exigir\",\n",
        "        \"que sanciones puede haber\",\n",
        "        \"que consecuencias hay\",\n",
        "    ]\n",
        "\n",
        "    return any(phrase in normalized_query for phrase in action_phrases)\n",
        "\n",
        "\n",
        "# --- Bloque Principal de Interacci√≥n ---\n",
        "intent_classification_cache = {}\n",
        "last_user_query = None #Para preguntas continuistas\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\": # Poner el c√≥digo de ejecuci√≥n aqu√≠ para evitar que se ejecute al importar\n",
        "\n",
        "    # Verificar que las variables necesarias existen\n",
        "    if 'llm' not in locals() or 'llm_evaluador' not in locals() or 'base_retriever' not in locals() or 'persisted_vectorstore' not in locals():\n",
        "         print(\"ERROR: Aseg√∫rate de haber ejecutado las celdas anteriores para inicializar llm, llm_evaluador, base_retriever y persisted_vectorstore.\")\n",
        "    else:\n",
        "         print(\"Modelo LLM y retriever listos.\")\n",
        "\n",
        "         # Inicializar memoria conversacional\n",
        "         conversation_memory = ImprovedMemory(max_turns=4) # Guardar 4 turnos\n",
        "         last_query=\"\"\n",
        "         print(\"\\nBienvenido al Asistente Legal de Protecci√≥n de Datos.\")\n",
        "         print(\"Escribe 'salir' para terminar.\")\n",
        "\n",
        "         while True:\n",
        "             user_query = input(\"\\nTu consulta: \")\n",
        "             if user_query.lower() == 'salir':\n",
        "                 break\n",
        "             if not user_query:\n",
        "                 continue\n",
        "\n",
        "             start_time = time.time()\n",
        "\n",
        "             # --- Llamada principal al sistema RAG con reintentos ---\n",
        "             result = get_response_with_retry(\n",
        "                 query=user_query,\n",
        "                 llm=llm,\n",
        "                 base_retriever=base_retriever,\n",
        "                 memory=conversation_memory, # Pasar la instancia de memoria\n",
        "                 last_query=last_query\n",
        "             )\n",
        "             last_query = user_query\n",
        "             end_time = time.time()\n",
        "\n",
        "             # --- Mostrar Resultados ---\n",
        "             print(\"\\n--- Respuesta del Asistente ---\")\n",
        "             print(result.get(\"response\", \"No se pudo generar respuesta.\"))\n",
        "             print(\"-\" * 30)\n",
        "             logger.info(f\"Respuesta generada en {end_time - start_time:.2f} segundos.\")\n",
        "             logger.info(f\"Categor√≠a: {result.get('category', 'N/A')}, Intentos: {result.get('attempt', 'N/A')}\")\n",
        "             if result.get(\"validation\"):\n",
        "                 logger.info(f\"Validaci√≥n: {result['validation']}\")\n",
        "             if result.get(\"error\"):\n",
        "                 logger.error(f\"Error reportado: {result['error']}\")\n",
        "\n",
        "             # Mostrar documentos fuente (opcional, para depuraci√≥n)\n",
        "             if result.get(\"docs\"):\n",
        "                 print(f\"\\nFuentes consultadas ({len(result['docs'])} documentos):\")\n",
        "                 for i, doc in enumerate(result[\"docs\"]):\n",
        "                     source = doc.metadata.get('source', 'Desconocido')\n",
        "                     page = doc.metadata.get('page', '?')\n",
        "                     tipo = doc.metadata.get('tipo', '')\n",
        "                     print(f\"  [{i+1}] {source} (P√°g: {page}, Tipo: {tipo})\") # Preview m√°s corto\n",
        "\n",
        "             # --- Feedback y Evaluaci√≥n (Opcional) ---\n",
        "             user_feedback = collect_feedback(user_query, result.get(\"response\"))\n",
        "             save_interaction(user_query, result, user_feedback)\n",
        "\n",
        "             if user_feedback:\n",
        "                 # Si hubo feedback positivo, guardar para posible entrenamiento\n",
        "                 save_interaction_for_training(user_query, result.get(\"response\"), user_feedback, result.get(\"docs\", []))\n",
        "\n",
        "             # Evaluar calidad autom√°ticamente (opcional, consume tokens)\n",
        "             # quality_metrics = evaluate_response_quality(user_query, result.get(\"response\"), llm)\n",
        "             # logger.info(f\"M√©tricas de calidad autom√°ticas: {quality_metrics}\")\n",
        "\n",
        "         print(\"\\n¬°Hasta luego!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoxVqhOEWj5b"
      },
      "source": [
        "# Comparaciones de modelos\n",
        "\n",
        "A continuaci√≥n esta el codigo para comparar las respuestas entre modelos\n",
        "\n",
        "Hay que resetear ollama entre consultas para evitar que use pueda recordar datos de la conversaci√≥n entre pruebas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ior64B0AfA6Y"
      },
      "outputs": [],
      "source": [
        "#Aqui pegamos nuestra pregunta\n",
        "pregunta=\"¬øCu√°les son los principios fundamentales que deben cumplirse para garantizar el tratamiento adecuado de datos personales seg√∫n la normativa de protecci√≥n de datos en Espa√±a?\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWBx9qAlajxd",
        "outputId": "53bb402a-6074-4e5b-baf7-4f6347b4eb6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7392 sha256=f57f9944ddce0f3b97c0d3ebf03273d6ba420fdad3938f992506e60bfb9d403b\n",
            "  Stored in directory: /root/.cache/pip/wheels/2b/4d/8f/55fb4f7b9b591891e8d3f72977c4ec6c7763b39c19f0861595\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (5.9.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install gputil\n",
        "!pip install psutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRlKHsidWshH"
      },
      "source": [
        "## Modelo Base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z-7dmDgZEpe"
      },
      "source": [
        "Descargamos el modelo base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ygs3fkqTY_1Y",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "!ollama run llama3:8b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRA0PR0yWjIu",
        "outputId": "556423d4-a229-46bc-d82d-b8800818a3c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- RESPUESTA ---\n",
            "Seg√∫n la normativa de protecci√≥n de datos personales en Espa√±a, espec√≠ficamente la Ley Org√°nica 3/2018, de 5 de diciembre, sobre Protecci√≥n de Datos Personales y garant√≠a de los derechos digitales (LOPDGDD) y el Reglamento (UE) 2016/679 del Parlamento Europeo y del Consejo, de 27 de abril de 2016, relativo a la protecci√≥n de las personas f√≠sicas con respecto al tratamiento de datos personales y al libre movimiento de dichos datos, y por la que se sustituye el Reglamento (UE) n¬∫ 95/46/CE (RGPD), los principios fundamentales para garantizar el tratamiento adecuado de datos personales son:\n",
            "\n",
            "1. **Leg√≠timo**: El tratamiento debe ser l√≠cito y basado en una base leg√≠tima, como la consignaci√≥n del consentimiento expl√≠cito, la ejecuci√≥n de un contrato o la satisfacci√≥n de un inter√©s leg√≠timo.\n",
            "2. **Especial**: Los datos personales deben tratarse de manera estricta y limitada a lo necesario para el prop√≥sito especifico, evitando cualquier tratamiento no autorizado o excesivo.\n",
            "3. **Transparencia**: El responsable del tratamiento debe informar claramente sobre la recopilaci√≥n y el uso de los datos personales, incluyendo la identificaci√≥n del responsable, la finalidad del tratamiento y las garant√≠as de seguridad implementadas.\n",
            "4. **Limitaci√≥n del acceso**: Solo aquellos que necesiten acceder a los datos personales para fines leg√≠timos deben tener acceso a ellos.\n",
            "5. **Accesibilidad**: Los titulares de los datos personales deben tener acceso a sus datos y poder ejercer sus derechos, como el derecho a la rectificaci√≥n o eliminaci√≥n.\n",
            "6. **Integridad y seguridad**: El responsable del tratamiento debe implementar medidas para garantizar la integridad y seguridad de los datos personales, evitando cualquier tipo de da√±o, p√©rdida, modificaci√≥n o acceso no autorizado.\n",
            "7. **No discriminaci√≥n**: El tratamiento de datos personales no puede ser discriminatorio y debe respetar las igualdades y diferencias entre las personas.\n",
            "8. **Responsabilidad**: Los responsables del tratamiento deben ser conscientes de su responsabilidad en el tratamiento de datos personales y tomar medidas para garantizar que se cumplan los principios fundamentales.\n",
            "\n",
            "Al cumplir con estos principios, se puede garantizar un tratamiento adecuado y seguro de los datos personales en Espa√±a.\n",
            "\n",
            "--- M√âTRICAS ---\n",
            "Modelo: llama3:8b\n",
            "Tokens generados: 335\n",
            "Tiempo total (incluye carga): 19.90 s\n",
            "Tiempo de inferencia (solo generaci√≥n): 19.77 s\n",
            "Velocidad de generaci√≥n: 16.94 tokens/seg\n",
            "RAM usada: 0.44 GB\n",
            "Uso CPU promedio durante inferencia: 64.5%\n",
            "VRAM usada: 5.73 GB\n",
            "Carga GPU: 92.0%\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import psutil\n",
        "import GPUtil\n",
        "from statistics import mean\n",
        "import threading\n",
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# --- Reiniciar Ollama ---\n",
        "!kill -9 $(ps aux | grep '[o]llama' | awk '{print $2}')\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "time.sleep(3)  # Esperar a que Ollama est√© listo\n",
        "\n",
        "# --- Configuraci√≥n de prueba ---\n",
        "model_name = \"llama3:8b\"\n",
        "\n",
        "# --- Monitorear uso de RAM ---\n",
        "def get_system_metrics():\n",
        "    ram_used = psutil.virtual_memory().used / (1024 ** 3)  # GB\n",
        "    return ram_used\n",
        "\n",
        "# --- Monitorear uso de GPU (VRAM) ---\n",
        "def get_gpu_metrics():\n",
        "    gpus = GPUtil.getGPUs()\n",
        "    if gpus:\n",
        "        gpu = gpus[0]\n",
        "        return gpu.memoryUsed / 1024, gpu.load * 100  # GB, %\n",
        "    return 0, 0\n",
        "\n",
        "# --- Iniciar modelo ---\n",
        "llm = OllamaLLM(model=model_name, max_tokens=250)\n",
        "\n",
        "# --- Medici√≥n de rendimiento ---\n",
        "start_total = time.time()\n",
        "\n",
        "ram_before = get_system_metrics()\n",
        "gpu_mem_before, gpu_load_before = get_gpu_metrics()\n",
        "\n",
        "# --- Medici√≥n activa de CPU en paralelo ---\n",
        "cpu_usage_during_infer = []\n",
        "stop_cpu_monitor = False\n",
        "\n",
        "def monitor_cpu():\n",
        "    while not stop_cpu_monitor:\n",
        "        cpu = psutil.cpu_percent(interval=0.1)\n",
        "        cpu_usage_during_infer.append(cpu)\n",
        "\n",
        "cpu_thread = threading.Thread(target=monitor_cpu)\n",
        "cpu_thread.start()\n",
        "\n",
        "# --- Inferencia ---\n",
        "start_infer = time.time()\n",
        "response = llm.invoke(pregunta)\n",
        "end_infer = time.time()\n",
        "\n",
        "# --- Detener CPU monitor ---\n",
        "stop_cpu_monitor = True\n",
        "cpu_thread.join()\n",
        "\n",
        "ram_after = get_system_metrics()\n",
        "gpu_mem_after, gpu_load_after = get_gpu_metrics()\n",
        "end_total = time.time()\n",
        "\n",
        "# --- M√©tricas calculadas ---\n",
        "total_time = end_total - start_total\n",
        "infer_time = end_infer - start_infer\n",
        "token_count = len(response.split())\n",
        "avg_cpu = mean(cpu_usage_during_infer) if cpu_usage_during_infer else 0\n",
        "\n",
        "# --- Salida ---\n",
        "print(\"\\n--- RESPUESTA ---\")\n",
        "print(response)\n",
        "\n",
        "print(\"\\n--- M√âTRICAS ---\")\n",
        "print(f\"Modelo: {model_name}\")\n",
        "print(f\"Tokens generados: {token_count}\")\n",
        "print(f\"Tiempo total (incluye carga): {total_time:.2f} s\")\n",
        "print(f\"Tiempo de inferencia (solo generaci√≥n): {infer_time:.2f} s\")\n",
        "print(f\"Velocidad de generaci√≥n: {token_count / infer_time:.2f} tokens/seg\")\n",
        "print(f\"RAM usada: {ram_after - ram_before:.2f} GB\")\n",
        "print(f\"Uso CPU promedio durante inferencia: {avg_cpu:.1f}%\")\n",
        "print(f\"VRAM usada: {gpu_mem_after - gpu_mem_before:.2f} GB\")\n",
        "print(f\"Carga GPU: {gpu_load_after:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoxPozkRXX33"
      },
      "source": [
        "## Modelo Base + RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ix4heZm2XaKT",
        "outputId": "889b5bc7-5a56-4794-8f68-6f7073f1ef3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "Asistente legal de protecci√≥n de datos listo (usando load_qa_chain). Escribe 'Exit' para salir.\n",
            "--------------------------------------------------\n",
            "\n",
            "Respuesta:\n",
            "En primer lugar, es importante destacar que la publicaci√≥n de las notas personales incluyendo el nombre completo y DNI del compa√±ero sin su consentimiento puede considerarse un delito contra la protecci√≥n de datos personales.\n",
            "\n",
            "Seg√∫n el Art√≠culo 4 del Reglamento General de Protecci√≥n de Datos (RGPD), los datos personales deben tratarse de manera l√≠cita, transparente y justificada. La publicaci√≥n de las notas personales sin el consentimiento del titular puede considerarse un tratamiento il√≠cito de sus datos.\n",
            "\n",
            "Adem√°s, la publicaci√≥n de informaci√≥n personal como el DNI puede considerarse una infracci√≥n del Art√≠culo 5 del RGPD, que establece que los datos personales deben ser procesados de manera tal que se garantice su integridad y confidencialidad.\n",
            "\n",
            "En cuanto a la identificaci√≥n de Sinosuque, aunque no hay suficiente informaci√≥n para determinar si es un responsable del tratamiento de datos (art√≠culo 4.7 del RGPD), es importante destacar que la publicaci√≥n de las notas personales puede considerarse un delito contra la protecci√≥n de datos personales.\n",
            "\n",
            "En Espa√±a, la infracci√≥n de la protecci√≥n de datos personales puede ser punible seg√∫n el Art√≠culo 20 de la Ley Org√°nica 15/1999, de 13 de diciembre, sobre Protecci√≥n de Datos de Car√°cter Personal. Entre los delitos que se pueden cometer en este sentido se encuentran:\n",
            "\n",
            "* La publicaci√≥n no autorizada de datos personales (Art√≠culo 21 de la LO 15/1999)\n",
            "* El tratamiento il√≠cito de datos personales (Art√≠culo 22 de la LO 15/1999)\n",
            "\n",
            "En conclusi√≥n, Sinosuque podr√≠a enfrentarse a delitos como la publicaci√≥n no autorizada de datos personales y el tratamiento il√≠cito de datos personales, seg√∫n la Ley Org√°nica 15/1999, de 13 de diciembre, sobre Protecci√≥n de Datos de Car√°cter Personal.\n",
            "\n",
            "Saliendo del asistente.\n"
          ]
        }
      ],
      "source": [
        "#Para resetear ollama\n",
        "!kill -9 $(ps aux | grep '[o]llama' | awk '{print $2}')\n",
        "\n",
        "import subprocess\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3) # Esperar a que Ollama se cargue\n",
        "\n",
        "\n",
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# Initialize the LLaMA model\n",
        "llm = OllamaLLM(model=\"llama3:8b\") #aqui poinemos el modelo base\n",
        "\n",
        "\n",
        "\n",
        "# Importaci√≥n correcta para versiones recientes de Langchain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "import torch\n",
        "\n",
        "\n",
        "# Define la plantilla del prompt DETALLADA\n",
        "prompt_template = \"\"\"\n",
        "Eres un asistente legal experto en protecci√≥n de datos en Espa√±a.\n",
        "Tu tarea es analizar la legalidad de la situaci√≥n descrita en la 'Pregunta' bas√°ndote **principalmente** en los siguientes 'Textos Legales Recuperados'.\n",
        "Si los textos recuperados contienen suficiente informaci√≥n, responde √∫nicamente bas√°ndote en ellos.\n",
        "Si los textos no contienen una respuesta clara pero la pregunta se refiere a conceptos b√°sicos del RGPD, usa lo aprendido en el entrenamiento para dar una respuesta general, especificando que no proviene de los textos recuperados.\n",
        "Si la pregunta requiere informaci√≥n espec√≠fica que no est√° en los textos recuperados ni en tu entrenamiento, ind√≠calo claramente.\n",
        "\n",
        "Textos Legales Recuperados:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "An√°lisis Legal y Conclusi√≥n:\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Crea la cadena usando load_qa_chain con el prompt personalizado\n",
        "# chain_type=\"stuff\" es adecuado si los chunks recuperados + pregunta caben en la ventana de contexto del LLM.\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=PROMPT)\n",
        "\n",
        "print (\"-\"*50)\n",
        "print (\"Asistente legal de protecci√≥n de datos listo (usando load_qa_chain). Escribe 'Exit' para salir.\")\n",
        "print (\"-\"*50)\n",
        "\n",
        "\n",
        "query=pregunta\n",
        "# 1. Recupera los documentos relevantes\n",
        "# Se puede ajustar 'k' aqu√≠ si quieres m√°s/menos contexto: retriever.search_kwargs = {'k': 5}\n",
        "docs_retrieved = retriever.invoke(query)\n",
        "\n",
        "# Prepara el input para la cadena\n",
        "input_data = {\n",
        "    \"input_documents\": docs_retrieved,\n",
        "    \"question\": query\n",
        "}\n",
        "\n",
        "# 2. Ejecuta la cadena de generaci√≥n con los documentos recuperados y la pregunta\n",
        "# Si usas una versi√≥n de Langchain > 0.1.0, invoke devuelve un diccionario.\n",
        "# Si usas una versi√≥n anterior, podr√≠a devolver directamente el string.\n",
        "# El par√°metro return_only_outputs=True ya no es necesario/v√°lido en invoke para versiones > 0.1.0\n",
        "result = chain.invoke(input_data)\n",
        "\n",
        "# Imprime la salida del LLM (la clave suele ser 'output_text' en versiones recientes)\n",
        "if isinstance(result, dict) and 'output_text' in result:\n",
        "    print(\"\\nRespuesta:\")\n",
        "    print(result['output_text'])\n",
        "elif isinstance(result, str): # Compatibilidad con versiones anteriores\n",
        "      print(\"\\nRespuesta:\")\n",
        "      print(result)\n",
        "else:\n",
        "      print(\"\\nRespuesta recibida (formato inesperado):\")\n",
        "      print(result)\n",
        "\n",
        "\n",
        "print(\"\\nSaliendo del asistente.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8VmL4S2W7Zf"
      },
      "source": [
        "## Finetunning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkurbP2nW_9W"
      },
      "source": [
        "### 8bit Q8_0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWCIODDTLDQ3",
        "outputId": "e6fd21ed-51ef-4a79-b250-fb72dd4ed9ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G‚†ô \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†ô \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†π \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†∏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†º \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†¥ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†¶ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†ß \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†á \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†è \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†ã \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†ô \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†π \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†∏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†º \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†¥ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†¶ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†ß \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†á \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†è \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†ã \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†ô \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†π \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†∏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†º \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†¥ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†¶ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†ß \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†á \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†è \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†ã \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†ô \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†π \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†∏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†¥ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†¥ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†¶ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†ß \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†á \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†è \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†ã \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†ô \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†π \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†∏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†º \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†¥ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†¶ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†ß \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†è \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†è \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†ô \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†ô \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†∏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†∏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†¥ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†¥ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†ß \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†á \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†á \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†è \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†ã \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†ô \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†π \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†∏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†¥ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†¶ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†ß \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†ß \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†á \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†è \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†ã \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†ô \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G‚†π \u001b[K\u001b[?25h\u001b[?2026l^C\n"
          ]
        }
      ],
      "source": [
        "!ollama run hf.co/serdom02/Leyeneitor_8bitQ8_0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sI2VEYV6XWmD",
        "outputId": "4d21e444-9ddd-470f-ce24-6a6628b220d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- RESPUESTA ---\n",
            "Para garantizar el tratamiento adecuado de datos personales en Espa√±a, se deben cumple los siguientes principios fundamentales:\n",
            "\n",
            "1. **Finalidad**: Los datos solo pueden tratarse para las finalidades expresadas al recopilarlos.\n",
            "2. **Transparencia**: El tratamiento debe ser transparente, informando claramente a los afectados sobre qui√©n es el responsable del tratamiento y con qu√© finalidad se usar√°n sus datos.\n",
            "3. **Principio de minimizaci√≥n**: Solo deben recogerse los datos personales estrictamente necesarios para la finalidad declarada.\n",
            "4. **Limitaci√≥n del plazo de conservaci√≥n**: Los datos solo pueden conservarse durante el tiempo necesario para cumplir con la finalidad de su tratamiento.\n",
            "5. **Confidencialidad**: Los datos deben tratarse en condiciones de confidencialidad, evitando su acceso a terceros no autorizados.\n",
            "6. **Principio de libertad**: Los derechos y libertades del afectado deben ser respetados, sin que se imponga un tratamiento desproporcionado.\n",
            "7. **Principio de proporcionalidad**: El tratamiento debe ser el m√≠nimo necesario para la finalidad perseguida.\n",
            "\n",
            " Adem√°s, los datos deben ser:\n",
            "\n",
            "8. **Accesibilidad**: Los afectados deben poder acceder a sus datos personales y solicitar su rectificaci√≥n o supresi√≥n.\n",
            "9. **Limitaci√≥n del plazo de conservaci√≥n**: Los datos solo pueden conservarse durante el tiempo necesario para cumplir con la finalidad de su tratamiento.\n",
            "10. **Sustituci√≥n de derechos**: No se puede condicionar el ejercicio de derecho fundamental a la cesi√≥n de datos a terceros.\n",
            "\n",
            "La normativa espa√±ola exige que estos principios se apliquen de forma exhaustiva en el tratamiento de datos personales, protegiendo los derechos fundamentales de las personas cuyos datos se recogen.\n",
            "\n",
            "--- M√âTRICAS ---\n",
            "Modelo: hf.co/serdom02/Leyeneitor_8bitQ8_0\n",
            "Tokens generados: 242\n",
            "Tiempo total (incluye carga): 20.91 s\n",
            "Tiempo de inferencia (solo generaci√≥n): 20.79 s\n",
            "Velocidad de generaci√≥n: 11.64 tokens/seg\n",
            "RAM usada: 0.42 GB\n",
            "Uso CPU promedio durante inferencia: 62.5%\n",
            "VRAM usada: 9.10 GB\n",
            "Carga GPU: 90.0%\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import psutil\n",
        "import GPUtil\n",
        "from statistics import mean\n",
        "import threading\n",
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# --- Reiniciar Ollama ---\n",
        "!kill -9 $(ps aux | grep '[o]llama' | awk '{print $2}')\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "time.sleep(3)  # Esperar a que Ollama est√© listo\n",
        "\n",
        "# --- Configuraci√≥n de prueba ---\n",
        "model_name = \"hf.co/serdom02/Leyeneitor_8bitQ8_0\"\n",
        "\n",
        "# --- Monitorear uso de RAM ---\n",
        "def get_system_metrics():\n",
        "    ram_used = psutil.virtual_memory().used / (1024 ** 3)  # GB\n",
        "    return ram_used\n",
        "\n",
        "# --- Monitorear uso de GPU (VRAM) ---\n",
        "def get_gpu_metrics():\n",
        "    gpus = GPUtil.getGPUs()\n",
        "    if gpus:\n",
        "        gpu = gpus[0]\n",
        "        return gpu.memoryUsed / 1024, gpu.load * 100  # GB, %\n",
        "    return 0, 0\n",
        "\n",
        "# --- Iniciar modelo ---\n",
        "llm = OllamaLLM(model=model_name, max_tokens=250)\n",
        "\n",
        "# --- Medici√≥n de rendimiento ---\n",
        "start_total = time.time()\n",
        "\n",
        "ram_before = get_system_metrics()\n",
        "gpu_mem_before, gpu_load_before = get_gpu_metrics()\n",
        "\n",
        "# --- Medici√≥n activa de CPU en paralelo ---\n",
        "cpu_usage_during_infer = []\n",
        "stop_cpu_monitor = False\n",
        "\n",
        "def monitor_cpu():\n",
        "    while not stop_cpu_monitor:\n",
        "        cpu = psutil.cpu_percent(interval=0.1)\n",
        "        cpu_usage_during_infer.append(cpu)\n",
        "\n",
        "cpu_thread = threading.Thread(target=monitor_cpu)\n",
        "cpu_thread.start()\n",
        "\n",
        "# --- Inferencia ---\n",
        "start_infer = time.time()\n",
        "response = llm.invoke(pregunta)\n",
        "end_infer = time.time()\n",
        "\n",
        "# --- Detener CPU monitor ---\n",
        "stop_cpu_monitor = True\n",
        "cpu_thread.join()\n",
        "\n",
        "ram_after = get_system_metrics()\n",
        "gpu_mem_after, gpu_load_after = get_gpu_metrics()\n",
        "end_total = time.time()\n",
        "\n",
        "# --- M√©tricas calculadas ---\n",
        "total_time = end_total - start_total\n",
        "infer_time = end_infer - start_infer\n",
        "token_count = len(response.split())\n",
        "avg_cpu = mean(cpu_usage_during_infer) if cpu_usage_during_infer else 0\n",
        "\n",
        "# --- Salida ---\n",
        "print(\"\\n--- RESPUESTA ---\")\n",
        "print(response)\n",
        "\n",
        "print(\"\\n--- M√âTRICAS ---\")\n",
        "print(f\"Modelo: {model_name}\")\n",
        "print(f\"Tokens generados: {token_count}\")\n",
        "print(f\"Tiempo total (incluye carga): {total_time:.2f} s\")\n",
        "print(f\"Tiempo de inferencia (solo generaci√≥n): {infer_time:.2f} s\")\n",
        "print(f\"Velocidad de generaci√≥n: {token_count / infer_time:.2f} tokens/seg\")\n",
        "print(f\"RAM usada: {ram_after - ram_before:.2f} GB\")\n",
        "print(f\"Uso CPU promedio durante inferencia: {avg_cpu:.1f}%\")\n",
        "print(f\"VRAM usada: {gpu_mem_after - gpu_mem_before:.2f} GB\")\n",
        "print(f\"Carga GPU: {gpu_load_after:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBPrNkBgXSRg"
      },
      "source": [
        "### 16bit GGUF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMDE2J2MLN42"
      },
      "outputs": [],
      "source": [
        "!ollama run hf.co/serdom02/model_16bitGGUF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTWJXFTKXXHU",
        "outputId": "d3f4da4c-040e-4285-872d-bcfe9d89dde4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- RESPUESTA ---\n",
            "Los principios fundamentales para garantizar el tratamiento adecuado de datos personales seg√∫n la normativa espa√±ola son: legalidad, equilibrio, minimizaci√≥n (m√≠nimo necesario), transparencia, limitaci√≥n del plazo de conservaci√≥n, confidencialidad, finalidad espec√≠fica, y posibilidad de ejercicio de los derechos del afectado.\n",
            "\n",
            "--- M√âTRICAS ---\n",
            "Modelo: hf.co/serdom02/model_16bitGGUF\n",
            "Tokens generados: 39\n",
            "Tiempo total (incluye carga): 102.15 s\n",
            "Tiempo de inferencia (solo generaci√≥n): 102.09 s\n",
            "Velocidad de generaci√≥n: 0.38 tokens/seg\n",
            "RAM usada: 0.52 GB\n",
            "Uso CPU promedio durante inferencia: 34.5%\n",
            "VRAM usada: 14.43 GB\n",
            "Carga GPU: 30.0%\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import psutil\n",
        "import GPUtil\n",
        "from statistics import mean\n",
        "import threading\n",
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# --- Reiniciar Ollama ---\n",
        "!kill -9 $(ps aux | grep '[o]llama' | awk '{print $2}')\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "time.sleep(3)  # Esperar a que Ollama est√© listo\n",
        "\n",
        "# --- Configuraci√≥n de prueba ---\n",
        "model_name = \"hf.co/serdom02/model_16bitGGUF\"\n",
        "\n",
        "# --- Monitorear uso de RAM ---\n",
        "def get_system_metrics():\n",
        "    ram_used = psutil.virtual_memory().used / (1024 ** 3)  # GB\n",
        "    return ram_used\n",
        "\n",
        "# --- Monitorear uso de GPU (VRAM) ---\n",
        "def get_gpu_metrics():\n",
        "    gpus = GPUtil.getGPUs()\n",
        "    if gpus:\n",
        "        gpu = gpus[0]\n",
        "        return gpu.memoryUsed / 1024, gpu.load * 100  # GB, %\n",
        "    return 0, 0\n",
        "\n",
        "# --- Iniciar modelo ---\n",
        "llm = OllamaLLM(model=model_name, max_tokens=250)\n",
        "\n",
        "# --- Medici√≥n de rendimiento ---\n",
        "start_total = time.time()\n",
        "\n",
        "ram_before = get_system_metrics()\n",
        "gpu_mem_before, gpu_load_before = get_gpu_metrics()\n",
        "\n",
        "# --- Medici√≥n activa de CPU en paralelo ---\n",
        "cpu_usage_during_infer = []\n",
        "stop_cpu_monitor = False\n",
        "\n",
        "def monitor_cpu():\n",
        "    while not stop_cpu_monitor:\n",
        "        cpu = psutil.cpu_percent(interval=0.1)\n",
        "        cpu_usage_during_infer.append(cpu)\n",
        "\n",
        "cpu_thread = threading.Thread(target=monitor_cpu)\n",
        "cpu_thread.start()\n",
        "\n",
        "# --- Inferencia ---\n",
        "start_infer = time.time()\n",
        "response = llm.invoke(pregunta)\n",
        "end_infer = time.time()\n",
        "\n",
        "# --- Detener CPU monitor ---\n",
        "stop_cpu_monitor = True\n",
        "cpu_thread.join()\n",
        "\n",
        "ram_after = get_system_metrics()\n",
        "gpu_mem_after, gpu_load_after = get_gpu_metrics()\n",
        "end_total = time.time()\n",
        "\n",
        "# --- M√©tricas calculadas ---\n",
        "total_time = end_total - start_total\n",
        "infer_time = end_infer - start_infer\n",
        "token_count = len(response.split())\n",
        "avg_cpu = mean(cpu_usage_during_infer) if cpu_usage_during_infer else 0\n",
        "\n",
        "# --- Salida ---\n",
        "print(\"\\n--- RESPUESTA ---\")\n",
        "print(response)\n",
        "\n",
        "print(\"\\n--- M√âTRICAS ---\")\n",
        "print(f\"Modelo: {model_name}\")\n",
        "print(f\"Tokens generados: {token_count}\")\n",
        "print(f\"Tiempo total (incluye carga): {total_time:.2f} s\")\n",
        "print(f\"Tiempo de inferencia (solo generaci√≥n): {infer_time:.2f} s\")\n",
        "print(f\"Velocidad de generaci√≥n: {token_count / infer_time:.2f} tokens/seg\")\n",
        "print(f\"RAM usada: {ram_after - ram_before:.2f} GB\")\n",
        "print(f\"Uso CPU promedio durante inferencia: {avg_cpu:.1f}%\")\n",
        "print(f\"VRAM usada: {gpu_mem_after - gpu_mem_before:.2f} GB\")\n",
        "print(f\"Carga GPU: {gpu_load_after:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8SWdO38XT5k"
      },
      "source": [
        "### q4_k_m GGUF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d4j3eDmLRzH"
      },
      "outputs": [],
      "source": [
        "!ollama run hf.co/serdom02/model_q4_k_mGGUF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8VuZ5VWXXi1",
        "outputId": "6dc9c016-4c08-4ab7-b944-fee11d00d368"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- RESPUESTA ---\n",
            "Seg√∫n la normativa espa√±ola, los principios fundamentales para el tratamiento de datos personales son:\n",
            "\n",
            "1. **Legalidad**: El tratamiento debe ser l√≠cito y debidamente autorizado por una ley o regulaci√≥n.\n",
            "2. **Eficacia**: El tratamiento debe ser preciso y completo, evitando la recopilaci√≥n de datos innecesarios.\n",
            "3. **Finalidad**: El tratamiento debe tener una finalidad clara y leg√≠tima, informada al titular de los datos.\n",
            "4. **Transparencia**: Debe existir una informaci√≥n clara y accesible sobre el tratamiento y sus finalidades.\n",
            "5. **Limitaci√≥n del plazo de conservaci√≥n**: Los datos solo deben conservarse durante el tiempo necesario para cumplir con la finalidad que justific√≥ su recopilaci√≥n.\n",
            "6. **Integridad y confidencialidad**: El tratamiento debe garantizar la integridad y confidencialidad de los datos, evitando accesos no autorizados.\n",
            "7. **Minimalizaci√≥n**: Solo se deben recopilar los datos necesarios para cumplir con la finalidad del tratamiento.\n",
            "\n",
            "Adem√°s, en algunos casos espec√≠ficos, tambi√©n es aplicable el principio de:\n",
            "\n",
            "8. **No discriminaci√≥n**: No se pueden tratar datos personales de manera que afecte a derechos fundamentales como la igualdad o la no discriminaci√≥n.\n",
            "\n",
            "--- M√âTRICAS ---\n",
            "Modelo: hf.co/serdom02/model_q4_k_mGGUF\n",
            "Tokens generados: 169\n",
            "Tiempo total (incluye carga): 14.33 s\n",
            "Tiempo de inferencia (solo generaci√≥n): 14.23 s\n",
            "Velocidad de generaci√≥n: 11.88 tokens/seg\n",
            "RAM usada: 0.45 GB\n",
            "Uso CPU promedio durante inferencia: 66.4%\n",
            "VRAM usada: 5.97 GB\n",
            "Carga GPU: 52.0%\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import psutil\n",
        "import GPUtil\n",
        "from statistics import mean\n",
        "import threading\n",
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# --- Reiniciar Ollama ---\n",
        "!kill -9 $(ps aux | grep '[o]llama' | awk '{print $2}')\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "time.sleep(3)  # Esperar a que Ollama est√© listo\n",
        "\n",
        "# --- Configuraci√≥n de prueba ---\n",
        "model_name = \"hf.co/serdom02/model_q4_k_mGGUF\"\n",
        "\n",
        "# --- Monitorear uso de RAM ---\n",
        "def get_system_metrics():\n",
        "    ram_used = psutil.virtual_memory().used / (1024 ** 3)  # GB\n",
        "    return ram_used\n",
        "\n",
        "# --- Monitorear uso de GPU (VRAM) ---\n",
        "def get_gpu_metrics():\n",
        "    gpus = GPUtil.getGPUs()\n",
        "    if gpus:\n",
        "        gpu = gpus[0]\n",
        "        return gpu.memoryUsed / 1024, gpu.load * 100  # GB, %\n",
        "    return 0, 0\n",
        "\n",
        "# --- Iniciar modelo ---\n",
        "llm = OllamaLLM(model=model_name, max_tokens=250)\n",
        "\n",
        "# --- Medici√≥n de rendimiento ---\n",
        "start_total = time.time()\n",
        "\n",
        "ram_before = get_system_metrics()\n",
        "gpu_mem_before, gpu_load_before = get_gpu_metrics()\n",
        "\n",
        "# --- Medici√≥n activa de CPU en paralelo ---\n",
        "cpu_usage_during_infer = []\n",
        "stop_cpu_monitor = False\n",
        "\n",
        "def monitor_cpu():\n",
        "    while not stop_cpu_monitor:\n",
        "        cpu = psutil.cpu_percent(interval=0.1)\n",
        "        cpu_usage_during_infer.append(cpu)\n",
        "\n",
        "cpu_thread = threading.Thread(target=monitor_cpu)\n",
        "cpu_thread.start()\n",
        "\n",
        "# --- Inferencia ---\n",
        "start_infer = time.time()\n",
        "response = llm.invoke(pregunta)\n",
        "end_infer = time.time()\n",
        "\n",
        "# --- Detener CPU monitor ---\n",
        "stop_cpu_monitor = True\n",
        "cpu_thread.join()\n",
        "\n",
        "ram_after = get_system_metrics()\n",
        "gpu_mem_after, gpu_load_after = get_gpu_metrics()\n",
        "end_total = time.time()\n",
        "\n",
        "# --- M√©tricas calculadas ---\n",
        "total_time = end_total - start_total\n",
        "infer_time = end_infer - start_infer\n",
        "token_count = len(response.split())\n",
        "avg_cpu = mean(cpu_usage_during_infer) if cpu_usage_during_infer else 0\n",
        "\n",
        "# --- Salida ---\n",
        "print(\"\\n--- RESPUESTA ---\")\n",
        "print(response)\n",
        "\n",
        "print(\"\\n--- M√âTRICAS ---\")\n",
        "print(f\"Modelo: {model_name}\")\n",
        "print(f\"Tokens generados: {token_count}\")\n",
        "print(f\"Tiempo total (incluye carga): {total_time:.2f} s\")\n",
        "print(f\"Tiempo de inferencia (solo generaci√≥n): {infer_time:.2f} s\")\n",
        "print(f\"Velocidad de generaci√≥n: {token_count / infer_time:.2f} tokens/seg\")\n",
        "print(f\"RAM usada: {ram_after - ram_before:.2f} GB\")\n",
        "print(f\"Uso CPU promedio durante inferencia: {avg_cpu:.1f}%\")\n",
        "print(f\"VRAM usada: {gpu_mem_after - gpu_mem_before:.2f} GB\")\n",
        "print(f\"Carga GPU: {gpu_load_after:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWMJxQ09X0f2"
      },
      "source": [
        "## Finetunning + RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFcUQV9HX2sX"
      },
      "source": [
        "### 8bit Q8_0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sA2brIQWYDVE",
        "outputId": "e08ec015-5955-454d-ba1a-b88bdb1ab1e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "Asistente legal de protecci√≥n de datos listo (usando load_qa_chain). Escribe 'Exit' para salir.\n",
            "--------------------------------------------------\n",
            "\n",
            "Respuesta:\n",
            "Sinosuque podr√≠a enfrentarse a multas econ√≥micas por vulneraci√≥n de la protecci√≥n de datos, ya que publicar informaci√≥n personal como el DNI sin consentimiento del afectado constituye una infracci√≥n grave seg√∫n el RGPD. Adem√°s, el insulto en l√≠nea puede ser considerado difamaci√≥n si afecta a la reputaci√≥n de las personas agraviadas.\n",
            "\n",
            "Saliendo del asistente.\n"
          ]
        }
      ],
      "source": [
        "#Para resetear ollama\n",
        "!kill -9 $(ps aux | grep '[o]llama' | awk '{print $2}')\n",
        "\n",
        "import subprocess\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3) # Esperar a que Ollama se cargue\n",
        "\n",
        "\n",
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# Initialize the LLaMA model\n",
        "llm = OllamaLLM(model=\"hf.co/serdom02/model_8bitQ8_0\") #aqui poinemos el modelo base\n",
        "\n",
        "# Importaci√≥n correcta para versiones recientes de Langchain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "import torch\n",
        "\n",
        "\n",
        "# Define la plantilla del prompt DETALLADA\n",
        "prompt_template = \"\"\"\n",
        "Eres un asistente legal experto en protecci√≥n de datos en Espa√±a.\n",
        "Tu tarea es analizar la legalidad de la situaci√≥n descrita en la 'Pregunta' bas√°ndote **principalmente** en los siguientes 'Textos Legales Recuperados'.\n",
        "Si los textos recuperados contienen suficiente informaci√≥n, responde √∫nicamente bas√°ndote en ellos.\n",
        "Si los textos no contienen una respuesta clara pero la pregunta se refiere a conceptos b√°sicos del RGPD, usa lo aprendido en el entrenamiento para dar una respuesta general, especificando que no proviene de los textos recuperados.\n",
        "Si la pregunta requiere informaci√≥n espec√≠fica que no est√° en los textos recuperados ni en tu entrenamiento, ind√≠calo claramente.\n",
        "\n",
        "Textos Legales Recuperados:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "An√°lisis Legal y Conclusi√≥n:\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Crea la cadena usando load_qa_chain con el prompt personalizado\n",
        "# chain_type=\"stuff\" es adecuado si los chunks recuperados + pregunta caben en la ventana de contexto del LLM.\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=PROMPT)\n",
        "\n",
        "print (\"-\"*50)\n",
        "print (\"Asistente legal de protecci√≥n de datos listo (usando load_qa_chain). Escribe 'Exit' para salir.\")\n",
        "print (\"-\"*50)\n",
        "\n",
        "\n",
        "query=pregunta\n",
        "# 1. Recupera los documentos relevantes\n",
        "# Se puede ajustar 'k' aqu√≠ si quieres m√°s/menos contexto: retriever.search_kwargs = {'k': 5}\n",
        "docs_retrieved = retriever.invoke(query)\n",
        "\n",
        "# Prepara el input para la cadena\n",
        "input_data = {\n",
        "    \"input_documents\": docs_retrieved,\n",
        "    \"question\": query\n",
        "}\n",
        "\n",
        "# 2. Ejecuta la cadena de generaci√≥n con los documentos recuperados y la pregunta\n",
        "# Si usas una versi√≥n de Langchain > 0.1.0, invoke devuelve un diccionario.\n",
        "# Si usas una versi√≥n anterior, podr√≠a devolver directamente el string.\n",
        "# El par√°metro return_only_outputs=True ya no es necesario/v√°lido en invoke para versiones > 0.1.0\n",
        "result = chain.invoke(input_data)\n",
        "\n",
        "# Imprime la salida del LLM (la clave suele ser 'output_text' en versiones recientes)\n",
        "if isinstance(result, dict) and 'output_text' in result:\n",
        "    print(\"\\nRespuesta:\")\n",
        "    print(result['output_text'])\n",
        "elif isinstance(result, str): # Compatibilidad con versiones anteriores\n",
        "      print(\"\\nRespuesta:\")\n",
        "      print(result)\n",
        "else:\n",
        "      print(\"\\nRespuesta recibida (formato inesperado):\")\n",
        "      print(result)\n",
        "\n",
        "\n",
        "print(\"\\nSaliendo del asistente.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz5jjApKYDsc"
      },
      "source": [
        "### 16bit GGUF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLKZ3HsTYG4l",
        "outputId": "d3f4a2f0-d956-4aa4-d832-bbaeacbf7b15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "Asistente legal de protecci√≥n de datos listo (usando load_qa_chain). Escribe 'Exit' para salir.\n",
            "--------------------------------------------------\n",
            "\n",
            "Respuesta:\n",
            "Sinosuque podr√≠a enfrentarse a varias sanciones por violaci√≥n de la protecci√≥n de datos. Al compartir el DNI y las notas acad√©micas del compa√±ero sin consentimiento, vulner√≥ el principio de minimizaci√≥n de datos personales (RGPD, art. 5). Adem√°s, la difusi√≥n de informaci√≥n privada con fines vejatorios constituir√≠a acoso online (Ley Org√°nica 3/2018), mientras que la publicaci√≥n de datos bancarios sin justificaci√≥n podr√≠a ser considerada revelaci√≥n indebida (RGPD, art. 4).\n",
            "\n",
            "Saliendo del asistente.\n"
          ]
        }
      ],
      "source": [
        "#Para resetear ollama\n",
        "!kill -9 $(ps aux | grep '[o]llama' | awk '{print $2}')\n",
        "\n",
        "import subprocess\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3) # Esperar a que Ollama se cargue\n",
        "\n",
        "\n",
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# Initialize the LLaMA model\n",
        "llm = OllamaLLM(model=\"hf.co/serdom02/model_16bitGGUF\") #aqui poinemos el modelo base\n",
        "\n",
        "# Importaci√≥n correcta para versiones recientes de Langchain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "import torch\n",
        "\n",
        "\n",
        "# Define la plantilla del prompt DETALLADA\n",
        "prompt_template = \"\"\"\n",
        "Eres un asistente legal experto en protecci√≥n de datos en Espa√±a.\n",
        "Tu tarea es analizar la legalidad de la situaci√≥n descrita en la 'Pregunta' bas√°ndote **principalmente** en los siguientes 'Textos Legales Recuperados'.\n",
        "Si los textos recuperados contienen suficiente informaci√≥n, responde √∫nicamente bas√°ndote en ellos.\n",
        "Si los textos no contienen una respuesta clara pero la pregunta se refiere a conceptos b√°sicos del RGPD, usa lo aprendido en el entrenamiento para dar una respuesta general, especificando que no proviene de los textos recuperados.\n",
        "Si la pregunta requiere informaci√≥n espec√≠fica que no est√° en los textos recuperados ni en tu entrenamiento, ind√≠calo claramente.\n",
        "\n",
        "Textos Legales Recuperados:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "An√°lisis Legal y Conclusi√≥n:\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Crea la cadena usando load_qa_chain con el prompt personalizado\n",
        "# chain_type=\"stuff\" es adecuado si los chunks recuperados + pregunta caben en la ventana de contexto del LLM.\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=PROMPT)\n",
        "\n",
        "print (\"-\"*50)\n",
        "print (\"Asistente legal de protecci√≥n de datos listo (usando load_qa_chain). Escribe 'Exit' para salir.\")\n",
        "print (\"-\"*50)\n",
        "\n",
        "\n",
        "query=pregunta\n",
        "# 1. Recupera los documentos relevantes\n",
        "# Se puede ajustar 'k' aqu√≠ si quieres m√°s/menos contexto: retriever.search_kwargs = {'k': 5}\n",
        "docs_retrieved = retriever.invoke(query)\n",
        "\n",
        "# Prepara el input para la cadena\n",
        "input_data = {\n",
        "    \"input_documents\": docs_retrieved,\n",
        "    \"question\": query\n",
        "}\n",
        "\n",
        "# 2. Ejecuta la cadena de generaci√≥n con los documentos recuperados y la pregunta\n",
        "# Si usas una versi√≥n de Langchain > 0.1.0, invoke devuelve un diccionario.\n",
        "# Si usas una versi√≥n anterior, podr√≠a devolver directamente el string.\n",
        "# El par√°metro return_only_outputs=True ya no es necesario/v√°lido en invoke para versiones > 0.1.0\n",
        "result = chain.invoke(input_data)\n",
        "\n",
        "# Imprime la salida del LLM (la clave suele ser 'output_text' en versiones recientes)\n",
        "if isinstance(result, dict) and 'output_text' in result:\n",
        "    print(\"\\nRespuesta:\")\n",
        "    print(result['output_text'])\n",
        "elif isinstance(result, str): # Compatibilidad con versiones anteriores\n",
        "      print(\"\\nRespuesta:\")\n",
        "      print(result)\n",
        "else:\n",
        "      print(\"\\nRespuesta recibida (formato inesperado):\")\n",
        "      print(result)\n",
        "\n",
        "\n",
        "print(\"\\nSaliendo del asistente.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QYAWjBEYFDn"
      },
      "source": [
        "### q4_k_m GGUF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dL5mYhqYEzo",
        "outputId": "8bcb88b2-d6cd-46a1-e5a9-0cd6794e824d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "Asistente legal de protecci√≥n de datos listo (usando load_qa_chain). Escribe 'Exit' para salir.\n",
            "--------------------------------------------------\n",
            "\n",
            "Respuesta:\n",
            "Sinosuque podr√≠a enfrentarse a varios delitos, como:\n",
            "\n",
            "* V√≠a de difusi√≥n no consentida (art√≠culo 7.Ley Org√°nica 3/1989): Al publicar las notas personales de su compa√±ero sin autorizaci√≥n, Sinosuque estuvo vulnerando su derecho a la protecci√≥n de datos.\n",
            "* Acoso o vejigio (art√≠culos 489 y siguientes del C√≥digo Penal): Si las publicaciones inclu√≠an contenido ofensivo o humillante, Sinosuque podr√≠a ser sancionado por acoso o vejigio.\n",
            "* Uso indebido de datos personales (art√≠culo 11.Ley Org√°nica 3/1989): Al compartir informaci√≥n sensible como el DNI sin consentimiento, Sinosuque incumpli√≥ con la normativa de protecci√≥n de datos.\n",
            "\n",
            "Es importante recordar que cualquier persona tiene derecho a la protecci√≥n de sus datos personales y a solicitar su supresi√≥n o limitaci√≥n en los casos previstos por la ley.\n",
            "\n",
            "Saliendo del asistente.\n"
          ]
        }
      ],
      "source": [
        "#Para resetear ollama\n",
        "!kill -9 $(ps aux | grep '[o]llama' | awk '{print $2}')\n",
        "\n",
        "import subprocess\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3) # Esperar a que Ollama se cargue\n",
        "\n",
        "\n",
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# Initialize the LLaMA model\n",
        "llm = OllamaLLM(model=\"hf.co/serdom02/model_q4_k_mGGUF\") #aqui poinemos el modelo base\n",
        "\n",
        "# Importaci√≥n correcta para versiones recientes de Langchain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "import torch\n",
        "\n",
        "\n",
        "# Define la plantilla del prompt DETALLADA\n",
        "prompt_template = \"\"\"\n",
        "Eres un asistente legal experto en protecci√≥n de datos en Espa√±a.\n",
        "Tu tarea es analizar la legalidad de la situaci√≥n descrita en la 'Pregunta' bas√°ndote **principalmente** en los siguientes 'Textos Legales Recuperados'.\n",
        "Si los textos recuperados contienen suficiente informaci√≥n, responde √∫nicamente bas√°ndote en ellos.\n",
        "Si los textos no contienen una respuesta clara pero la pregunta se refiere a conceptos b√°sicos del RGPD, usa lo aprendido en el entrenamiento para dar una respuesta general, especificando que no proviene de los textos recuperados.\n",
        "Si la pregunta requiere informaci√≥n espec√≠fica que no est√° en los textos recuperados ni en tu entrenamiento, ind√≠calo claramente.\n",
        "\n",
        "Textos Legales Recuperados:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "An√°lisis Legal y Conclusi√≥n:\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Crea la cadena usando load_qa_chain con el prompt personalizado\n",
        "# chain_type=\"stuff\" es adecuado si los chunks recuperados + pregunta caben en la ventana de contexto del LLM.\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=PROMPT)\n",
        "\n",
        "print (\"-\"*50)\n",
        "print (\"Asistente legal de protecci√≥n de datos listo (usando load_qa_chain). Escribe 'Exit' para salir.\")\n",
        "print (\"-\"*50)\n",
        "\n",
        "\n",
        "query=pregunta\n",
        "# 1. Recupera los documentos relevantes\n",
        "# Se puede ajustar 'k' aqu√≠ si quieres m√°s/menos contexto: retriever.search_kwargs = {'k': 5}\n",
        "docs_retrieved = retriever.invoke(query)\n",
        "\n",
        "# Prepara el input para la cadena\n",
        "input_data = {\n",
        "    \"input_documents\": docs_retrieved,\n",
        "    \"question\": query\n",
        "}\n",
        "\n",
        "# 2. Ejecuta la cadena de generaci√≥n con los documentos recuperados y la pregunta\n",
        "# Si usas una versi√≥n de Langchain > 0.1.0, invoke devuelve un diccionario.\n",
        "# Si usas una versi√≥n anterior, podr√≠a devolver directamente el string.\n",
        "# El par√°metro return_only_outputs=True ya no es necesario/v√°lido en invoke para versiones > 0.1.0\n",
        "result = chain.invoke(input_data)\n",
        "\n",
        "# Imprime la salida del LLM (la clave suele ser 'output_text' en versiones recientes)\n",
        "if isinstance(result, dict) and 'output_text' in result:\n",
        "    print(\"\\nRespuesta:\")\n",
        "    print(result['output_text'])\n",
        "elif isinstance(result, str): # Compatibilidad con versiones anteriores\n",
        "      print(\"\\nRespuesta:\")\n",
        "      print(result)\n",
        "else:\n",
        "      print(\"\\nRespuesta recibida (formato inesperado):\")\n",
        "      print(result)\n",
        "\n",
        "\n",
        "print(\"\\nSaliendo del asistente.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVn5G4j93f2x"
      },
      "source": [
        "# Servidor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VzaBVm53lVJ"
      },
      "source": [
        "En este apartado se configura el backend que da servicio a la pagina web que usa el usuario para interactuar con el modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9r5qu7R3lwa"
      },
      "source": [
        "## Paso 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQ3wKgOb3oN_"
      },
      "source": [
        "Creamos la funci√≥n que vamos a utilizar para interactuar con el modelo (Hay que ejecutar antes las celdas de la parte Aplicaci√≥n Final Mejorada)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1In3ms-6fe-V"
      },
      "source": [
        "Se Asocia una instancia de ImprovedMemory a cada nombre de usuario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwsZfjXJk6x-"
      },
      "outputs": [],
      "source": [
        "def responder_web(pregunta: str, usuario: str, app) -> str:\n",
        "    try:\n",
        "        sesiones = app.state.user_sessions\n",
        "        usuario = usuario.strip().lower()\n",
        "\n",
        "        # Obtener o crear sesi√≥n del usuario\n",
        "        if usuario not in sesiones:\n",
        "            sesiones[usuario] = UsuarioSession()\n",
        "\n",
        "        sesion = sesiones[usuario]\n",
        "\n",
        "        # Inyectar el last_query en get_response_with_retry (requiere adaptaci√≥n)\n",
        "        result = get_response_with_retry(\n",
        "            query=pregunta,\n",
        "            llm=llm,\n",
        "            base_retriever=base_retriever,\n",
        "            memory=sesion.memory,\n",
        "            last_query=sesion.last_query\n",
        "        )\n",
        "\n",
        "        # Actualizar last_query\n",
        "        sesion.last_query = pregunta\n",
        "\n",
        "        return result.get(\"response\", \"No se pudo generar una respuesta.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error en responder_web: {e}\", exc_info=True)\n",
        "        return f\"Ocurri√≥ un error al procesar tu consulta: {e}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rw4mRKzz36rJ"
      },
      "source": [
        "## Paso 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E448E2T39bL"
      },
      "source": [
        "Instalamos la dependencias y creamos la API usando FastAPI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLqVQo1j37cT",
        "outputId": "5f85d1c5-50e3-47e9-a1e7-d99e69fdd2dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: fastapi in c:\\programdata\\anaconda3\\lib\\site-packages (0.115.12)\n",
            "Requirement already satisfied: uvicorn in c:\\programdata\\anaconda3\\lib\\site-packages (0.34.2)\n",
            "Requirement already satisfied: nest_asyncio in c:\\programdata\\anaconda3\\lib\\site-packages (1.6.0)\n",
            "Requirement already satisfied: pyngrok in c:\\programdata\\anaconda3\\lib\\site-packages (7.2.5)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from fastapi) (0.46.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from fastapi) (2.10.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: click>=7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from uvicorn) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyngrok) (6.0.1)\n",
            "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click>=7.0->uvicorn) (0.4.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.2.0)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.7)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi uvicorn nest_asyncio pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdBIkRHy4B_y",
        "outputId": "e4fc5395-801b-4416-a8b0-d6b26db3c710"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 15:01:05,891 - pyngrok.ngrok - INFO - Opening tunnel named: http-8000-5ce72bd7-578c-4bd1-9b78-8465225c6ca9\n",
            "2025-05-08 15:01:05,959 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:01:05+0200 lvl=info msg=\"no configuration paths supplied\"\n",
            "2025-05-08 15:01:05,961 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:01:05+0200 lvl=info msg=\"using configuration at default config path\" path=C:\\\\Users\\\\sdominguez\\\\AppData\\\\Local/ngrok/ngrok.yml\n",
            "2025-05-08 15:01:05,963 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:01:05+0200 lvl=info msg=\"open config file\" path=C:\\\\Users\\\\sdominguez\\\\AppData\\\\Local\\\\ngrok\\\\ngrok.yml err=nil\n",
            "2025-05-08 15:01:05,965 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:01:05+0200 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\n",
            "2025-05-08 15:01:06,336 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:01:06+0200 lvl=info msg=\"client session established\" obj=tunnels.session\n",
            "2025-05-08 15:01:06,337 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:01:06+0200 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "2025-05-08 15:01:06,642 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:01:06+0200 lvl=info msg=start pg=/api/tunnels id=fbe052db02c42d39\n",
            "2025-05-08 15:01:06,648 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:01:06+0200 lvl=info msg=end pg=/api/tunnels id=fbe052db02c42d39 status=200 dur=0s\n",
            "2025-05-08 15:01:06,651 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:01:06+0200 lvl=info msg=start pg=/api/tunnels id=628925aeebf17b2b\n",
            "2025-05-08 15:01:06,653 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:01:06+0200 lvl=info msg=end pg=/api/tunnels id=628925aeebf17b2b status=200 dur=0s\n",
            "2025-05-08 15:01:06,655 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:01:06+0200 lvl=info msg=start pg=/api/tunnels id=efd96a404d8805c8\n",
            "2025-05-08 15:01:06,919 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:01:06+0200 lvl=info msg=\"started tunnel\" obj=tunnels name=http-8000-5ce72bd7-578c-4bd1-9b78-8465225c6ca9 addr=http://localhost:8000 url=https://2c8f-138-100-68-196.ngrok-free.app\n",
            "2025-05-08 15:01:06,922 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:01:06+0200 lvl=info msg=end pg=/api/tunnels id=efd96a404d8805c8 status=201 dur=268.3244ms\n",
            "INFO:     Started server process [6096]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tu servidor est√° disponible en: NgrokTunnel: \"https://2c8f-138-100-68-196.ngrok-free.app\" -> \"http://localhost:8000\"\n",
            "INFO:     138.100.68.196:0 - \"OPTIONS /chat HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 15:02:31,080 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:02:31+0200 lvl=info msg=\"join connections\" obj=join id=66ec1d75393b l=127.0.0.1:8000 r=138.100.68.196:56615\n",
            "2025-05-08 15:02:31,342 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu\n",
            "2025-05-08 15:02:31,344 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-mpnet-base-v2\n",
            "2025-05-08 15:02:38,091 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:02:38,234 - asistente_legal - INFO - Clasificaci√≥n: GENERAL_INFO, Confianza: 50\n",
            "2025-05-08 15:02:38,235 - asistente_legal - INFO - Intenci√≥n clasificada como: GENERAL_INFO (Confianza: 50%) - Usar RAG: True\n",
            "2025-05-08 15:02:38,237 - asistente_legal - INFO - Usando el prompt: general_info_prompt\n",
            "2025-05-08 15:02:38,240 - asistente_legal - INFO - Intento RAG 1 para consulta: 'porque' (Categor√≠a: GENERAL_INFO)\n",
            "C:\\Users\\sdominguez\\AppData\\Local\\Temp\\ipykernel_6096\\1207801112.py:564: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  retrieved_docs = retriever.get_relevant_documents(query)\n",
            "2025-05-08 15:02:38,301 - asistente_legal - INFO - Recuperados 5 documentos.\n",
            "C:\\Users\\sdominguez\\AppData\\Local\\Temp\\ipykernel_6096\\1207801112.py:449: LangChainDeprecationWarning: This class is deprecated. See the following migration guides for replacements based on `chain_type`:\n",
            "stuff: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain\n",
            "map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\n",
            "refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\n",
            "map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\n",
            "\n",
            "See also guides on retrieval and question-answering here: https://python.langchain.com/docs/how_to/#qa-with-rag\n",
            "  qa_chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=prompt, verbose=False) # verbose=True para debug\n",
            "2025-05-08 15:02:45,725 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:02:48,315 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:02:50,162 - asistente_legal - INFO - Validaci√≥n: {'valid': False, 'precision': False, 'completitud': None, 'sin_errores': None, 'raw_validation_output': '[PRECISI√ìN: No, la normativa es m√°s amplia que solo el consentimiento o una necesidad leg√≠tima. Existen otras bases legales como inter√©s p√∫blico, obligaci√≥n legal o intereses empresariales.]'}\n",
            "2025-05-08 15:02:50,164 - asistente_legal - WARNING - Respuesta no v√°lida en intento 1: {'valid': False, 'precision': False, 'completitud': None, 'sin_errores': None, 'raw_validation_output': '[PRECISI√ìN: No, la normativa es m√°s amplia que solo el consentimiento o una necesidad leg√≠tima. Existen otras bases legales como inter√©s p√∫blico, obligaci√≥n legal o intereses empresariales.]'}\n",
            "2025-05-08 15:02:50,166 - asistente_legal - INFO - Usando el prompt: general_info_prompt\n",
            "2025-05-08 15:02:50,168 - asistente_legal - INFO - Prompt adaptado con sugerencias del validador.\n",
            "2025-05-08 15:02:50,169 - asistente_legal - INFO - Intento RAG 2 para consulta: 'porque' (Categor√≠a: GENERAL_INFO)\n",
            "2025-05-08 15:02:50,172 - asistente_legal - INFO - Reintento 1: aumentando k y activando reranking.\n",
            "2025-05-08 15:02:50,211 - asistente_legal - INFO - Recuperados 8 documentos.\n",
            "2025-05-08 15:02:50,212 - asistente_legal - INFO - Iniciando reranking para 8 documentos recuperados...\n",
            "2025-05-08 15:02:50,529 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:02:50,714 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:02:50,995 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:02:51,291 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:02:51,575 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:02:51,890 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:02:52,172 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:02:52,485 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:02:52,527 - asistente_legal - INFO - Reranking completado. 4 documentos seleccionados.\n",
            "2025-05-08 15:02:52,528 - asistente_legal - INFO - Documentos despu√©s de reranking: 4\n",
            "2025-05-08 15:02:54,606 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:02:56,007 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:02:59,985 - asistente_legal - INFO - Validaci√≥n: {'valid': False, 'precision': True, 'completitud': False, 'sin_errores': False, 'raw_validation_output': '[PRECISI√ìN: S√≠, la respuesta refleja el principio de transparencia del RGPD y la LOPDGDD.], [COMPLETITUD: No, no explica qu√© se entiende por \"inter√©s leg√≠timo\" o c√≥mo se garantiza la protecci√≥n de datos en cada caso concreto.], [ERRORES: S√≠, omite detalles clave sobre los derechos del usuario y no ofrece orientaci√≥n espec√≠fica para situaciones complejas.]'}\n",
            "2025-05-08 15:02:59,987 - asistente_legal - WARNING - Respuesta no v√°lida en intento 2: {'valid': False, 'precision': True, 'completitud': False, 'sin_errores': False, 'raw_validation_output': '[PRECISI√ìN: S√≠, la respuesta refleja el principio de transparencia del RGPD y la LOPDGDD.], [COMPLETITUD: No, no explica qu√© se entiende por \"inter√©s leg√≠timo\" o c√≥mo se garantiza la protecci√≥n de datos en cada caso concreto.], [ERRORES: S√≠, omite detalles clave sobre los derechos del usuario y no ofrece orientaci√≥n espec√≠fica para situaciones complejas.]'}\n",
            "2025-05-08 15:02:59,988 - asistente_legal - INFO - Usando el prompt: general_info_prompt\n",
            "2025-05-08 15:02:59,990 - asistente_legal - INFO - Prompt adaptado con sugerencias del validador.\n",
            "2025-05-08 15:02:59,993 - asistente_legal - INFO - Intento RAG 3 para consulta: 'porque' (Categor√≠a: GENERAL_INFO)\n",
            "2025-05-08 15:02:59,995 - asistente_legal - INFO - Reintento 2: creando un retriever m√°s flexible (MMR + reformulaci√≥n posible).\n",
            "2025-05-08 15:03:00,037 - asistente_legal - INFO - Recuperados 10 documentos.\n",
            "2025-05-08 15:03:00,037 - asistente_legal - INFO - Iniciando reranking para 10 documentos recuperados...\n",
            "2025-05-08 15:03:00,352 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:03:00,672 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:03:00,986 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:03:01,264 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:03:01,550 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:03:01,759 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:03:02,037 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:03:02,336 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:03:02,378 - asistente_legal - INFO - Reranking completado. 4 documentos seleccionados.\n",
            "2025-05-08 15:03:02,379 - asistente_legal - INFO - Documentos despu√©s de reranking: 4\n",
            "2025-05-08 15:03:05,920 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:03:07,467 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:03:08,387 - asistente_legal - INFO - Validaci√≥n: {'valid': False, 'precision': False, 'completitud': False, 'sin_errores': False, 'raw_validation_output': '[PRECISI√ìN: No], [COMPLETITUD: No], [ERRORES: S√≠]'}\n",
            "2025-05-08 15:03:08,388 - asistente_legal - WARNING - Respuesta no v√°lida en intento 3: {'valid': False, 'precision': False, 'completitud': False, 'sin_errores': False, 'raw_validation_output': '[PRECISI√ìN: No], [COMPLETITUD: No], [ERRORES: S√≠]'}\n",
            "2025-05-08 15:03:08,390 - asistente_legal - INFO - Iniciando auto-refinamiento de respuesta con feedback del validador.\n",
            "C:\\Users\\sdominguez\\AppData\\Local\\Temp\\ipykernel_6096\\1207801112.py:636: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  refinement_chain = LLMChain(llm=llm, prompt=refinement_prompt)\n",
            "2025-05-08 15:03:08,749 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     138.100.68.196:0 - \"POST /chat HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 15:03:21,949 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:03:21+0200 lvl=info msg=\"join connections\" obj=join id=7f62c61d1a9b l=127.0.0.1:8000 r=138.100.68.196:56662\n",
            "2025-05-08 15:03:21,959 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu\n",
            "2025-05-08 15:03:21,960 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-mpnet-base-v2\n",
            "2025-05-08 15:03:24,355 - asistente_legal - INFO - Detectada pregunta de seguimiento. Incorporando √∫ltima consulta como contexto.\n",
            "2025-05-08 15:03:28,762 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:03:28,930 - asistente_legal - INFO - Clasificaci√≥n: CONVERSATION, Confianza: 99\n",
            "2025-05-08 15:03:28,931 - asistente_legal - INFO - Intenci√≥n clasificada como: CONVERSATION (Confianza: 99%) - Usar RAG: False\n",
            "2025-05-08 15:03:29,167 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     138.100.68.196:0 - \"POST /chat HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 15:04:32,387 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:04:32+0200 lvl=info msg=\"join connections\" obj=join id=51d850ccb46b l=127.0.0.1:8000 r=138.100.68.196:56662\n",
            "2025-05-08 15:04:32,399 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu\n",
            "2025-05-08 15:04:32,401 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-mpnet-base-v2\n",
            "2025-05-08 15:04:36,319 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:04:36,479 - asistente_legal - INFO - Clasificaci√≥n: CONVERSATION, Confianza: 98\n",
            "2025-05-08 15:04:36,481 - asistente_legal - INFO - Intenci√≥n clasificada como: CONVERSATION (Confianza: 98%) - Usar RAG: False\n",
            "2025-05-08 15:04:36,745 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     138.100.68.196:0 - \"POST /chat HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 15:04:52,680 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:04:52+0200 lvl=info msg=\"join connections\" obj=join id=9de6fce4ebb1 l=127.0.0.1:8000 r=138.100.68.196:56662\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     138.100.68.196:0 - \"OPTIONS /reset HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 15:04:52,934 - asistente_legal - INFO - Solicitud de reseteo para: sergio\n",
            "2025-05-08 15:04:52,936 - asistente_legal - INFO - Memoria borrada para sergio\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     138.100.68.196:0 - \"POST /reset HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 15:04:58,768 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:04:58+0200 lvl=info msg=\"join connections\" obj=join id=0b467bb5f2be l=127.0.0.1:8000 r=138.100.68.196:56662\n",
            "2025-05-08 15:04:58,778 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu\n",
            "2025-05-08 15:04:58,779 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-mpnet-base-v2\n",
            "2025-05-08 15:05:01,120 - asistente_legal - INFO - Resultado de clasificaci√≥n obtenido de cach√© para: di mi nombre, quiero saber si lo has podido recordar\n",
            "2025-05-08 15:05:01,121 - asistente_legal - INFO - Intenci√≥n clasificada como: CONVERSATION (Confianza: 98%) - Usar RAG: False\n",
            "2025-05-08 15:05:01,660 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     138.100.68.196:0 - \"POST /chat HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 15:05:18,721 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:05:18+0200 lvl=info msg=\"join connections\" obj=join id=1e4e84cd385f l=127.0.0.1:8000 r=138.100.68.196:56662\n",
            "2025-05-08 15:05:18,731 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu\n",
            "2025-05-08 15:05:18,733 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-mpnet-base-v2\n",
            "2025-05-08 15:05:22,445 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:05:22,614 - asistente_legal - INFO - Clasificaci√≥n: LEGAL_ANALYSIS, Confianza: 92\n",
            "2025-05-08 15:05:22,615 - asistente_legal - INFO - Intenci√≥n clasificada como: LEGAL_ANALYSIS (Confianza: 92%) - Usar RAG: True\n",
            "2025-05-08 15:05:22,617 - asistente_legal - INFO - Usando el prompt: legal_analysis_prompt\n",
            "2025-05-08 15:05:22,619 - asistente_legal - INFO - Intento RAG 1 para consulta: 'Un gimnasio quiere implementar un sistema de acceso mediante huella dactilar para sus socios. Planean incluir una cl√°usula en el contrato de inscripci√≥n donde el socio 'acepta el uso de sus datos biom√©tricos para el control de acceso'. ¬øEs esta forma de obtener el consentimiento v√°lida seg√∫n el RGPD y la LOPDGDD, considerando que los datos biom√©tricos son una categor√≠a especial de datos? Justifica la respuesta con los art√≠culos pertinentes que encuentres.' (Categor√≠a: LEGAL_ANALYSIS)\n",
            "2025-05-08 15:05:22,696 - asistente_legal - INFO - Recuperados 5 documentos.\n",
            "2025-05-08 15:05:32,124 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:05:43,761 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:05:48,295 - asistente_legal - INFO - Validaci√≥n: {'valid': False, 'precision': False, 'completitud': False, 'sin_errores': False, 'raw_validation_output': '[PRECISI√ìN: No, la normativa exige una informaci√≥n detallada y proporcional sobre el tratamiento de datos biom√©tricos, no solo su inclusi√≥n en un contrato.] [COMPLETITUD: No, la respuesta no explica espec√≠ficamente c√≥mo informar a los socios sobre sus derechos con estos datos sensibles.] [ERRORES: S√≠, se omite que el consentimiento debe ser libremente otorgado, y no se mencionan las garant√≠as de seguridad adicionales para proteger estos datos.]'}\n",
            "2025-05-08 15:05:48,297 - asistente_legal - WARNING - Respuesta no v√°lida en intento 1: {'valid': False, 'precision': False, 'completitud': False, 'sin_errores': False, 'raw_validation_output': '[PRECISI√ìN: No, la normativa exige una informaci√≥n detallada y proporcional sobre el tratamiento de datos biom√©tricos, no solo su inclusi√≥n en un contrato.] [COMPLETITUD: No, la respuesta no explica espec√≠ficamente c√≥mo informar a los socios sobre sus derechos con estos datos sensibles.] [ERRORES: S√≠, se omite que el consentimiento debe ser libremente otorgado, y no se mencionan las garant√≠as de seguridad adicionales para proteger estos datos.]'}\n",
            "2025-05-08 15:05:48,299 - asistente_legal - INFO - Usando el prompt: legal_analysis_prompt\n",
            "2025-05-08 15:05:48,302 - asistente_legal - INFO - Prompt adaptado con sugerencias del validador.\n",
            "2025-05-08 15:05:48,304 - asistente_legal - INFO - Intento RAG 2 para consulta: 'Un gimnasio quiere implementar un sistema de acceso mediante huella dactilar para sus socios. Planean incluir una cl√°usula en el contrato de inscripci√≥n donde el socio 'acepta el uso de sus datos biom√©tricos para el control de acceso'. ¬øEs esta forma de obtener el consentimiento v√°lida seg√∫n el RGPD y la LOPDGDD, considerando que los datos biom√©tricos son una categor√≠a especial de datos? Justifica la respuesta con los art√≠culos pertinentes que encuentres.' (Categor√≠a: LEGAL_ANALYSIS)\n",
            "2025-05-08 15:05:48,306 - asistente_legal - INFO - Reintento 1: aumentando k y activando reranking.\n",
            "2025-05-08 15:05:48,371 - asistente_legal - INFO - Recuperados 8 documentos.\n",
            "2025-05-08 15:05:48,372 - asistente_legal - INFO - Iniciando reranking para 8 documentos recuperados...\n",
            "2025-05-08 15:05:48,740 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:05:49,048 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:05:49,342 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:05:49,627 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:05:49,922 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:05:50,216 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:05:50,527 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:05:50,817 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:05:50,863 - asistente_legal - INFO - Reranking completado. 4 documentos seleccionados.\n",
            "2025-05-08 15:05:50,865 - asistente_legal - INFO - Documentos despu√©s de reranking: 4\n",
            "2025-05-08 15:05:54,427 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:06:08,734 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:06:12,520 - asistente_legal - INFO - Validaci√≥n: {'valid': False, 'precision': False, 'completitud': False, 'sin_errores': False, 'raw_validation_output': '[PRECISI√ìN: No, la normativa no se aplica de forma general sin consideraciones adicionales.]\\n[COMPLETITUD: No, omite detalles sobre el consentimiento informado y la alternativa para los socios que no quieran usar su huella dactilar.]\\n[ERRORES: S√≠, contiene una afirmaci√≥n incorrecta al considerar que el consentimiento es suficiente solo por ser incluido en un contrato.]'}\n",
            "2025-05-08 15:06:12,522 - asistente_legal - WARNING - Respuesta no v√°lida en intento 2: {'valid': False, 'precision': False, 'completitud': False, 'sin_errores': False, 'raw_validation_output': '[PRECISI√ìN: No, la normativa no se aplica de forma general sin consideraciones adicionales.]\\n[COMPLETITUD: No, omite detalles sobre el consentimiento informado y la alternativa para los socios que no quieran usar su huella dactilar.]\\n[ERRORES: S√≠, contiene una afirmaci√≥n incorrecta al considerar que el consentimiento es suficiente solo por ser incluido en un contrato.]'}\n",
            "2025-05-08 15:06:12,525 - asistente_legal - INFO - Usando el prompt: legal_analysis_prompt\n",
            "2025-05-08 15:06:12,527 - asistente_legal - INFO - Prompt adaptado con sugerencias del validador.\n",
            "2025-05-08 15:06:12,528 - asistente_legal - INFO - Intento RAG 3 para consulta: 'Un gimnasio quiere implementar un sistema de acceso mediante huella dactilar para sus socios. Planean incluir una cl√°usula en el contrato de inscripci√≥n donde el socio 'acepta el uso de sus datos biom√©tricos para el control de acceso'. ¬øEs esta forma de obtener el consentimiento v√°lida seg√∫n el RGPD y la LOPDGDD, considerando que los datos biom√©tricos son una categor√≠a especial de datos? Justifica la respuesta con los art√≠culos pertinentes que encuentres.' (Categor√≠a: LEGAL_ANALYSIS)\n",
            "2025-05-08 15:06:12,529 - asistente_legal - INFO - Reintento 2: creando un retriever m√°s flexible (MMR + reformulaci√≥n posible).\n",
            "2025-05-08 15:06:12,596 - asistente_legal - INFO - Recuperados 10 documentos.\n",
            "2025-05-08 15:06:12,597 - asistente_legal - INFO - Iniciando reranking para 10 documentos recuperados...\n",
            "2025-05-08 15:06:12,967 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:06:13,265 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:06:13,567 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:06:13,862 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:06:14,160 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:06:14,458 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:06:14,765 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:06:15,065 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:06:15,109 - asistente_legal - INFO - Reranking completado. 4 documentos seleccionados.\n",
            "2025-05-08 15:06:15,111 - asistente_legal - INFO - Documentos despu√©s de reranking: 4\n",
            "2025-05-08 15:06:19,195 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:06:33,282 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:06:37,730 - asistente_legal - INFO - Validaci√≥n: {'valid': None, 'precision': None, 'completitud': None, 'sin_errores': None, 'raw_validation_output': '1. **No**, aunque el consentimiento no sea v√°lido por defecto, la respuesta deber√≠a ofrecer m√°s informaci√≥n sobre los requisitos espec√≠ficos para tratar datos biom√©tricos.\\n2. **No**, la respuesta podr√≠a ser m√°s completa si explicara mejor las garant√≠as adicionales necesarias para proteger estos datos sensibles.\\n3. **S√≠**, se omite la importancia de informar detalladamente sobre el tratamiento especial de datos biom√©tricos y las medidas de seguridad adicionales requeridas por la normativa.'}\n",
            "2025-05-08 15:06:37,732 - asistente_legal - WARNING - Respuesta no v√°lida en intento 3: {'valid': None, 'precision': None, 'completitud': None, 'sin_errores': None, 'raw_validation_output': '1. **No**, aunque el consentimiento no sea v√°lido por defecto, la respuesta deber√≠a ofrecer m√°s informaci√≥n sobre los requisitos espec√≠ficos para tratar datos biom√©tricos.\\n2. **No**, la respuesta podr√≠a ser m√°s completa si explicara mejor las garant√≠as adicionales necesarias para proteger estos datos sensibles.\\n3. **S√≠**, se omite la importancia de informar detalladamente sobre el tratamiento especial de datos biom√©tricos y las medidas de seguridad adicionales requeridas por la normativa.'}\n",
            "2025-05-08 15:06:37,734 - asistente_legal - INFO - Iniciando auto-refinamiento de respuesta con feedback del validador.\n",
            "2025-05-08 15:06:38,402 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:06:54,107 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:06:54+0200 lvl=info msg=\"join connections\" obj=join id=6f89c1ec2513 l=127.0.0.1:8000 r=138.100.68.196:56662\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     138.100.68.196:0 - \"POST /chat HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 15:06:55,326 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu\n",
            "2025-05-08 15:06:55,327 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-mpnet-base-v2\n",
            "2025-05-08 15:07:02,051 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:07:02,220 - asistente_legal - INFO - Clasificaci√≥n: CONVERSATION, Confianza: 99\n",
            "2025-05-08 15:07:02,222 - asistente_legal - INFO - Intenci√≥n clasificada como: CONVERSATION (Confianza: 99%) - Usar RAG: False\n",
            "2025-05-08 15:07:02,443 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     138.100.68.196:0 - \"POST /chat HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 15:07:36,863 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:07:36+0200 lvl=info msg=\"join connections\" obj=join id=e83afe023116 l=127.0.0.1:8000 r=138.100.68.196:56662\n",
            "2025-05-08 15:07:36,872 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu\n",
            "2025-05-08 15:07:36,874 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-mpnet-base-v2\n",
            "2025-05-08 15:07:39,339 - asistente_legal - INFO - Resultado de clasificaci√≥n obtenido de cach√© para: di mi nombre, quiero saber si lo has podido recordar\n",
            "2025-05-08 15:07:39,341 - asistente_legal - INFO - Intenci√≥n clasificada como: CONVERSATION (Confianza: 98%) - Usar RAG: False\n",
            "2025-05-08 15:07:40,330 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     138.100.68.196:0 - \"POST /chat HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 15:07:58,976 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:07:58+0200 lvl=info msg=\"join connections\" obj=join id=f31b8fae2836 l=127.0.0.1:8000 r=138.100.68.196:56662\n",
            "2025-05-08 15:07:58,986 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu\n",
            "2025-05-08 15:07:58,988 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-mpnet-base-v2\n",
            "2025-05-08 15:08:01,521 - asistente_legal - INFO - Detectada pregunta de seguimiento. Incorporando √∫ltima consulta como contexto.\n",
            "2025-05-08 15:08:02,597 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:08:02,762 - asistente_legal - INFO - Clasificaci√≥n: CONVERSATION, Confianza: 98\n",
            "2025-05-08 15:08:02,764 - asistente_legal - INFO - Intenci√≥n clasificada como: CONVERSATION (Confianza: 98%) - Usar RAG: False\n",
            "2025-05-08 15:08:03,011 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     138.100.68.196:0 - \"POST /chat HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 15:09:41,699 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:09:41+0200 lvl=info msg=\"join connections\" obj=join id=f467c384cb89 l=127.0.0.1:8000 r=138.100.68.196:56662\n",
            "2025-05-08 15:09:41,702 - asistente_legal - INFO - Solicitud de reseteo para: pepe\n",
            "2025-05-08 15:09:41,707 - asistente_legal - INFO - Memoria borrada para pepe\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     138.100.68.196:0 - \"POST /reset HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 15:09:49,326 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:09:49+0200 lvl=info msg=\"join connections\" obj=join id=bab9af4b3e0d l=127.0.0.1:8000 r=138.100.68.196:56662\n",
            "2025-05-08 15:09:49,337 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu\n",
            "2025-05-08 15:09:49,339 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-mpnet-base-v2\n",
            "2025-05-08 15:09:51,721 - asistente_legal - INFO - Resultado de clasificaci√≥n obtenido de cach√© para: di mi nombre, quiero saber si lo has podido recordar\n",
            "2025-05-08 15:09:51,723 - asistente_legal - INFO - Intenci√≥n clasificada como: CONVERSATION (Confianza: 98%) - Usar RAG: False\n",
            "2025-05-08 15:09:52,122 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     138.100.68.196:0 - \"POST /chat HTTP/1.1\" 200 OK\n"
          ]
        }
      ],
      "source": [
        "import nest_asyncio\n",
        "import uvicorn\n",
        "from fastapi import FastAPI, Request\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Necesario para que FastAPI funcione dentro del notebook\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Crea la app\n",
        "app = FastAPI()\n",
        "app.state.user_sessions = {}\n",
        "\n",
        "# CORS para permitir acceso desde GitHub Pages u otro frontend\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # O restringe a tu dominio exacto\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "@app.post(\"/reset\") #peticiones HTTP POST que lleguen a la ruta /reset por ej POST https://ip.ngrok.io/reset\n",
        "async def reset_memoria(request: Request):\n",
        "    datos = await request.json()\n",
        "    usuario = datos.get(\"usuario\", \"\").strip().lower()\n",
        "    sesiones = request.app.state.user_sessions\n",
        "\n",
        "    logger.info(f\"Solicitud de reseteo para: {usuario}\")\n",
        "    if usuario in sesiones:\n",
        "        del sesiones[usuario]\n",
        "        logger.info(f\"Memoria borrada para {usuario}\")\n",
        "        return {\"status\": \"ok\", \"mensaje\": f\"Memoria reiniciada para {usuario}\"}\n",
        "    else:\n",
        "        logger.warning(f\"No se encontr√≥ memoria activa para {usuario}\")\n",
        "        return {\"status\": \"no-op\", \"mensaje\": \"Usuario no ten√≠a memoria activa\"}\n",
        "\n",
        "\n",
        "\n",
        "# Ruta del chat\n",
        "@app.post(\"/chat\")\n",
        "async def chat(request: Request):\n",
        "    datos = await request.json()\n",
        "    pregunta = datos.get(\"mensaje\", \"\")\n",
        "    usuario = datos.get(\"usuario\", \"invitado\").strip().lower()\n",
        "\n",
        "    respuesta = responder_web(pregunta, usuario, request.app)\n",
        "    return {\"respuesta\": respuesta}\n",
        "\n",
        "# Crear t√∫nel ngrok (puedes copiar la URL que imprime)\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"Tu servidor est√° disponible en: {public_url}\")\n",
        "\n",
        "# Lanzar el servidor\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VFohj9Ife-W"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "NgUHfiVbwTfQ"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0015e48e30504dcfa20e224ddfcf823d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "005b82a3932a4ecb9770921e61eee168": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0067bf50ac57403d95bed599f67e05a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0aed6b8fffc9441dbe28059150085e39",
              "IPY_MODEL_5650051de25b49b3a3cd2ec15dd6ef35",
              "IPY_MODEL_e0b9c95bd1cb4c34ad24e7983d5fb019"
            ],
            "layout": "IPY_MODEL_b2d07be45d1f4b1ea9845b302c32ef9e"
          }
        },
        "00a9009b6cb34da0b8d11233bafe42f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0104974262494a418b716c5fcbd36207": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "014e363aae62423787d393d949df2c9b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0176b9971228499fa742bfce6f754663": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3dcd01f64e374bc6ae6c1d587b81bac5",
            "max": 345,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_939fd41a1efd49f6a0e5e880c8a6aa7e",
            "value": 345
          }
        },
        "03ffb1d6834b444383139a123f373dd1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0487c8e4a08f4e20b33aae61a069b7d7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04df9580b1ec4ca79d282383eafabaa2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0561192c90dd4ea6a120a2b9940e5ab8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05af9e6bb60047a3ad7e3142a177ca14": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "066a8a3400384015a2ab91be7925e856": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07e44da4ada941eb8fc5b45251049802": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08746819f1ba4895bcbc13fd0ecbd0ac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08974acddcf14d3684295695bbe6fbac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08ac4c49dee94f17a6ef197a985fd780": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90dbffdf96bf4d609c59d0c631d6fdfd",
            "max": 122,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7d238b16dac94154bd3e24df5636bc65",
            "value": 122
          }
        },
        "08e35bdac488451b82a2f9b785aa6138": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08f864ad78c8485bb45fa50dda7f188a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6cbfef56108b4992b385d82ff36cdbc4",
              "IPY_MODEL_1c7015ec55f545b8a1a26cbf8e32cc39",
              "IPY_MODEL_a4e479e28ce94b0f9b63dd4c435e6e7a"
            ],
            "layout": "IPY_MODEL_81acfacf75ef46d88c4b530896c1d6bb"
          }
        },
        "0a248e4eeb884e16966688be3c5f815b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b0226de01884c16bd3824355fd2daa2",
            "max": 5702746403,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6c0fad39a89342be9404a09ec7c4fcb9",
            "value": 5702745860
          }
        },
        "0ae11327bf7f40fa9a452c313fb4be49": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ae3f54842c04d6ea1b5fa1be3bfbc5e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0aed6b8fffc9441dbe28059150085e39": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0104974262494a418b716c5fcbd36207",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_dc68039b2b7840dabb1d39d90e90afd1",
            "value": "Flattening‚Äáthe‚Äáindices:‚Äá100%"
          }
        },
        "0b98782b5d1d4862af17ae67cc5777ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ed1a808877b54b14a01c0d4ab757dc81",
              "IPY_MODEL_5462e6730819423292ff22ba556578d2",
              "IPY_MODEL_148d32d501274b418bb8d63f2831da34"
            ],
            "layout": "IPY_MODEL_9dff1428059d4bc1b396338372a0ac66"
          }
        },
        "0d29128b3b8d42178f3ae6d6b4772d2f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dbbc41cb5da49c09557d77318e1d89a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e7b0815f4424615866b745392e1348c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0fcfe5f97c8147a3a375cec97fecd359": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "121e5097c4da4938af588656ee8a14dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9013e44d4b234355bf5cb5ab2cf282a1",
              "IPY_MODEL_54421f39df1a41efacb0b907c57922f6",
              "IPY_MODEL_23ca931599514777b22a9c78d175ed21"
            ],
            "layout": "IPY_MODEL_5ca0d161ddec4450bec2a0cb8d0ad683"
          }
        },
        "131b5d0fad894cdf9ac88fff9c123efe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "138f00d720ac475c9c752dd46e878db2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13f56339d8ff4e548f3389b99b6cd09f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "148d32d501274b418bb8d63f2831da34": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_adc72504dd424f948cc7117553212d57",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0e7b0815f4424615866b745392e1348c",
            "value": "‚Äá723/723‚Äá[00:00&lt;00:00,‚Äá29.3kB/s]"
          }
        },
        "14d56ccfd0904350abe91735f301aab7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_da6f107176a642e3bf5d4620ae011d03",
              "IPY_MODEL_ed9e04c3c7f84a649a4c36b8f517ca90",
              "IPY_MODEL_e093a79cbce24644ac2f510187798adf"
            ],
            "layout": "IPY_MODEL_4adaeabf499941c1b5c3ca13bccda9f8"
          }
        },
        "15614a90630f48f28e345ee0e71d4306": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "157dcb1f0bf242139f3546cc93b7b936": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15a9b66c449a43ee9e759b29457e1df3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1681c2a889474147a281aa57ea49fa01": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16d2bef217f04594bbc94acd7971ddac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "174183bcb63a4efa8f1420bfc4d0b5c8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1843dac8987a4441819a184141373de2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bd00d3d8b214109973049badc3cb4be",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_39bc31df5c4947ae90a0bf862e739ae8",
            "value": "Flattening‚Äáthe‚Äáindices:‚Äá100%"
          }
        },
        "193259b79e294176983888af51b51103": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a2674c45ca046cba90f0040eef6fe84": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1e88ba228d844f697ac66ab1f11a0f5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_2d9361af973f426aaf8b6e7816c18f80",
            "value": "‚Äá53.0/53.0‚Äá[00:00&lt;00:00,‚Äá1.59kB/s]"
          }
        },
        "1a9d2ffe04bf4ba2a2f9c372307ab239": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d4c6e36a1b8485d9f8371bd4e88e859",
            "max": 796,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7926b22c81d24b3faa488d7e8cd45e25",
            "value": 796
          }
        },
        "1b9f60aac66949cba9a3f20759d1f5c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a816656658de4212968acb301df96937",
            "max": 51052,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_989344ac67d24b4098b718695ece3f9b",
            "value": 51052
          }
        },
        "1bed7cd946514427849fee72d8762fb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff68046ed0bd417e92e303c15c43d7f2",
              "IPY_MODEL_b7c8a149304948f7850e24b3b6e2f3b4",
              "IPY_MODEL_9402beee1ca44853b81ca1425b38bdad"
            ],
            "layout": "IPY_MODEL_709541f6add042f9942ece0d18d0ed8b"
          }
        },
        "1c7015ec55f545b8a1a26cbf8e32cc39": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_741258f9827546319a1dc4d2f892210a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e10e80363b754204887fffc4af7cd3d8",
            "value": 1
          }
        },
        "1cd4963370fd42d8beae2e798ffee9a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec0f34d02eee4fcf9d7f51d86b768b46",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_30ed64dac9cc4a218734f8678d6b35b4",
            "value": "sentencepiece.bpe.model:‚Äá100%"
          }
        },
        "1eabbd3c84064a2a86020c1c28b0426c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1fbc81be877b4712933eb6e5c0a28cd4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20804d2b3c7e4ef1ba3d67916d9768d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "20e48edeeb834c16a0e592889bfb33a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2227fdd27eaa4ce49be2a0f852496be2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4d842b8447c4d2db29a2a5646be53db",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_16d2bef217f04594bbc94acd7971ddac",
            "value": "‚Äá229/229‚Äá[00:00&lt;00:00,‚Äá5.67kB/s]"
          }
        },
        "22d6e496881c4d6d89d060a4c785c22c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22ffe8f6ecc941b492179e53a8ad6928": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1cd4963370fd42d8beae2e798ffee9a3",
              "IPY_MODEL_b5d505d5789f43cca86cb1a621745015",
              "IPY_MODEL_2676c6f283104ded9ae4c7f9420f1473"
            ],
            "layout": "IPY_MODEL_7ae263e1a157415b989d86b39dc86ab9"
          }
        },
        "2337590ca184415e9c27920a91f2bddb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73d06604b4f540b58e9cf87c7f56152b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3e53b04b3f754fc282692b31ea10baae",
            "value": "Flattening‚Äáthe‚Äáindices:‚Äá100%"
          }
        },
        "23ca931599514777b22a9c78d175ed21": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24e1e0ebc9f04fadbae02b192b57bfa4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_07e44da4ada941eb8fc5b45251049802",
            "value": "‚Äá1.11G/1.11G‚Äá[00:06&lt;00:00,‚Äá143MB/s]"
          }
        },
        "24e1e0ebc9f04fadbae02b192b57bfa4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "256205400ed448e0a51efdd73e593eb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2676c6f283104ded9ae4c7f9420f1473": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5dd53b147d08431e8ecc73aa798ad889",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_615c3c243a214a27a95a1b3cc807bc21",
            "value": "‚Äá5.07M/5.07M‚Äá[00:00&lt;00:00,‚Äá144MB/s]"
          }
        },
        "26d9440af08a4404abe52a76f096bb6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_138f00d720ac475c9c752dd46e878db2",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3c65196ca805431ebb766846980c6d11",
            "value": "unsloth.Q8_0.gguf:‚Äá"
          }
        },
        "278d6110c5d946748d5dd90256c326c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_174183bcb63a4efa8f1420bfc4d0b5c8",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_66106b47a3ba43ceb3b943dee3dbe3a0",
            "value": "‚Äá796/796‚Äá[00:00&lt;00:00,‚Äá27148.04‚Äáexamples/s]"
          }
        },
        "27ae2827671c4143b0b2b2075fc1f972": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_592e543174ac4c149b978a9b076e6947",
              "IPY_MODEL_0a248e4eeb884e16966688be3c5f815b",
              "IPY_MODEL_9c0b992d847a4edc8bae02bdb0a47a52"
            ],
            "layout": "IPY_MODEL_c9b8ab31024948dbab3b9df93a5cc238"
          }
        },
        "283f2a5610894deb9a7b10c6403cd4bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_289b9bc44ac44e6b9876b95b2f0a3133",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_20e48edeeb834c16a0e592889bfb33a6",
            "value": "‚Äá402/402‚Äá[00:00&lt;00:00,‚Äá45.5kB/s]"
          }
        },
        "285d4440076a4c68a6f8317266e880c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5808bf7c644e4228ae669343d44c59bd",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4a2150ec026648bcba1c75a31618131d",
            "value": "tokenizer_config.json:‚Äá100%"
          }
        },
        "288b72f069454a308bbcdd817d8e0af2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_774c667bb01445a79cf8381a7dc73314",
              "IPY_MODEL_4f2a96b0b9654e56a30bfe89a5386a23",
              "IPY_MODEL_92c00c5cb9c24de1af7e40aa0ed15b47"
            ],
            "layout": "IPY_MODEL_fc8181c289ff44c5a83a88684203906e"
          }
        },
        "289b9bc44ac44e6b9876b95b2f0a3133": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b0226de01884c16bd3824355fd2daa2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b400f884db74feb8be26093bdd792a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2b8e98da62ad465793e858bbdc7db9ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40217752dc6245a1b2c67ec4e79e379c",
            "max": 796,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d9476b1342c74f95b72318a5b809402b",
            "value": 796
          }
        },
        "2ce45738eaed494288408212e78724f2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d9361af973f426aaf8b6e7816c18f80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e39a170df4e413f960ec363bbea2095": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f913635f3df441b82a5374c8a4f7422": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fd1ce058941425da07b9eb0b22fd09f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30ed64dac9cc4a218734f8678d6b35b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32840c7d1b414841bc28c7a9fd81be33": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "332acfff1e5e459c8d229d3a0d2169d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_559cf60479a44452900421c985949a3a",
              "IPY_MODEL_bdf00aa9b8da43b0b229c5a4566ff4bd",
              "IPY_MODEL_fd633cad3d994284a8084f93c2668669"
            ],
            "layout": "IPY_MODEL_6a329639b41149069cd87e5df3b7b5a9"
          }
        },
        "33893db8b278422ba6f508f5a9c78177": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33f28884e71c4a3bba92ef0615ee3a48": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34b786a9cd8d42b7b1c69ca91d3c9359": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ccdb1a7be02b4f77a4635f1612be9360",
              "IPY_MODEL_6daa63e3012b435a887107e392aa6651",
              "IPY_MODEL_a3ee21e71ae44f1ebbe0f6e2e06bcbcb"
            ],
            "layout": "IPY_MODEL_36c4e3436ae64ee9bbf784aa8c39c06b"
          }
        },
        "34d9097b358f4066b81610a643a91367": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "355096deee0842068a23019ab8d5ee1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ae3f54842c04d6ea1b5fa1be3bfbc5e",
            "max": 8540771296,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f7a60f172ea441df9fde0e5852305f57",
            "value": 8540771296
          }
        },
        "365a847d2fb949b08ad2799ddb9bdfe8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36c4e3436ae64ee9bbf784aa8c39c06b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "375e18cf4fae4b38935c18b1b026d194": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95dd39f5325241719f5838f21828ca20",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ff2bab5e8aad485a86195e2f69834c5e",
            "value": "generation_config.json:‚Äá100%"
          }
        },
        "38053e33cafd4207bc3d5ed0537b92ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "382bbae83bbc416395edb127b6bec9ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38afaf81d45f45fca24a96b6ff25a4b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "39ae9634aef84b949cef2f9107773320": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39bc31df5c4947ae90a0bf862e739ae8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c65196ca805431ebb766846980c6d11": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3cbca6985d8340a9b0e073a24622004f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3cdf4dc180b64a54844353e64725cf3e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d274df88eac4e16b4c220b1a6372a57": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3dcd01f64e374bc6ae6c1d587b81bac5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e4bc9f043854488b878656483ad6688": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e53b04b3f754fc282692b31ea10baae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40217752dc6245a1b2c67ec4e79e379c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41c05166642345b0a00f7d186218be49": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43ec4ef991ca471fb7b409eee3553ed0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49c87fcf94e74d7da5dc14803b102cb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b950535e3e7347a0af7628d5bf7b80ef",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b82314f359324390b94eb921847d5e08",
            "value": "config_sentence_transformers.json:‚Äá100%"
          }
        },
        "4a2150ec026648bcba1c75a31618131d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a8bbd1eb7a54f94a0933ea719e27a2e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4adaeabf499941c1b5c3ca13bccda9f8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bebb5119aab4cd196509bd84f5e6015": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d0f2246d36943cfab3dd60260c92912": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d5d028e5e304226b654fca8859514a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e3861b0138546da835a21cb88267d2e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e81edbb12584ac9b2bcc3d34aff7e8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f2a96b0b9654e56a30bfe89a5386a23": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1fd1ac2fbb6434a97aedd66419f3a1f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_38afaf81d45f45fca24a96b6ff25a4b4",
            "value": 1
          }
        },
        "503b8be124f24ea49d8061863ad68f0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5095763539d846d39b9348e51bd7a627": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "50efde67ef904d4693da167df766d993": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "51ff0c36700e4c92866ea7239e40e7f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "536953943b1a44ad9c8953bd0e5e5260": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53bd285ba55e428bbdcad85c808dca54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "54421f39df1a41efacb0b907c57922f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92e8fb8fd761435b8873b6ea8bf8fe91",
            "max": 1112201288,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_83bd0985ce934f8e90925148a2fb29c2",
            "value": 1112201288
          }
        },
        "5462e6730819423292ff22ba556578d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a47b8d1c755f486686afbd01f85dff34",
            "max": 723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_32840c7d1b414841bc28c7a9fd81be33",
            "value": 723
          }
        },
        "559cf60479a44452900421c985949a3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_711c3e4a2fc94774931c340fcdd94039",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3cbca6985d8340a9b0e073a24622004f",
            "value": "Flattening‚Äáthe‚Äáindices:‚Äá100%"
          }
        },
        "5650051de25b49b3a3cd2ec15dd6ef35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b61c3734c88480e84c4ca8d2ede49bf",
            "max": 796,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f6971aba51584d6e879ae4a026f71110",
            "value": 796
          }
        },
        "5808bf7c644e4228ae669343d44c59bd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58e4b815ec4a4747a0b56e33a540c9bb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "592e543174ac4c149b978a9b076e6947": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62abec66de98471dbac0a2993bea23ad",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c1d1a9e630ae45a896a48c40d2ef9849",
            "value": "model.safetensors:‚Äá100%"
          }
        },
        "5a5e5d22b7ae4273bbb8664a877397e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bd00d3d8b214109973049badc3cb4be": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c1ac9846bfb48d19298a110c29bf2e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a8bbd1eb7a54f94a0933ea719e27a2e",
            "max": 796,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b5a109286ea842b0b2ba9534713c1ffe",
            "value": 796
          }
        },
        "5ca0d161ddec4450bec2a0cb8d0ad683": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d9c82cc676d40c9a92d9ccca8a4e3e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dcf287617bb4db5ac7b65bcf81d1887": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dd53b147d08431e8ecc73aa798ad889": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e5c95ab789c470090fbee9c41bc596a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "615c3c243a214a27a95a1b3cc807bc21": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61950496fcb54235a6a0e1fff0991037": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "629ec748ee184a8eb69a3cb65ec65e0f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62abec66de98471dbac0a2993bea23ad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64d9434b4d4c43bcab5188ef0cd22ccc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66106b47a3ba43ceb3b943dee3dbe3a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "67f6191f7d934ba995d4deda864aa147": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5dcf287617bb4db5ac7b65bcf81d1887",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c9bd4c42e09a459abd7574111b60812b",
            "value": "Converting‚Äáto‚ÄáShareGPT:‚Äá100%"
          }
        },
        "6895588ff3ae4dfdb44f545b9f2c9a0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77256958d12141eb809dd0c585983dee",
              "IPY_MODEL_0176b9971228499fa742bfce6f754663",
              "IPY_MODEL_fdf7e9de3889464e940f1637fc8ec65e"
            ],
            "layout": "IPY_MODEL_9e6500d06fad492783db0849ceb7f26b"
          }
        },
        "69de33eb6326457298070f7fe84cfff1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_743d80e6489542f7b9493fef1b9998a3",
            "max": 796,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2b400f884db74feb8be26093bdd792a2",
            "value": 796
          }
        },
        "6a329639b41149069cd87e5df3b7b5a9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a3b9333aeae4a00a21d6b857c5c54c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a4f221663c444b3a82a71f850162467": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b61c3734c88480e84c4ca8d2ede49bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c0fad39a89342be9404a09ec7c4fcb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6cbfef56108b4992b385d82ff36cdbc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58e4b815ec4a4747a0b56e33a540c9bb",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_51ff0c36700e4c92866ea7239e40e7f3",
            "value": "100%"
          }
        },
        "6ce15f1736b343948196bb520cae9a84": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a42fb19dbd8f4557bf3bc62f79c729bd",
              "IPY_MODEL_2b8e98da62ad465793e858bbdc7db9ab",
              "IPY_MODEL_8bb5db854b6743218980a1909031f9d1"
            ],
            "layout": "IPY_MODEL_6a4f221663c444b3a82a71f850162467"
          }
        },
        "6cea37a6ac1244faa10da603cf53c985": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6daa63e3012b435a887107e392aa6651": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c893c659fe674f73a391219bcc9ce9f2",
            "max": 796,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fe2cb4a4a87e425bb741dc2e96cb3c19",
            "value": 796
          }
        },
        "6e4fb3251724456fae6b6f13a6c12097": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6f342ebd95af4f33b51649261a89bea2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "709541f6add042f9942ece0d18d0ed8b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70c1b4c89dc04af4b88cec3d9fd76143": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "711c3e4a2fc94774931c340fcdd94039": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7232fabaecbe44f0bfb85efabc852adf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_285d4440076a4c68a6f8317266e880c7",
              "IPY_MODEL_ddf79a29dab14cc9916571d5ef10b224",
              "IPY_MODEL_283f2a5610894deb9a7b10c6403cd4bb"
            ],
            "layout": "IPY_MODEL_2e39a170df4e413f960ec363bbea2095"
          }
        },
        "731686f94c174dc0b65556bca9eca0b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c01604f930e94bc79dc10369f36819b0",
              "IPY_MODEL_931336892af0406cb159b2ebef825df3",
              "IPY_MODEL_fa438c641a1e41c08b2ebbb4e87a2018"
            ],
            "layout": "IPY_MODEL_8106f79da6974840a0c228354066bdac"
          }
        },
        "73d06604b4f540b58e9cf87c7f56152b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "741258f9827546319a1dc4d2f892210a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "743d80e6489542f7b9493fef1b9998a3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75e83248694a401283c01c1f1052519d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ebca870c675e4ade83fd5712b97a96f8",
              "IPY_MODEL_d223583186b748afa0af48cf021b1ac6",
              "IPY_MODEL_f802cc878c98461598a166e9b59fcd3f"
            ],
            "layout": "IPY_MODEL_db15ee6f267a4620978d4ef3b822d711"
          }
        },
        "7661221145d04590a76c181231d8eb56": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_df917f2ba37a41039d8305bce24a3c6d",
              "IPY_MODEL_1b9f60aac66949cba9a3f20759d1f5c4",
              "IPY_MODEL_c672b2b5cb574c33955eb644fbe35b98"
            ],
            "layout": "IPY_MODEL_c47b1a19346a4a568abf1a3281233887"
          }
        },
        "76b661b674b845738960e48caba5ae30": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f913635f3df441b82a5374c8a4f7422",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_536953943b1a44ad9c8953bd0e5e5260",
            "value": "‚Äá10/10‚Äá[01:49&lt;00:00,‚Äá‚Äá9.60s/it]"
          }
        },
        "77256958d12141eb809dd0c585983dee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61950496fcb54235a6a0e1fff0991037",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b587a76b966544dba862b0eff56543b4",
            "value": "special_tokens_map.json:‚Äá100%"
          }
        },
        "774c667bb01445a79cf8381a7dc73314": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08e35bdac488451b82a2f9b785aa6138",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f93543b90d1f4db491d07f743561389f",
            "value": "Generating‚Äátrain‚Äásplit:‚Äá"
          }
        },
        "777674f274264d22ade6581c5b9de8e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7e598dfeef2432eb628d35ff2a1628e",
            "max": 796,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9f97bbebf0e140fab4945e412b6ed2fa",
            "value": 796
          }
        },
        "782afad7b7474bc09650a78759a916e9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7926b22c81d24b3faa488d7e8cd45e25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7ae263e1a157415b989d86b39dc86ab9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d238b16dac94154bd3e24df5636bc65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7df0ca10a6b64cf49549a592019744af": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fdf5fdff9f64e749803ad4c4f4a5db7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "808f22940b3e4d56a044858af7717a82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_375e18cf4fae4b38935c18b1b026d194",
              "IPY_MODEL_f7039d5c73024c2bb49232de870c0753",
              "IPY_MODEL_b013a83d84e340ca9bdb032e04a4fe82"
            ],
            "layout": "IPY_MODEL_629ec748ee184a8eb69a3cb65ec65e0f"
          }
        },
        "80ca55c4cb834e07a222a05eed953108": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8106f79da6974840a0c228354066bdac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81acfacf75ef46d88c4b530896c1d6bb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83bd0985ce934f8e90925148a2fb29c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "848cfb3b291b46b2b5ad89ae0b370abc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1681c2a889474147a281aa57ea49fa01",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8d3865be2e614d97a600f65d8fcd2f53",
            "value": "Generando‚Äádataset:‚Äá100%"
          }
        },
        "84ded545991a482ea540a08ec0efac93": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d0f2246d36943cfab3dd60260c92912",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_86f61fb933494d148da6d6b0d64cf56c",
            "value": "tokenizer.json:‚Äá100%"
          }
        },
        "8521f1b081c8426ab052535bccb87a51": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d274df88eac4e16b4c220b1a6372a57",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e5caecafe5ba42fbbdeabcc688980df9",
            "value": "Map:‚Äá100%"
          }
        },
        "866d6d319be342cc9c9c1584d79e276f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f03102b868c54d0aa2e47d4971d01cd0",
              "IPY_MODEL_8a74df7879114abf83c02f59b86179e3",
              "IPY_MODEL_e1897a57fea34737ac7848789f951bae"
            ],
            "layout": "IPY_MODEL_34d9097b358f4066b81610a643a91367"
          }
        },
        "86f61fb933494d148da6d6b0d64cf56c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "887830af532541a88b552b499e751cf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8879a8c8c73442f1913797a796271b23": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_67f6191f7d934ba995d4deda864aa147",
              "IPY_MODEL_f84e15027cc044b18041b2c14a6ca5b9",
              "IPY_MODEL_c31c0805bee54f6e9d2ee25efb5455b3"
            ],
            "layout": "IPY_MODEL_33f28884e71c4a3bba92ef0615ee3a48"
          }
        },
        "898936f8509843b089c36807a31e9f73": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8a74df7879114abf83c02f59b86179e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04df9580b1ec4ca79d282383eafabaa2",
            "max": 239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b75d3f266ade490b8b49393883fad4c5",
            "value": 239
          }
        },
        "8aa62bcdb50444f68cbcb6d8da3c8a5b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bb5db854b6743218980a1909031f9d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f604e6f0f77c4d63bf49f4728d411f73",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4bebb5119aab4cd196509bd84f5e6015",
            "value": "‚Äá796/796‚Äá[00:03&lt;00:00,‚Äá286.31‚Äáexamples/s]"
          }
        },
        "8d3865be2e614d97a600f65d8fcd2f53": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9013e44d4b234355bf5cb5ab2cf282a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0015e48e30504dcfa20e224ddfcf823d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4e81edbb12584ac9b2bcc3d34aff7e8f",
            "value": "model.safetensors:‚Äá100%"
          }
        },
        "90dbffdf96bf4d609c59d0c631d6fdfd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92c00c5cb9c24de1af7e40aa0ed15b47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a5e5d22b7ae4273bbb8664a877397e2",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_131b5d0fad894cdf9ac88fff9c123efe",
            "value": "‚Äá796/0‚Äá[00:00&lt;00:00,‚Äá7426.77‚Äáexamples/s]"
          }
        },
        "92e8fb8fd761435b8873b6ea8bf8fe91": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "931336892af0406cb159b2ebef825df3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d29128b3b8d42178f3ae6d6b4772d2f",
            "max": 796,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5095763539d846d39b9348e51bd7a627",
            "value": 796
          }
        },
        "9392415e91b049ac9b173eaecb6e8c5f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "939fd41a1efd49f6a0e5e880c8a6aa7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9402beee1ca44853b81ca1425b38bdad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cdf4dc180b64a54844353e64725cf3e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_382bbae83bbc416395edb127b6bec9ec",
            "value": "‚Äá3.90k/3.90k‚Äá[00:00&lt;00:00,‚Äá134kB/s]"
          }
        },
        "946f98b1fd3544509b709249cb96374b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1843dac8987a4441819a184141373de2",
              "IPY_MODEL_5c1ac9846bfb48d19298a110c29bf2e7",
              "IPY_MODEL_278d6110c5d946748d5dd90256c326c9"
            ],
            "layout": "IPY_MODEL_03ffb1d6834b444383139a123f373dd1"
          }
        },
        "9585c39899db42858cecf93aff9d5c15": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95dd39f5325241719f5838f21828ca20": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "968ab1d17382458daefc7e2724c6912e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_26d9440af08a4404abe52a76f096bb6e",
              "IPY_MODEL_355096deee0842068a23019ab8d5ee1c",
              "IPY_MODEL_c5f6ba1f51a1425b90b3946ca66cc685"
            ],
            "layout": "IPY_MODEL_43ec4ef991ca471fb7b409eee3553ed0"
          }
        },
        "97a26d010d1340b2811b8cdaf4c8bce9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "982efc40a70245b7ad8468c8bede757b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "989344ac67d24b4098b718695ece3f9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9932f74e242f4119bf2ef6c3c9306087": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "99f72a35e9b84bb391be822e751193ab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a4aa0c0a5844740abe4d275fe038c43": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f52a496379bf4f91bba56679c7495c67",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_898936f8509843b089c36807a31e9f73",
            "value": 10
          }
        },
        "9a665075297c4dd5b36cb32d5a4e76f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9bb6e499ffb0400e9abfb4c4df69ad10": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c0b992d847a4edc8bae02bdb0a47a52": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db50691252d44ad2ad8059a948b2a05f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6a3b9333aeae4a00a21d6b857c5c54c1",
            "value": "‚Äá5.70G/5.70G‚Äá[00:53&lt;00:00,‚Äá371MB/s]"
          }
        },
        "9d4c6e36a1b8485d9f8371bd4e88e859": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9dd150275e074a99813d6d33f497a678": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_49c87fcf94e74d7da5dc14803b102cb3",
              "IPY_MODEL_08ac4c49dee94f17a6ef197a985fd780",
              "IPY_MODEL_f3e1221f87d3405f9e0a2626f2202dfc"
            ],
            "layout": "IPY_MODEL_d35613bcb73e4faeb93e74fd71441c48"
          }
        },
        "9dff1428059d4bc1b396338372a0ac66": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e6500d06fad492783db0849ceb7f26b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f97bbebf0e140fab4945e412b6ed2fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a3b7c42a2be84209a595cc209ba12755": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5250b86c3a544929037a12dac2d898d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_887830af532541a88b552b499e751cf2",
            "value": "Extending‚Äáconversations:‚Äá100%"
          }
        },
        "a3ee21e71ae44f1ebbe0f6e2e06bcbcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9585c39899db42858cecf93aff9d5c15",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_982efc40a70245b7ad8468c8bede757b",
            "value": "‚Äá796/796‚Äá[00:00&lt;00:00,‚Äá2044.37‚Äáexamples/s]"
          }
        },
        "a42fb19dbd8f4557bf3bc62f79c729bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de7427201d2d46b0a507f4e43dec20ed",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_15a9b66c449a43ee9e759b29457e1df3",
            "value": "Unsloth:‚ÄáTokenizing‚Äá[&quot;text&quot;]‚Äá(num_proc=2):‚Äá100%"
          }
        },
        "a47b8d1c755f486686afbd01f85dff34": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4b5c9120f6d484285cad69f75a4c4e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70c1b4c89dc04af4b88cec3d9fd76143",
            "max": 796,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4d5d028e5e304226b654fca8859514a4",
            "value": 796
          }
        },
        "a4e479e28ce94b0f9b63dd4c435e6e7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08746819f1ba4895bcbc13fd0ecbd0ac",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fec29fe05b334fb488537ab92066ddb5",
            "value": "‚Äá1/1‚Äá[01:14&lt;00:00,‚Äá74.24s/it]"
          }
        },
        "a581b54fb35b483dbe8db53c11ab2e07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5fc373283e74ad38980e2a1c588619e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a3b7c42a2be84209a595cc209ba12755",
              "IPY_MODEL_69de33eb6326457298070f7fe84cfff1",
              "IPY_MODEL_a8fb0593650f4007be70d0fbb0fdfced"
            ],
            "layout": "IPY_MODEL_bdb97cdc500d4ef6be5175bdfd970914"
          }
        },
        "a816656658de4212968acb301df96937": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a86a7074d2aa4c0b9cc07304df896a09": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8fb0593650f4007be70d0fbb0fdfced": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab828a2583c44d1183da61e4663e1bba",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_bb1368132ca94034bbae49c2150ad2e1",
            "value": "‚Äá796/796‚Äá[00:00&lt;00:00,‚Äá6913.35‚Äáexamples/s]"
          }
        },
        "ab828a2583c44d1183da61e4663e1bba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac6bd87456c84942b74b461d8a8126b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c1b94c562bc04a9197388bbe47290df8",
              "IPY_MODEL_c543d5b573f746ac8cbf54af455a242c",
              "IPY_MODEL_1a2674c45ca046cba90f0040eef6fe84"
            ],
            "layout": "IPY_MODEL_0fcfe5f97c8147a3a375cec97fecd359"
          }
        },
        "ac984ea7b70c4cda959655d375c978b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "adb37cae3e0c4ab986c5c0704becf8f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "adc72504dd424f948cc7117553212d57": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b013a83d84e340ca9bdb032e04a4fe82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f50acf3c440e46da9e2445e7a010f81d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_20804d2b3c7e4ef1ba3d67916d9768d1",
            "value": "‚Äá220/220‚Äá[00:00&lt;00:00,‚Äá22.7kB/s]"
          }
        },
        "b252adcd86424e549429e91c42e4231c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b264b66edb224b469dde034cec7af3a9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2d07be45d1f4b1ea9845b302c32ef9e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3b7b90e3607427f94cffed566a51bcb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5250b86c3a544929037a12dac2d898d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b587a76b966544dba862b0eff56543b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5a109286ea842b0b2ba9534713c1ffe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b5d505d5789f43cca86cb1a621745015": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_365a847d2fb949b08ad2799ddb9bdfe8",
            "max": 5069051,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9932f74e242f4119bf2ef6c3c9306087",
            "value": 5069051
          }
        },
        "b75d3f266ade490b8b49393883fad4c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b7c8a149304948f7850e24b3b6e2f3b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_782afad7b7474bc09650a78759a916e9",
            "max": 3896,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e4cf5cf7bdda4c4b8785ed121d800531",
            "value": 3896
          }
        },
        "b82314f359324390b94eb921847d5e08": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b85cbf9a65f54e699e1c539938fe5a6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b950535e3e7347a0af7628d5bf7b80ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba9223c462864718968d1d2399fba000": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bae2e5076ac64f6f9b89e73197d2de1f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb1368132ca94034bbae49c2150ad2e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bdb97cdc500d4ef6be5175bdfd970914": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdf00aa9b8da43b0b229c5a4566ff4bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7a644cb693544c9b42f2764cbd4abf8",
            "max": 796,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6e4fb3251724456fae6b6f13a6c12097",
            "value": 796
          }
        },
        "bf4f47b2da0344fe8f03eb26d982b96a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf90afd20c1f4717910955a9d481ae9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_84ded545991a482ea540a08ec0efac93",
              "IPY_MODEL_e218685f2f2a403daeaf63c34204d6e5",
              "IPY_MODEL_dee2d0ee41014773b887fb8365d9506d"
            ],
            "layout": "IPY_MODEL_5d9c82cc676d40c9a92d9ccca8a4e3e7"
          }
        },
        "bfbaf1e01bc34bb9a3d7a11d47fdc683": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c01604f930e94bc79dc10369f36819b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf4f47b2da0344fe8f03eb26d982b96a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9a665075297c4dd5b36cb32d5a4e76f8",
            "value": "Flattening‚Äáthe‚Äáindices:‚Äá100%"
          }
        },
        "c1b94c562bc04a9197388bbe47290df8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b252adcd86424e549429e91c42e4231c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_c58bab0f2bd546bcb962fb45a3d6de17",
            "value": "sentence_bert_config.json:‚Äá100%"
          }
        },
        "c1d1a9e630ae45a896a48c40d2ef9849": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2e2e5027bff43b5b47f07cd7b80ff66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c315c9e0934e4965ade0eddda9035103": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fdf5fdff9f64e749803ad4c4f4a5db7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ac984ea7b70c4cda959655d375c978b3",
            "value": "‚Äá796/796‚Äá[00:00&lt;00:00,‚Äá55683.41‚Äáexamples/s]"
          }
        },
        "c31c0805bee54f6e9d2ee25efb5455b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d19d330b75eb4951ba2f4a65825d7eba",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_08974acddcf14d3684295695bbe6fbac",
            "value": "‚Äá796/796‚Äá[00:00&lt;00:00,‚Äá30649.08‚Äáexamples/s]"
          }
        },
        "c418be902e11469cbb66deb117f9362d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c47b1a19346a4a568abf1a3281233887": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c543d5b573f746ac8cbf54af455a242c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3b7b90e3607427f94cffed566a51bcb",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c2e2e5027bff43b5b47f07cd7b80ff66",
            "value": 53
          }
        },
        "c58bab0f2bd546bcb962fb45a3d6de17": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5f6ba1f51a1425b90b3946ca66cc685": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9392415e91b049ac9b173eaecb6e8c5f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_33893db8b278422ba6f508f5a9c78177",
            "value": "‚Äá8.54G/?‚Äá[01:13&lt;00:00,‚Äá649MB/s]"
          }
        },
        "c672b2b5cb574c33955eb644fbe35b98": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e3861b0138546da835a21cb88267d2e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_41c05166642345b0a00f7d186218be49",
            "value": "‚Äá51.1k/51.1k‚Äá[00:00&lt;00:00,‚Äá4.79MB/s]"
          }
        },
        "c7a644cb693544c9b42f2764cbd4abf8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c893c659fe674f73a391219bcc9ce9f2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9b8ab31024948dbab3b9df93a5cc238": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9bd4c42e09a459abd7574111b60812b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cac764d083e34e8798681ea14fcf1a75": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dde8564ed950447490393f433add95a6",
              "IPY_MODEL_d5aa82b997ab4be189285e681efc564d",
              "IPY_MODEL_2227fdd27eaa4ce49be2a0f852496be2"
            ],
            "layout": "IPY_MODEL_99f72a35e9b84bb391be822e751193ab"
          }
        },
        "ccdb1a7be02b4f77a4635f1612be9360": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_014e363aae62423787d393d949df2c9b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_157dcb1f0bf242139f3546cc93b7b936",
            "value": "Unsloth:‚ÄáStandardizing‚Äáformats‚Äá(num_proc=2):‚Äá100%"
          }
        },
        "cd4a034727aa4a5eabe5d0160d2ca331": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7c0d3b5f98d4943aae99cb39a1a8974",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0ae11327bf7f40fa9a452c313fb4be49",
            "value": "‚Äá796/796‚Äá[00:00&lt;00:00,‚Äá31142.82‚Äáexamples/s]"
          }
        },
        "cdc832f653e340999903760c0fe93d51": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cecb244f1d094f188023e10ea1863471": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e4bc9f043854488b878656483ad6688",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0561192c90dd4ea6a120a2b9940e5ab8",
            "value": "‚Äá796/796‚Äá[00:00&lt;00:00,‚Äá4886.58‚Äáexamples/s]"
          }
        },
        "cf1622cec12146bdac9caa903985e340": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d19d330b75eb4951ba2f4a65825d7eba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1e7a5f5fb554318a2dcf99b68077cf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d223583186b748afa0af48cf021b1ac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13f56339d8ff4e548f3389b99b6cd09f",
            "max": 9085698,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_256205400ed448e0a51efdd73e593eb8",
            "value": 9085698
          }
        },
        "d34e92f473bf4d60924f5d9018132646": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d35613bcb73e4faeb93e74fd71441c48": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3d61388da654567bcf61577a65f60a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5aa82b997ab4be189285e681efc564d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f342ebd95af4f33b51649261a89bea2",
            "max": 229,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d1e7a5f5fb554318a2dcf99b68077cf4",
            "value": 229
          }
        },
        "d7c0d3b5f98d4943aae99cb39a1a8974": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7e598dfeef2432eb628d35ff2a1628e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9476b1342c74f95b72318a5b809402b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "da6f107176a642e3bf5d4620ae011d03": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_005b82a3932a4ecb9770921e61eee168",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d3d61388da654567bcf61577a65f60a5",
            "value": "config.json:‚Äá100%"
          }
        },
        "dac4ebc417c14936af453c15258c605e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dacc18e6fa914351b71468698aea27ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fd33365745b64ffe80031385cb1ae772",
              "IPY_MODEL_a4b5c9120f6d484285cad69f75a4c4e7",
              "IPY_MODEL_cd4a034727aa4a5eabe5d0160d2ca331"
            ],
            "layout": "IPY_MODEL_22d6e496881c4d6d89d060a4c785c22c"
          }
        },
        "dacd4821451644cebf3cdc739762698f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db15ee6f267a4620978d4ef3b822d711": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db50691252d44ad2ad8059a948b2a05f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db9b43bd16f541f5bfca796e23391575": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc68039b2b7840dabb1d39d90e90afd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dde8564ed950447490393f433add95a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0487c8e4a08f4e20b33aae61a069b7d7",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6cea37a6ac1244faa10da603cf53c985",
            "value": "modules.json:‚Äá100%"
          }
        },
        "ddf79a29dab14cc9916571d5ef10b224": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dacd4821451644cebf3cdc739762698f",
            "max": 402,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c418be902e11469cbb66deb117f9362d",
            "value": 402
          }
        },
        "de7427201d2d46b0a507f4e43dec20ed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dee2d0ee41014773b887fb8365d9506d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7df0ca10a6b64cf49549a592019744af",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_38053e33cafd4207bc3d5ed0537b92ea",
            "value": "‚Äá9.08M/9.08M‚Äá[00:00&lt;00:00,‚Äá24.6MB/s]"
          }
        },
        "df917f2ba37a41039d8305bce24a3c6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fbc81be877b4712933eb6e5c0a28cd4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_00a9009b6cb34da0b8d11233bafe42f9",
            "value": "tokenizer_config.json:‚Äá100%"
          }
        },
        "e0058619ab004c3891f1f211e7138bf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8521f1b081c8426ab052535bccb87a51",
              "IPY_MODEL_777674f274264d22ade6581c5b9de8e6",
              "IPY_MODEL_cecb244f1d094f188023e10ea1863471"
            ],
            "layout": "IPY_MODEL_fda1be9042914fec8a145e0cac60ca01"
          }
        },
        "e06d7dd1a809445db2ae8996af6dabf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e093a79cbce24644ac2f510187798adf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf1622cec12146bdac9caa903985e340",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_15614a90630f48f28e345ee0e71d4306",
            "value": "‚Äá190/190‚Äá[00:00&lt;00:00,‚Äá11.5kB/s]"
          }
        },
        "e0b9c95bd1cb4c34ad24e7983d5fb019": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6a99c6a2a1d4de7b8ae59675718052f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0dbbc41cb5da49c09557d77318e1d89a",
            "value": "‚Äá796/796‚Äá[00:00&lt;00:00,‚Äá21355.30‚Äáexamples/s]"
          }
        },
        "e10e80363b754204887fffc4af7cd3d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e1897a57fea34737ac7848789f951bae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_193259b79e294176983888af51b51103",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_dac4ebc417c14936af453c15258c605e",
            "value": "‚Äá239/239‚Äá[00:00&lt;00:00,‚Äá20.5kB/s]"
          }
        },
        "e1fd1ac2fbb6434a97aedd66419f3a1f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "e218685f2f2a403daeaf63c34204d6e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfbaf1e01bc34bb9a3d7a11d47fdc683",
            "max": 9081518,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_adb37cae3e0c4ab986c5c0704becf8f3",
            "value": 9081518
          }
        },
        "e4cf5cf7bdda4c4b8785ed121d800531": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e5caecafe5ba42fbbdeabcc688980df9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e80f507e19904e4fa56d9da77d4f535e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eaa5c7ea39344301bab459bc04d5716b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2337590ca184415e9c27920a91f2bddb",
              "IPY_MODEL_1a9d2ffe04bf4ba2a2f9c372307ab239",
              "IPY_MODEL_c315c9e0934e4965ade0eddda9035103"
            ],
            "layout": "IPY_MODEL_cdc832f653e340999903760c0fe93d51"
          }
        },
        "ebca870c675e4ade83fd5712b97a96f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b264b66edb224b469dde034cec7af3a9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_a581b54fb35b483dbe8db53c11ab2e07",
            "value": "tokenizer.json:‚Äá100%"
          }
        },
        "ec0f34d02eee4fcf9d7f51d86b768b46": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed1a808877b54b14a01c0d4ab757dc81": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97a26d010d1340b2811b8cdaf4c8bce9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e06d7dd1a809445db2ae8996af6dabf9",
            "value": "config.json:‚Äá100%"
          }
        },
        "ed9e04c3c7f84a649a4c36b8f517ca90": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bae2e5076ac64f6f9b89e73197d2de1f",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_50efde67ef904d4693da167df766d993",
            "value": 190
          }
        },
        "edd2ae155b8f45658f0fed29db260588": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_848cfb3b291b46b2b5ad89ae0b370abc",
              "IPY_MODEL_9a4aa0c0a5844740abe4d275fe038c43",
              "IPY_MODEL_76b661b674b845738960e48caba5ae30"
            ],
            "layout": "IPY_MODEL_8aa62bcdb50444f68cbcb6d8da3c8a5b"
          }
        },
        "f03102b868c54d0aa2e47d4971d01cd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ce45738eaed494288408212e78724f2",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9bb6e499ffb0400e9abfb4c4df69ad10",
            "value": "special_tokens_map.json:‚Äá100%"
          }
        },
        "f1e88ba228d844f697ac66ab1f11a0f5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3e1221f87d3405f9e0a2626f2202dfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db9b43bd16f541f5bfca796e23391575",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b85cbf9a65f54e699e1c539938fe5a6f",
            "value": "‚Äá122/122‚Äá[00:00&lt;00:00,‚Äá3.41kB/s]"
          }
        },
        "f4d842b8447c4d2db29a2a5646be53db": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f50acf3c440e46da9e2445e7a010f81d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f52a496379bf4f91bba56679c7495c67": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5943977ac2e47f79c80772f7aed2bd8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f604e6f0f77c4d63bf49f4728d411f73": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6971aba51584d6e879ae4a026f71110": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f6a99c6a2a1d4de7b8ae59675718052f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7039d5c73024c2bb49232de870c0753": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a86a7074d2aa4c0b9cc07304df896a09",
            "max": 220,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_80ca55c4cb834e07a222a05eed953108",
            "value": 220
          }
        },
        "f7a60f172ea441df9fde0e5852305f57": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f802cc878c98461598a166e9b59fcd3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fd1ce058941425da07b9eb0b22fd09f",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e80f507e19904e4fa56d9da77d4f535e",
            "value": "‚Äá9.09M/9.09M‚Äá[00:01&lt;00:00,‚Äá8.54MB/s]"
          }
        },
        "f84e15027cc044b18041b2c14a6ca5b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5943977ac2e47f79c80772f7aed2bd8",
            "max": 796,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_53bd285ba55e428bbdcad85c808dca54",
            "value": 796
          }
        },
        "f93543b90d1f4db491d07f743561389f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa438c641a1e41c08b2ebbb4e87a2018": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d34e92f473bf4d60924f5d9018132646",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_fce4bd29180641d29cc7e6310706d109",
            "value": "‚Äá796/796‚Äá[00:00&lt;00:00,‚Äá28681.22‚Äáexamples/s]"
          }
        },
        "fc8181c289ff44c5a83a88684203906e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fce4bd29180641d29cc7e6310706d109": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd33365745b64ffe80031385cb1ae772": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39ae9634aef84b949cef2f9107773320",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_066a8a3400384015a2ab91be7925e856",
            "value": "Merging‚Äácolumns:‚Äá100%"
          }
        },
        "fd633cad3d994284a8084f93c2668669": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05af9e6bb60047a3ad7e3142a177ca14",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1eabbd3c84064a2a86020c1c28b0426c",
            "value": "‚Äá796/796‚Äá[00:00&lt;00:00,‚Äá27088.13‚Äáexamples/s]"
          }
        },
        "fda1be9042914fec8a145e0cac60ca01": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdf7e9de3889464e940f1637fc8ec65e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e5c95ab789c470090fbee9c41bc596a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_503b8be124f24ea49d8061863ad68f0b",
            "value": "‚Äá345/345‚Äá[00:00&lt;00:00,‚Äá38.9kB/s]"
          }
        },
        "fe2cb4a4a87e425bb741dc2e96cb3c19": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fec29fe05b334fb488537ab92066ddb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff2bab5e8aad485a86195e2f69834c5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff68046ed0bd417e92e303c15c43d7f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba9223c462864718968d1d2399fba000",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_64d9434b4d4c43bcab5188ef0cd22ccc",
            "value": "README.md:‚Äá100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}