{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgUHfiVbwTfQ"
      },
      "source": [
        "# GENERAR DATASET FORMA AUTOMATICA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUqzE0k2U1lb"
      },
      "source": [
        "> **Para ejecutar el siguiente codigo no hace falta estar conectados a una GPU**\n",
        "\n",
        "\n",
        "El siguiente codigo nos permite generar preguntas para nuestro dataset usando OpenRouter, en esta web podemos usar casi todos los modelos del mercado pero para generar el dataset vamos a limitarnos a las versiones gratuitas de DeepSeek-R1 y DeepSeek-V3\n",
        "\n",
        "La API de OpenRouter tiene limitaciones cuando se usan modelos gratuitos:\n",
        "\n",
        "\n",
        "> Free limit: If you are using a free model variant (with an ID ending in :free), then you will be limited to 20 requests per minute and 200 requests per day.\n",
        "\n",
        "\n",
        "\n",
        "El siguiente codigo tiene dos modos:\n",
        "\n",
        "*   Modo de único Prompt: Le metemos solo un prompt del excel ([Prompts para generar el dataset usando distintos modelos](https://docs.google.com/spreadsheets/d/1MQF8Z5_HqVSOzDXD8Ya7zEljJ13rD1Kw9FsUd6xhEJo/edit?pli=1&gid=0#gid=0)) y nos va a generar las preguntas y las respuestas siguiendo las instrucciones del prompt\n",
        "\n",
        "*   Modo Múltiples Prompts: Copiamos distintas preguntas creadas por nosotros o por otros modelos LLM a un archivo de texto o csv, este modo además permite darle un prompt inicial de contexto antes de que empiece a generar las respuestas a nuestras preguntas, hay un ejemplo de prompt de contexto en el ([excel de prompts](https://docs.google.com/spreadsheets/d/1MQF8Z5_HqVSOzDXD8Ya7zEljJ13rD1Kw9FsUd6xhEJo/edit?pli=1&gid=0#gid=0))\n",
        "\n",
        "Una vez genera el contenido, nos descarga el csv con las preguntas, las respuestas, y el modelo que ha utilizado\n",
        "\n",
        "Se pueden generar varios modelos para que ambos generen preguntas y se vayan alternando.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBo74hgrStBc"
      },
      "source": [
        "**Paquita dice que antes de dar a play lo pienses dos veces a la hora de agregar cuantas consultas quieres hacer, ya que si no tienes un conteo de las restantes en la API el codigo mostrará que no se pueden generar más consultas y el csv no se generará con las consultas antes del error**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw-k3y8dY9c5"
      },
      "source": [
        "## Codigo para generar el contenido del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "edd2ae155b8f45658f0fed29db260588",
            "848cfb3b291b46b2b5ad89ae0b370abc",
            "9a4aa0c0a5844740abe4d275fe038c43",
            "76b661b674b845738960e48caba5ae30",
            "8aa62bcdb50444f68cbcb6d8da3c8a5b",
            "1681c2a889474147a281aa57ea49fa01",
            "8d3865be2e614d97a600f65d8fcd2f53",
            "f52a496379bf4f91bba56679c7495c67",
            "898936f8509843b089c36807a31e9f73",
            "2f913635f3df441b82a5374c8a4f7422",
            "536953943b1a44ad9c8953bd0e5e5260"
          ]
        },
        "id": "SZh0d5RstAcX",
        "outputId": "5bf8064d-b45e-4e5f-9145-eebaab902f18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.1.0\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<h3>Generador de Dataset para Fine-Tuning</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Introduce tu clave API de OpenRouter: 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\n",
            "\n",
            "Introduce el prompt general (escribe o pega el texto y presiona Enter dos veces para finalizar):\n",
            "Para terminar la entrada, escribe una línea que solo contenga '***FIN***'\n",
            "💡 Objetivo: Generar UN ÚNICO par de pregunta-respuesta sobre privacidad y protección de datos en España para entrenar un modelo con RAG.  ⚠️ INSTRUCCIÓN CRUCIAL: Genera SÓLO UN par pregunta-respuesta en cada ejecución. No incluyas múltiples ejemplos ni numeración.  📊 Temas a cubrir (diversidad temática): 🔹 Derechos digitales: acceso, rectificación, supresión, oposición, portabilidad, limitación, olvido. 🔹 Consentimiento: validez, revocación, menores, excepciones. 🔹 Cookies y tracking: tipos, banners, rechazos, perfilado. 🔹 Datos sensibles: salud, biométricos, ideología, orientación sexual. 🔹 Contextos específicos: laboral, educativo, sanitario, financiero, comercial. 🔹 Tecnologías emergentes: IA, reconocimiento facial, IoT, blockchain. 🔹 Seguridad: brechas, notificaciones, medidas técnicas. 🔹 Transferencias: internacionales, entre empresas, cesiones. 🔹 Videovigilancia: ámbito privado, público, laboral. 🔹 Responsabilidades: empresas, DPO, encargados, autoridades. 🔹 Sanciones: sin mencionar cantidades específicas.  🔹 Características de las preguntas: ✔️ Pregunta concreta (10-30 palabras) enfocada en un único tema ✔️ Evita preguntas genéricas; usa casos prácticos realistas ✔️ Incluye contexto específico (quién, dónde, situación concreta) ✔️ Varía la formulación: usa \"¿Es legal...?\", \"¿Qué derechos tengo si...?\", \"¿Qué ocurre cuando...?\" ✔️ Asegúrate de que sea una pregunta ORIGINAL y diferente a los ejemplos  🔹 Características de las respuestas: ✅ 2-4 frases concisas pero completas ✅ Explica condiciones y matices (no solo \"sí/no\") ✅ Lenguaje claro sin jerga legal ✅ No menciones artículos o leyes específicas ✅ Tono profesional pero accesible ✅ Información actualizada según RGPD y LOPDGDD  🔹 Precisión de las respuestas: ✅ Asegúrate de que la respuesta refleje con precisión el marco legal español actual (RGPD y LOPDGDD) ✅ Si existe ambigüedad legal, menciónalo explícitamente ✅ Evita respuestas que puedan resultar engañosas por simplificar en exceso ✅ No incluyas opiniones personales o interpretaciones controvertidas   🔹 Tono de la respuesta: ✅ Profesional pero accesible ✅ Objetivo y no alarmista ✅ Informativo sin ser condescendiente ✅ Directo sin ser brusco  🔹 Escenarios específicos a considerar (Estos son solo ejemplos puedes generar tus propios escenarios, se original): - Una persona intentando ejercer sus derechos frente a una gran empresa - Un empleado con preocupaciones sobre la privacidad en su trabajo - Un padre/madre preocupado por los datos de sus hijos menores - Un usuario de aplicaciones móviles o servicios online - Un propietario de pequeño negocio que necesita cumplir con la normativa - Situaciones en comunidades de vecinos o espacios compartidos - Interacciones con administraciones públicas - Espionaje entre compañeros de trabajo - Empresas comerciando con datos privador  ⚠️ IMPORTANTE: Asegúrate de que la pregunta sea ORIGINAL y no se parezca demasiado a los ejemplos proporcionados. Busca ángulos o situaciones novedosas dentro del tema elegido.   ⚠️ FORMATO EXACTO A SEGUIR:   Pregunta sobre los temas nombrados anteriormente RESPUESTAMODELO Respuesta clara, razonada, y convincente  📝 Ejemplos (para referencia):   \"Un comercio online me instaló cookies de seguimiento sin avisarme. ¿Esto es legal?\" RESPUESTAMODELO \"No, en España un sitio web solo puede instalar cookies de seguimiento si obtiene tu consentimiento previo. Deben informarte de manera clara sobre su uso y permitirte aceptarlas o rechazarlas fácilmente. Solo las cookies estrictamente necesarias pueden activarse sin tu permiso.\"  \"Una empresa de crédito ha rechazado mi solicitud basándose en un algoritmo. ¿Puedo exigir una explicación?\" RESPUESTAMODELO \"Sí, cuando una decisión automatizada afecta significativamente tus derechos, puedes solicitar una explicación clara sobre los criterios utilizados. También puedes pedir una revisión manual si crees que la decisión ha sido injusta o errónea.\"  \"Trabajo desde casa y mi empresa usa software de monitoreo en mi ordenador. ¿Hasta qué punto es legal?\" RESPUESTAMODELO \"El monitoreo es legal si está justificado y comunicado de manera transparente. Tu empresa debe informarte sobre qué datos recopila, con qué finalidad y durante cuánto tiempo. Además, el control debe ser proporcional y no invadir tu privacidad más allá de lo necesario para evaluar tu rendimiento laboral.\"\n",
            "***FIN***\n",
            "Introduce el número de ejemplos a generar por modelo [10]: 10\n",
            "Introduce los modelos a utilizar (separados por comas) [deepseek/deepseek-chat:free]: \n",
            "Introduce el nombre del archivo de salida [dataset.csv]: \n",
            "\n",
            "Resumen de la configuración:\n",
            "- Prompt general: '💡 Objetivo: Generar UN ÚNICO par de pregunta-respuesta sobre privacidad y protección de datos en Esp...'\n",
            "- Número de iteraciones por modelo: 10\n",
            "- Modelos: deepseek/deepseek-chat:free\n",
            "- Archivo de salida: dataset.csv\n",
            "\n",
            "¿Confirmar y comenzar la generación? (s/n): s\n",
            "Modo: Prompt único - Generando 10 ejemplos por modelo\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "edd2ae155b8f45658f0fed29db260588",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generando dataset:   0%|          | 0/10 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Procesando modelo: deepseek/deepseek-chat:free\n",
            "\n",
            "Iteración 1/10:\n",
            "Tema: cookies y navegación web\n",
            "Pregunta: \"¿Puede una página web bloquear el acceso si rechazo todas las cookies excepto las estrictamente nec...\n",
            "Respuesta: No, en España no es legal bloquear el acceso a un sitio web por rechazar cookies no esenciales. Solo...\n",
            "\n",
            "Iteración 2/10:\n",
            "Tema: derechos ARCO\n",
            "Pregunta: \"Una empresa ha rechazado mi solicitud de eliminar mis datos personales alegando que los necesita pa...\n",
            "Respuesta: \"Sí, si la empresa debe conservar tus datos para cumplir con una obligación legal, puede rechazar tu...\n",
            "\n",
            "Iteración 3/10:\n",
            "Tema: redes sociales\n",
            "Pregunta: ¿Puedo pedir a una red social que elimine publicaciones antiguas donde aparezco sin mi consentimient...\n",
            "Respuesta: Sí, tienes derecho a solicitar la eliminación de publicaciones en las que apareces sin tu consentimi...\n",
            "\n",
            "Iteración 4/10:\n",
            "Tema: videovigilancia\n",
            "Pregunta: \"Mi comunidad de vecinos ha instalado cámaras en las zonas comunes sin informar a los residentes. ¿E...\n",
            "Respuesta: \"No, la instalación de cámaras en zonas comunes debe ser comunicada previamente a los vecinos median...\n",
            "\n",
            "Iteración 5/10:\n",
            "Tema: menores y consentimiento\n",
            "Pregunta: \"Un profesor quiere crear un grupo de WhatsApp con alumnos menores de 14 años para enviarles tareas....\n",
            "Respuesta: Sí, el consentimiento debe ser otorgado por los padres o tutores legales, ya que los menores de 14 a...\n",
            "\n",
            "Iteración 6/10:\n",
            "Tema: geolocalización\n",
            "Pregunta: \"Una app de transporte me pide acceso constante a mi ubicación, incluso cuando no la estoy usando. ¿...\n",
            "Respuesta: \"No, una app solo puede acceder a tu ubicación cuando es estrictamente necesario para su funcionamie...\n",
            "\n",
            "Iteración 7/10:\n",
            "Tema: datos biométricos\n",
            "Pregunta: \"Mi empresa quiere implantar un sistema de control de acceso mediante reconocimiento facial. ¿Qué de...\n",
            "Respuesta: \"Tienes derecho a que te informen de manera clara sobre el uso, almacenamiento y finalidad de tus da...\n",
            "\n",
            "Iteración 8/10:\n",
            "Tema: marketing directo\n",
            "Pregunta: \"Una tienda online me ha enviado publicidad por correo electrónico sin mi consentimiento. ¿Qué puedo...\n",
            "Respuesta: \"Puedes solicitar que dejen de enviarte publicidad, ya que el marketing directo requiere tu consenti...\n",
            "\n",
            "Iteración 9/10:\n",
            "Tema: filtraciones de datos\n",
            "Pregunta: He descubierto que mis datos personales han sido filtrados en una brecha de seguridad de mi banco. ¿...\n",
            "Respuesta: Debes notificar inmediatamente al banco para que tomen medidas y te informen sobre el alcance de la ...\n",
            "\n",
            "Iteración 10/10:\n",
            "Tema: transferencias internacionales\n",
            "Pregunta: \"Una empresa estadounidense quiere transferir mis datos personales desde España a sus servidores en ...\n",
            "Respuesta: \"Para que la transferencia sea legal, la empresa debe garantizar que tus datos estarán protegidos ba...\n",
            "Dataset guardado exitosamente en dataset.csv\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_643eea60-61a2-4f02-8106-c8bc46ed0da3\", \"dataset.csv\", 5121)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "✅ Se han generado 10 ejemplos únicos para el dataset.\n",
            "❌ Se han detectado 0 ejemplos con formato incorrecto o duplicados.\n"
          ]
        }
      ],
      "source": [
        "!pip install requests tqdm python-dotenv\n",
        "\n",
        "import os\n",
        "import csv\n",
        "import time\n",
        "import requests\n",
        "import json\n",
        "import re\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "from tqdm.notebook import tqdm\n",
        "from google.colab import files\n",
        "\n",
        "\n",
        "def extract_question_answer(response_text: str) -> Tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Extrae la pregunta y respuesta de un texto generado por el modelo usando varios métodos.\n",
        "\n",
        "    Args:\n",
        "        response_text: Texto generado por el modelo.\n",
        "\n",
        "    Returns:\n",
        "        Tupla con (pregunta, respuesta).\n",
        "    \"\"\"\n",
        "    # Normalizar el texto para lidiar con diferentes formatos\n",
        "    text = response_text.strip()\n",
        "\n",
        "    # Método 1: Buscar el delimitador exacto\n",
        "    if \"RESPUESTAMODELO\" in text:\n",
        "        parts = text.split(\"RESPUESTAMODELO\", 1)\n",
        "        return parts[0].strip(), parts[1].strip()\n",
        "\n",
        "    # Método 2: Buscar variaciones del delimitador\n",
        "    for delimiter in [\"RESPUESTA MODELO\", \"Respuesta Modelo\", \"Respuesta:\", \"Respuesta del modelo:\"]:\n",
        "        if delimiter in text:\n",
        "            parts = text.split(delimiter, 1)\n",
        "            return parts[0].strip(), parts[1].strip()\n",
        "\n",
        "    # Método 3: Buscar un patrón de pregunta y respuesta\n",
        "    # Asumiendo que la pregunta termina con un signo de interrogación\n",
        "    # y la respuesta comienza en la siguiente línea\n",
        "    if \"?\" in text:\n",
        "        # Encontrar la última pregunta en el texto\n",
        "        question_parts = text.split(\"?\")\n",
        "        # La pregunta es todo hasta el último signo de interrogación\n",
        "        all_but_last = \"?\".join(question_parts[:-1]) + \"?\"\n",
        "        last_part = question_parts[-1]\n",
        "\n",
        "        # Si hay más de una línea después del signo de interrogación,\n",
        "        # la primera línea podría ser parte de la pregunta\n",
        "        lines_after = last_part.strip().split(\"\\n\")\n",
        "\n",
        "        if len(lines_after) > 1 and not lines_after[0]:\n",
        "            # La respuesta comienza después de una línea en blanco\n",
        "            question = all_but_last.strip()\n",
        "            answer = \"\\n\".join(lines_after[1:]).strip()\n",
        "        else:\n",
        "            # La respuesta comienza inmediatamente después del signo de interrogación\n",
        "            question = all_but_last.strip()\n",
        "            answer = last_part.strip()\n",
        "\n",
        "        return question, answer\n",
        "\n",
        "    # Si nada funciona, intenta una división por la mitad (última opción)\n",
        "    lines = text.strip().split(\"\\n\")\n",
        "    if len(lines) >= 2:\n",
        "        # Asumimos que la mitad es pregunta y la otra mitad respuesta\n",
        "        midpoint = len(lines) // 2\n",
        "        return \"\\n\".join(lines[:midpoint]).strip(), \"\\n\".join(lines[midpoint:]).strip()\n",
        "\n",
        "    # Si todo falla, devuelve un error\n",
        "    print(f\"ERROR: No se pudo extraer la pregunta y respuesta. Texto completo:\\n{text}\")\n",
        "    return \"ERROR: No se pudo extraer la pregunta\", \"ERROR: No se pudo extraer la respuesta\"\n",
        "\n",
        "def validate_qa_pair(question: str, answer: str) -> bool:\n",
        "    \"\"\"\n",
        "    Valida que el par pregunta-respuesta sea coherente.\n",
        "\n",
        "    Args:\n",
        "        question: La pregunta extraída.\n",
        "        answer: La respuesta extraída.\n",
        "\n",
        "    Returns:\n",
        "        True si parece un par válido, False en caso contrario.\n",
        "    \"\"\"\n",
        "    # La pregunta debería terminar con signo de interrogación\n",
        "    has_question_mark = \"?\" in question\n",
        "\n",
        "    # Verificar longitudes mínimas\n",
        "    question_length_ok = len(question.split()) >= 3\n",
        "    answer_length_ok = len(answer.split()) >= 5\n",
        "\n",
        "    # La respuesta no debe contener la palabra \"pregunta\" o \"question\"\n",
        "    no_question_in_answer = \"pregunta\" not in answer.lower() and \"question\" not in answer.lower()\n",
        "\n",
        "    # Verificar que no contenga instrucciones del formato\n",
        "    no_instructions = \"formato\" not in question.lower() and \"instrucciones\" not in question.lower()\n",
        "\n",
        "    # Verificar que no haya mensajes de error\n",
        "    no_errors = \"ERROR:\" not in question and \"ERROR:\" not in answer\n",
        "\n",
        "    return has_question_mark and question_length_ok and answer_length_ok and no_question_in_answer and no_instructions and no_errors\n",
        "\n",
        "def is_similar_to_existing(question: str, existing_questions: List[str], threshold: float = 0.7) -> bool:\n",
        "    \"\"\"\n",
        "    Comprueba si una pregunta es similar a las existentes usando una comparación simple.\n",
        "\n",
        "    Args:\n",
        "        question: Pregunta a comprobar\n",
        "        existing_questions: Lista de preguntas existentes\n",
        "        threshold: Umbral de similitud (0-1)\n",
        "\n",
        "    Returns:\n",
        "        True si es similar, False si no\n",
        "    \"\"\"\n",
        "    # Normalizar la pregunta (minúsculas, sin puntuación)\n",
        "    normalized_question = re.sub(r'[^\\w\\s]', '', question.lower())\n",
        "    words = set(normalized_question.split())\n",
        "\n",
        "    for existing in existing_questions:\n",
        "        normalized_existing = re.sub(r'[^\\w\\s]', '', existing.lower())\n",
        "        existing_words = set(normalized_existing.split())\n",
        "\n",
        "        # Calcular similitud Jaccard (proporción de palabras en común)\n",
        "        intersection = len(words.intersection(existing_words))\n",
        "        union = len(words.union(existing_words))\n",
        "\n",
        "        if union > 0 and intersection / union > threshold:\n",
        "            return True\n",
        "\n",
        "    return False\n",
        "\n",
        "def call_openrouter_api(\n",
        "    prompt: str,\n",
        "    model: str,\n",
        "    api_key: str,\n",
        "    system_message: Optional[str] = None,\n",
        "    max_retries: int = 3\n",
        ") -> Optional[Dict[Any, Any]]:\n",
        "    \"\"\"\n",
        "    Realiza una llamada a la API de OpenRouter.\n",
        "\n",
        "    Args:\n",
        "        prompt: Prompt a enviar.\n",
        "        model: Nombre del modelo a utilizar.\n",
        "        api_key: Clave de API de OpenRouter.\n",
        "        system_message: Mensaje de sistema opcional.\n",
        "        max_retries: Número máximo de reintentos en caso de fallo.\n",
        "\n",
        "    Returns:\n",
        "        Respuesta de la API o None si hubo un error.\n",
        "    \"\"\"\n",
        "    url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {api_key}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"HTTP-Referer\": \"https://colab.research.google.com/\"\n",
        "    }\n",
        "\n",
        "    messages = []\n",
        "    if system_message:\n",
        "        messages.append({\"role\": \"system\", \"content\": system_message})\n",
        "\n",
        "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    data = {\n",
        "        \"model\": model,\n",
        "        \"messages\": messages\n",
        "    }\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.post(url, headers=headers, json=data)\n",
        "            response.raise_for_status()\n",
        "            return response.json()\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error en la llamada a la API (intento {attempt+1}/{max_retries}): {e}\")\n",
        "            if attempt < max_retries - 1:\n",
        "                # Esperar un tiempo exponencial antes de reintentar\n",
        "                wait_time = 2 ** attempt\n",
        "                print(f\"Reintentando en {wait_time} segundos...\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                print(\"Se alcanzó el número máximo de reintentos.\")\n",
        "                return None\n",
        "\n",
        "def extract_response(api_response: Dict[Any, Any]) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Extrae la respuesta del modelo desde la respuesta de la API.\n",
        "\n",
        "    Args:\n",
        "        api_response: Respuesta de la API.\n",
        "\n",
        "    Returns:\n",
        "        Texto de la respuesta o None si no se puede extraer.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return api_response['choices'][0]['message']['content']\n",
        "    except (KeyError, IndexError, TypeError) as e:\n",
        "        print(f\"Error al extraer la respuesta: {e}\")\n",
        "        print(f\"Respuesta completa de la API: {json.dumps(api_response, indent=2)}\")\n",
        "        return None\n",
        "\n",
        "def save_to_csv(data: List[Dict[str, str]], output_file: str) -> None:\n",
        "    \"\"\"\n",
        "    Guarda los datos en un archivo CSV y permite descargarlo desde Colab.\n",
        "\n",
        "    Args:\n",
        "        data: Lista de diccionarios con los datos a guardar.\n",
        "        output_file: Ruta al archivo de salida.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(output_file, 'w', encoding='utf-8', newline='') as file:\n",
        "            fieldnames = ['instruction', 'output', 'model']\n",
        "            writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "\n",
        "            writer.writeheader()\n",
        "            for item in data:\n",
        "                writer.writerow(item)\n",
        "\n",
        "        print(f\"Dataset guardado exitosamente en {output_file}\")\n",
        "\n",
        "        # Permitir la descarga del archivo desde Colab\n",
        "        files.download(output_file)\n",
        "    except Exception as e:\n",
        "        print(f\"Error al guardar el dataset: {e}\")\n",
        "\n",
        "def generate_dataset_single_prompt(\n",
        "    prompt: str,\n",
        "    output_file: str,\n",
        "    models: List[str],\n",
        "    api_key: str,\n",
        "    num_iterations: int = 10\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Genera un dataset usando un único prompt general múltiples veces.\n",
        "\n",
        "    Args:\n",
        "        prompt: Prompt general para generar preguntas y respuestas.\n",
        "        output_file: Ruta al archivo de salida.\n",
        "        models: Lista de modelos a utilizar.\n",
        "        api_key: Clave de API de OpenRouter.\n",
        "        num_iterations: Número de veces que se utilizará el prompt.\n",
        "    \"\"\"\n",
        "    print(f\"Modo: Prompt único - Generando {num_iterations} ejemplos por modelo\")\n",
        "\n",
        "    # Definir temas para rotar\n",
        "    topics = [\n",
        "        \"cookies y navegación web\",\n",
        "        \"derechos ARCO\",\n",
        "        \"redes sociales\",\n",
        "        \"videovigilancia\",\n",
        "        \"menores y consentimiento\",\n",
        "        \"geolocalización\",\n",
        "        \"datos biométricos\",\n",
        "        \"marketing directo\",\n",
        "        \"filtraciones de datos\",\n",
        "        \"transferencias internacionales\",\n",
        "        \"derecho al olvido\",\n",
        "        \"uso de IA con datos personales\",\n",
        "        \"aplicaciones móviles\",\n",
        "        \"datos en el ámbito laboral\",\n",
        "        \"información de salud\",\n",
        "        \"datos bancarios y financieros\"\n",
        "    ]\n",
        "\n",
        "    # Crear archivo para ejemplos fallidos\n",
        "    fallidos_file = f\"ejemplos_fallidos_{int(time.time())}.txt\"\n",
        "\n",
        "    # Preparar el dataset\n",
        "    dataset = []\n",
        "    existing_questions = []\n",
        "    fallidos = 0\n",
        "\n",
        "    # Procesar cada modelo y generar múltiples ejemplos\n",
        "    total_iterations = len(models) * num_iterations\n",
        "    with tqdm(total=total_iterations, desc=\"Generando dataset\") as pbar:\n",
        "        for model in models:\n",
        "            print(f\"\\nProcesando modelo: {model}\")\n",
        "\n",
        "            for i in range(num_iterations):\n",
        "                # Seleccionar tema para esta iteración\n",
        "                current_topic = topics[i % len(topics)]\n",
        "\n",
        "                # Crear un prompt específico para esta iteración\n",
        "                iteration_prompt = f\"\"\"\n",
        "{prompt}\n",
        "\n",
        "⚠️ INSTRUCCIÓN CRUCIAL: Genera UN ÚNICO par de pregunta-respuesta sobre protección de datos.\n",
        "\n",
        "TEMA ESPECÍFICO: Genera una pregunta relacionada con \"{current_topic}\".\n",
        "Asegúrate de que sea una pregunta concreta y relevante para usuarios españoles.\n",
        "\n",
        "FORMATO EXACTO A SEGUIR:\n",
        "[Escribe aquí UNA ÚNICA pregunta sobre {current_topic}]\n",
        "RESPUESTAMODELO\n",
        "[Escribe aquí la respuesta a esa única pregunta]\n",
        "\n",
        "RECUERDA: Solo UN par pregunta-respuesta. Termina tu respuesta después de contestar la pregunta.\n",
        "\"\"\"\n",
        "\n",
        "                # Intentar hasta 3 veces si obtenemos duplicados\n",
        "                max_attempts = 3\n",
        "                success = False\n",
        "\n",
        "                for attempt in range(max_attempts):\n",
        "                    # Llamar a la API\n",
        "                    response_data = call_openrouter_api(iteration_prompt, model, api_key)\n",
        "\n",
        "                    if response_data:\n",
        "                        # Extraer la respuesta\n",
        "                        response_text = extract_response(response_data)\n",
        "\n",
        "                        if response_text:\n",
        "                            # Extraer pregunta y respuesta del texto generado\n",
        "                            question, answer = extract_question_answer(response_text)\n",
        "\n",
        "                            # Validar el par pregunta-respuesta\n",
        "                            if validate_qa_pair(question, answer):\n",
        "                                # Verificar si es similar a preguntas existentes\n",
        "                                if not is_similar_to_existing(question, existing_questions):\n",
        "                                    # Añadir al dataset\n",
        "                                    dataset.append({\n",
        "                                        'instruction': question,\n",
        "                                        'output': answer,\n",
        "                                        'model': model\n",
        "                                    })\n",
        "                                    # Guardar para futuras comparaciones\n",
        "                                    existing_questions.append(question)\n",
        "\n",
        "                                    print(f\"\\nIteración {i+1}/{num_iterations}:\")\n",
        "                                    print(f\"Tema: {current_topic}\")\n",
        "                                    print(f\"Pregunta: {question[:100]}...\")\n",
        "                                    print(f\"Respuesta: {answer[:100]}...\")\n",
        "\n",
        "                                    success = True\n",
        "                                    break\n",
        "                                else:\n",
        "                                    print(f\"\\n⚠️ Intento {attempt+1}: Pregunta similar ya existe, reintentando...\")\n",
        "                                    # Añadir más variación al prompt\n",
        "                                    iteration_prompt += f\"\\n\\nIMPORTANTE: Asegúrate de que la pregunta sea ORIGINAL y DIFERENTE a esta: \\\"{question}\\\"\"\n",
        "                            else:\n",
        "                                print(f\"\\n⚠️ Intento {attempt+1}: Par pregunta-respuesta no válido, reintentando...\")\n",
        "                                print(f\"\\nRespuesta completa del modelo:\")\n",
        "                                print(\"-\" * 50)\n",
        "                                print(response_text)\n",
        "                                print(\"-\" * 50)\n",
        "\n",
        "                if not success:\n",
        "                    fallidos += 1\n",
        "                    print(f\"\\n❌ No se pudo generar un par único en {max_attempts} intentos para el tema: {current_topic}\")\n",
        "\n",
        "                    # Guardar ejemplo fallido\n",
        "                    with open(fallidos_file, \"a\", encoding=\"utf-8\") as f:\n",
        "                        f.write(f\"--- EJEMPLO FALLIDO {fallidos} ---\\n\")\n",
        "                        f.write(f\"MODELO: {model}\\n\")\n",
        "                        f.write(f\"TEMA: {current_topic}\\n\")\n",
        "                        if 'response_text' in locals():\n",
        "                            f.write(f\"ÚLTIMO INTENTO:\\n{response_text}\\n\\n\")\n",
        "                        f.write(\"-\" * 50 + \"\\n\\n\")\n",
        "\n",
        "                pbar.update(1)\n",
        "\n",
        "                # Pequeña pausa para evitar sobrecargar la API\n",
        "                time.sleep(0.5)\n",
        "\n",
        "    # Guardar el dataset\n",
        "    if dataset:\n",
        "        save_to_csv(dataset, output_file)\n",
        "        print(f\"\\n✅ Se han generado {len(dataset)} ejemplos únicos para el dataset.\")\n",
        "        print(f\"❌ Se han detectado {fallidos} ejemplos con formato incorrecto o duplicados.\")\n",
        "        if fallidos > 0:\n",
        "            print(f\"📝 Los ejemplos fallidos se han guardado en '{fallidos_file}'.\")\n",
        "    else:\n",
        "        print(\"❌ No se pudieron generar ejemplos válidos para el dataset.\")\n",
        "\n",
        "# Ejecutar en Colab (interfaz interactiva)\n",
        "def main_colab():\n",
        "    from IPython.display import display, HTML\n",
        "\n",
        "    display(HTML(\"<h3>Generador de Dataset para Fine-Tuning</h3>\"))\n",
        "\n",
        "    # Solicitar los parámetros\n",
        "    api_key = input(\"Introduce tu clave API de OpenRouter: \")\n",
        "\n",
        "\n",
        "    # Entrada directa del prompt\n",
        "    print(\"\\nIntroduce el prompt general (escribe o pega el texto y presiona Enter dos veces para finalizar):\")\n",
        "    print(\"Para terminar la entrada, escribe una línea que solo contenga '***FIN***'\")\n",
        "\n",
        "    lines = []\n",
        "    while True:\n",
        "        line = input()\n",
        "        if line == \"***FIN***\":\n",
        "            break\n",
        "        lines.append(line)\n",
        "\n",
        "    general_prompt = \"\\n\".join(lines)\n",
        "\n",
        "    if not general_prompt.strip():\n",
        "        print(\"El prompt no puede estar vacío.\")\n",
        "        return\n",
        "\n",
        "\n",
        "    # Solicitar el número de iteraciones\n",
        "    num_iterations = input(\"Introduce el número de ejemplos a generar por modelo [10]: \")\n",
        "    num_iterations = int(num_iterations) if num_iterations.strip() else 10\n",
        "\n",
        "    # Solicitar los modelos\n",
        "    models_input = input(\"Introduce los modelos a utilizar (separados por comas) [deepseek/deepseek-chat:free]: \")\n",
        "    models = [m.strip() for m in models_input.split(\",\")] if models_input.strip() else [\"deepseek/deepseek-chat:free\"]\n",
        "\n",
        "    # Solicitar el nombre del archivo de salida\n",
        "    output_file = input(\"Introduce el nombre del archivo de salida [dataset.csv]: \")\n",
        "    output_file = output_file if output_file.strip() else \"dataset.csv\"\n",
        "\n",
        "    print(\"\\nResumen de la configuración:\")\n",
        "    print(f\"- Prompt general: '{general_prompt[:100]}...'\")\n",
        "    print(f\"- Número de iteraciones por modelo: {num_iterations}\")\n",
        "    print(f\"- Modelos: {', '.join(models)}\")\n",
        "    print(f\"- Archivo de salida: {output_file}\")\n",
        "\n",
        "    confirm = input(\"\\n¿Confirmar y comenzar la generación? (s/n): \")\n",
        "    if confirm.lower() in [\"s\", \"si\", \"sí\", \"y\", \"yes\"]:\n",
        "        generate_dataset_single_prompt(\n",
        "            prompt=general_prompt,\n",
        "            output_file=output_file,\n",
        "            models=models,\n",
        "            api_key=api_key,\n",
        "            num_iterations=num_iterations\n",
        "        )\n",
        "    else:\n",
        "        print(\"Operación cancelada.\")\n",
        "\n",
        "\n",
        "\n",
        "# Código para ejecutar directamente en una celda de Colab\n",
        "main_colab()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bn823uUfWxYC"
      },
      "source": [
        "Prompt único para generar el dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0t6fjKVrwSWJ"
      },
      "outputs": [],
      "source": [
        "💡 Objetivo: Generar UN ÚNICO par de pregunta-respuesta sobre privacidad y protección de datos en España para entrenar un modelo con RAG.\n",
        "\n",
        "⚠️ INSTRUCCIÓN CRUCIAL: Genera SÓLO UN par pregunta-respuesta en cada ejecución. No incluyas múltiples ejemplos ni numeración.\n",
        "\n",
        "📊 Temas a cubrir (diversidad temática):\n",
        "🔹 Derechos digitales: acceso, rectificación, supresión, oposición, portabilidad, limitación, olvido.\n",
        "🔹 Consentimiento: validez, revocación, menores, excepciones.\n",
        "🔹 Cookies y tracking: tipos, banners, rechazos, perfilado.\n",
        "🔹 Datos sensibles: salud, biométricos, ideología, orientación sexual.\n",
        "🔹 Contextos específicos: laboral, educativo, sanitario, financiero, comercial.\n",
        "🔹 Tecnologías emergentes: IA, reconocimiento facial, IoT, blockchain.\n",
        "🔹 Seguridad: brechas, notificaciones, medidas técnicas.\n",
        "🔹 Transferencias: internacionales, entre empresas, cesiones.\n",
        "🔹 Videovigilancia: ámbito privado, público, laboral.\n",
        "🔹 Responsabilidades: empresas, DPO, encargados, autoridades.\n",
        "🔹 Sanciones: sin mencionar cantidades específicas.\n",
        "\n",
        "🔹 Características de las preguntas:\n",
        "✔️ Pregunta concreta (10-30 palabras) enfocada en un único tema\n",
        "✔️ Evita preguntas genéricas; usa casos prácticos realistas\n",
        "✔️ Incluye contexto específico (quién, dónde, situación concreta)\n",
        "✔️ Varía la formulación: usa \"¿Es legal...?\", \"¿Qué derechos tengo si...?\", \"¿Qué ocurre cuando...?\"\n",
        "✔️ Asegúrate de que sea una pregunta ORIGINAL y diferente a los ejemplos\n",
        "\n",
        "🔹 Características de las respuestas:\n",
        "✅ 2-4 frases concisas pero completas\n",
        "✅ Explica condiciones y matices (no solo \"sí/no\")\n",
        "✅ Lenguaje claro sin jerga legal\n",
        "✅ No menciones artículos o leyes específicas\n",
        "✅ Tono profesional pero accesible\n",
        "✅ Información actualizada según RGPD y LOPDGDD\n",
        "\n",
        "🔹 Precisión de las respuestas:\n",
        "✅ Asegúrate de que la respuesta refleje con precisión el marco legal español actual (RGPD y LOPDGDD)\n",
        "✅ Si existe ambigüedad legal, menciónalo explícitamente\n",
        "✅ Evita respuestas que puedan resultar engañosas por simplificar en exceso\n",
        "✅ No incluyas opiniones personales o interpretaciones controvertidas\n",
        "\n",
        "\n",
        "🔹 Tono de la respuesta:\n",
        "✅ Profesional pero accesible\n",
        "✅ Objetivo y no alarmista\n",
        "✅ Informativo sin ser condescendiente\n",
        "✅ Directo sin ser brusco\n",
        "\n",
        "🔹 Escenarios específicos a considerar (Estos son solo ejemplos puedes generar tus propios escenarios, se original):\n",
        "- Una persona intentando ejercer sus derechos frente a una gran empresa\n",
        "- Un empleado con preocupaciones sobre la privacidad en su trabajo\n",
        "- Un padre/madre preocupado por los datos de sus hijos menores\n",
        "- Un usuario de aplicaciones móviles o servicios online\n",
        "- Un propietario de pequeño negocio que necesita cumplir con la normativa\n",
        "- Situaciones en comunidades de vecinos o espacios compartidos\n",
        "- Interacciones con administraciones públicas\n",
        "- Espionaje entre compañeros de trabajo\n",
        "- Empresas comerciando con datos privador\n",
        "\n",
        "⚠️ IMPORTANTE: Asegúrate de que la pregunta sea ORIGINAL y no se parezca demasiado a los ejemplos proporcionados. Busca ángulos o situaciones novedosas dentro del tema elegido.\n",
        "\n",
        "\n",
        "⚠️ FORMATO EXACTO A SEGUIR:\n",
        "\n",
        "\n",
        "Pregunta sobre los temas nombrados anteriormente\n",
        "RESPUESTAMODELO\n",
        "Respuesta clara, razonada, y convincente\n",
        "\n",
        "📝 Ejemplos (para referencia):\n",
        "\n",
        "\n",
        "\"Un comercio online me instaló cookies de seguimiento sin avisarme. ¿Esto es legal?\"\n",
        "RESPUESTAMODELO\n",
        "\"No, en España un sitio web solo puede instalar cookies de seguimiento si obtiene tu consentimiento previo. Deben informarte de manera clara sobre su uso y permitirte aceptarlas o rechazarlas fácilmente. Solo las cookies estrictamente necesarias pueden activarse sin tu permiso.\"\n",
        "\n",
        "\"Una empresa de crédito ha rechazado mi solicitud basándose en un algoritmo. ¿Puedo exigir una explicación?\"\n",
        "RESPUESTAMODELO\n",
        "\"Sí, cuando una decisión automatizada afecta significativamente tus derechos, puedes solicitar una explicación clara sobre los criterios utilizados. También puedes pedir una revisión manual si crees que la decisión ha sido injusta o errónea.\"\n",
        "\n",
        "\"Trabajo desde casa y mi empresa usa software de monitoreo en mi ordenador. ¿Hasta qué punto es legal?\"\n",
        "RESPUESTAMODELO\n",
        "\"El monitoreo es legal si está justificado y comunicado de manera transparente. Tu empresa debe informarte sobre qué datos recopila, con qué finalidad y durante cuánto tiempo. Además, el control debe ser proporcional y no invadir tu privacidad más allá de lo necesario para evaluar tu rendimiento laboral.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHSZV3dZ9X0Y"
      },
      "source": [
        "Para detectar preguntas duplicadas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "NId9n1E59ZmX",
        "outputId": "9d443bf7-1cb9-4a6c-cc5e-7d593baf79f3"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_4a617a46-ea2b-4463-946d-5f3e6936ff7f\", \"dataframe_limpio.csv\", 309598)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_csv('/content/FinalData - Hoja 1 (4).csv')\n",
        "\n",
        "# Calcular similitud entre preguntas\n",
        "vectorizer = TfidfVectorizer(stop_words='english')  # Puedes ajustar a español si necesitas\n",
        "vectors = vectorizer.fit_transform(df['instruction'])\n",
        "similarity = cosine_similarity(vectors)\n",
        "\n",
        "# Umbral de similaridad (ajustar según necesidad)\n",
        "threshold = 0.8\n",
        "\n",
        "# Identificar grupos de preguntas similares\n",
        "grupos_similares = []\n",
        "ya_procesadas = set()\n",
        "\n",
        "for i in range(len(df)):\n",
        "    if i in ya_procesadas:\n",
        "        continue\n",
        "\n",
        "    similar_indices = []\n",
        "    for j in range(len(df)):\n",
        "        if i != j and similarity[i, j] > threshold:\n",
        "            similar_indices.append(j)\n",
        "\n",
        "    if similar_indices:\n",
        "        grupo = [i] + similar_indices\n",
        "        grupos_similares.append(grupo)\n",
        "        ya_procesadas.update(similar_indices)\n",
        "\n",
        "# Mostrar los grupos de preguntas similares\n",
        "print(f\"Se encontraron {len(grupos_similares)} grupos de preguntas similares:\\n\")\n",
        "\n",
        "for idx, grupo in enumerate(grupos_similares):\n",
        "    print(f\"Grupo {idx+1} (Similaridad > {threshold}):\")\n",
        "    for i, ind in enumerate(grupo):\n",
        "        print(f\"  {i+1}. Índice {ind}: {df['instruction'].iloc[ind][:100]}...\")\n",
        "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "\n",
        "# Opcional: Exportar los resultados a un CSV para revisar más fácilmente\n",
        "output_rows = []\n",
        "\n",
        "for group_idx, grupo in enumerate(grupos_similares):\n",
        "    for idx in grupo:\n",
        "        output_rows.append({\n",
        "            'grupo': group_idx + 1,\n",
        "            'indice_original': idx,\n",
        "            'pregunta': df['instruction'].iloc[idx],\n",
        "            'respuesta': df['output'].iloc[idx],\n",
        "            'modelo': df['model'].iloc[idx] if 'model' in df.columns else 'Unknown'\n",
        "        })\n",
        "\n",
        "grupos_df = pd.DataFrame(output_rows)\n",
        "output_file = 'grupos_similares.csv'\n",
        "grupos_df.to_csv(output_file, index=False)\n",
        "print(f\"Resultados exportados a {output_file}\")\n",
        "\n",
        "# Descargar el archivo\n",
        "files.download(output_file)\n",
        "\n",
        "# Contar cuántas preguntas están en los grupos (posibles duplicados)\n",
        "num_duplicados = sum(len(grupo) for grupo in grupos_similares) - len(grupos_similares)\n",
        "print(f\"\\nNúmero total de posibles preguntas duplicadas: {num_duplicados}\")\n",
        "print(f\"Número de filas en el dataset original: {len(df)}\")\n",
        "print(f\"Número estimado de filas después de eliminar duplicados: {len(df) - num_duplicados}\")\n",
        "\n",
        "grupos_similares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJ7qPeJLTl28"
      },
      "source": [
        "# Finetunning Llama 3\n",
        "---Resumen\n",
        "Aqui poner que es unsloth que usamos y que modelos se pueden usar para hacer finetunning\n",
        "\n",
        "La biblioteca Unsloth permite completar el proceso de entrenamiento y entrenamiento fino 2x más rápido y requiere mucha menos VRAM gracias a derivaciones matemáticas complejas y kernels de GPUs optimizados manualmente\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSX6xhJ5TbIZ"
      },
      "outputs": [],
      "source": [
        "#%%Capture para evitar que genere salida en collab el comando de install\n",
        "%%capture\n",
        "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
        "!pip install unsloth\n",
        "# Get latest Unsloth\n",
        "!pip install --upgrade --no-deps \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YS0fAdKLiWlX"
      },
      "source": [
        "Debido a que tenemos recursos limitados vamos a usar modelos cuantizados en 4 bits para reducir el consumo de memoria y el espacio en disco que utilizan.\n",
        "\n",
        "¿Que significa cuantizar modelos?\n",
        "\n",
        "Normalmente los modelos almacenan sus parámetros en Punto Flotante de 16 Bits, al cuantizar los modelos de 16 Bits a 4 Bits conseguimos:\n",
        "\n",
        "✅ Menos uso de VRAM/RAM → Nos permite utilizar modelos más grandes que debido a limitaciones de google collab no podríamos ejecutar en este entorno.\n",
        "\n",
        "✅ Inferencia más rápida → Al reducir los datos aumenta la velocidad de cálculo.\n",
        "\n",
        "✅ Descarga más rápida → Al reducir el tamaño en disco que ocupan conseguimos ahorrarnos tiempo en descargar/subir los distintos modelos además de ahorrar espacio en google collab, el cual esta muy limitado.\n",
        "\n",
        "¿Que desventajas tiene?\n",
        "\n",
        "\n",
        "🔴 Pérdida de precisión → Reducir los bits disminuye la calidad de las respuestas, afectando tareas complejas.\n",
        "\n",
        "🔴 Problemas en cálculos precisos → Modelos cuantizados pueden fallar en matemáticas avanzadas o generación de código detallado.\n",
        "\n",
        "🟠  Fine-tuning más difícil → La cuantización a 4 bits reduce la precisión de los pesos. Unsloth utiliza técnicas optimizadas para reducir el efecto negativo que esto produce en el finetunning, pero sigue habiendo ligeras restricciones comparado con modelos en FP16 o FP32."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333,
          "referenced_widgets": [
            "27ae2827671c4143b0b2b2075fc1f972",
            "592e543174ac4c149b978a9b076e6947",
            "0a248e4eeb884e16966688be3c5f815b",
            "9c0b992d847a4edc8bae02bdb0a47a52",
            "c9b8ab31024948dbab3b9df93a5cc238",
            "62abec66de98471dbac0a2993bea23ad",
            "c1d1a9e630ae45a896a48c40d2ef9849",
            "2b0226de01884c16bd3824355fd2daa2",
            "6c0fad39a89342be9404a09ec7c4fcb9",
            "db50691252d44ad2ad8059a948b2a05f",
            "6a3b9333aeae4a00a21d6b857c5c54c1",
            "808f22940b3e4d56a044858af7717a82",
            "375e18cf4fae4b38935c18b1b026d194",
            "f7039d5c73024c2bb49232de870c0753",
            "b013a83d84e340ca9bdb032e04a4fe82",
            "629ec748ee184a8eb69a3cb65ec65e0f",
            "95dd39f5325241719f5838f21828ca20",
            "ff2bab5e8aad485a86195e2f69834c5e",
            "a86a7074d2aa4c0b9cc07304df896a09",
            "80ca55c4cb834e07a222a05eed953108",
            "f50acf3c440e46da9e2445e7a010f81d",
            "20804d2b3c7e4ef1ba3d67916d9768d1",
            "7661221145d04590a76c181231d8eb56",
            "df917f2ba37a41039d8305bce24a3c6d",
            "1b9f60aac66949cba9a3f20759d1f5c4",
            "c672b2b5cb574c33955eb644fbe35b98",
            "c47b1a19346a4a568abf1a3281233887",
            "1fbc81be877b4712933eb6e5c0a28cd4",
            "00a9009b6cb34da0b8d11233bafe42f9",
            "a816656658de4212968acb301df96937",
            "989344ac67d24b4098b718695ece3f9b",
            "4e3861b0138546da835a21cb88267d2e",
            "41c05166642345b0a00f7d186218be49",
            "75e83248694a401283c01c1f1052519d",
            "ebca870c675e4ade83fd5712b97a96f8",
            "d223583186b748afa0af48cf021b1ac6",
            "f802cc878c98461598a166e9b59fcd3f",
            "db15ee6f267a4620978d4ef3b822d711",
            "b264b66edb224b469dde034cec7af3a9",
            "a581b54fb35b483dbe8db53c11ab2e07",
            "13f56339d8ff4e548f3389b99b6cd09f",
            "256205400ed448e0a51efdd73e593eb8",
            "2fd1ce058941425da07b9eb0b22fd09f",
            "e80f507e19904e4fa56d9da77d4f535e",
            "6895588ff3ae4dfdb44f545b9f2c9a0a",
            "77256958d12141eb809dd0c585983dee",
            "0176b9971228499fa742bfce6f754663",
            "fdf7e9de3889464e940f1637fc8ec65e",
            "9e6500d06fad492783db0849ceb7f26b",
            "61950496fcb54235a6a0e1fff0991037",
            "b587a76b966544dba862b0eff56543b4",
            "3dcd01f64e374bc6ae6c1d587b81bac5",
            "939fd41a1efd49f6a0e5e880c8a6aa7e",
            "5e5c95ab789c470090fbee9c41bc596a",
            "503b8be124f24ea49d8061863ad68f0b"
          ]
        },
        "id": "g4aEbvOzUK-d",
        "outputId": "717721f4-7035-4211-ec56-06d5a6c1bb97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "Unsloth: Failed to patch Gemma3ForConditionalGeneration.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.51.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27ae2827671c4143b0b2b2075fc1f972",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "808f22940b3e4d56a044858af7717a82",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/220 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7661221145d04590a76c181231d8eb56",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/51.1k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "75e83248694a401283c01c1f1052519d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6895588ff3ae4dfdb44f545b9f2c9a0a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/345 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Se puede poner la longitud que se quiera, Unsloth utiliza RoPE Scaling\n",
        "dtype = None # None para detección automática de la GPU. Float16 para Tesla T4, V100, Bfloat16 para Ampere+\n",
        "load_in_4bit = True # Usamos 4bit para reducir el uso de memoria. Can be False.\n",
        "\n",
        "\n",
        "#Modelos 4bit cuantizados por unsloth\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
        "    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGPMaCTDqUyj"
      },
      "source": [
        "Ahora vamos a utilizar LoRA (Low-Rank Adapter), esto nos permite re-entrenar un modelo sin modificar toda su estructura. En lugar de ajustar todos los parámetros del modelo base, LoRA introduce dos nuevas matrices que se encargan de aprender y almacenar las actualizaciones específicas necesarias durante el fine-tuning.\n",
        "\n",
        "Una matriz se especializa en capturar las modificaciones necesarias durante el entrenamiento y la otra ayuda a combinarlas con los parámetros originales del modelo.\n",
        "\n",
        "Gracias a que solo se actualizan estas nuevas matrices en lugar de todos los parámetros del modelo original, podemos realizar fine-tunning de modelos más grandes con un hardware limitado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNWXr-KltxGq",
        "outputId": "0ffcb475-e060-483b-d066-5a541d0742d4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.3.19 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
          ]
        }
      ],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEZyUecvuras"
      },
      "source": [
        "## Preparación del dataset\n",
        "\n",
        "Para que Ollama y llama.cpp funcionen como un chatbot personalizado como por ejemplo ChatGPT, solo debemos tener 2 columnas: una de instrucciones y una de salida, por lo que nuestro dataset solo consta de esta dos columnas.\n",
        "\n",
        "Vamos a utilizar la librería de load_dataset del paquete datasets de hugging face, la cual nos facilita el uso de datasets para fine-tunning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "288b72f069454a308bbcdd817d8e0af2",
            "774c667bb01445a79cf8381a7dc73314",
            "4f2a96b0b9654e56a30bfe89a5386a23",
            "92c00c5cb9c24de1af7e40aa0ed15b47",
            "fc8181c289ff44c5a83a88684203906e",
            "08e35bdac488451b82a2f9b785aa6138",
            "f93543b90d1f4db491d07f743561389f",
            "e1fd1ac2fbb6434a97aedd66419f3a1f",
            "38afaf81d45f45fca24a96b6ff25a4b4",
            "5a5e5d22b7ae4273bbb8664a877397e2",
            "131b5d0fad894cdf9ac88fff9c123efe"
          ]
        },
        "id": "QmWNeFp-vHGZ",
        "outputId": "91e3d0fb-3389-4407-a20c-2946802e9e94"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "288b72f069454a308bbcdd817d8e0af2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['instruction', 'output', 'model']\n",
            "{'instruction': 'Si accedo a mi correo personal desde el ordenador del trabajo, ¿puede mi empleador leer mis mensajes?', 'output': 'No, tu empleador no puede acceder a tus correos personales sin tu consentimiento, incluso si usas un dispositivo corporativo. Solo puede supervisar las comunicaciones laborales y debe informarte sobre cualquier política de monitoreo. Sin embargo, es recomendable no usar equipos de empresa para asuntos personales.', 'model': 'deepseek/deepseek-chat-v3-0324:free'}\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\n",
        "    \"csv\",\n",
        "    data_files = \"/content/input.csv\",\n",
        "    split = \"train\",\n",
        ")\n",
        "print(dataset.column_names)\n",
        "print(dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-tD_iyYxTR4"
      },
      "source": [
        "Una vez ya tenemos cargado el dataset le vamos a dar un formato adecuado para fine-tunning.\n",
        "\n",
        "Vamos a utilizar la libreria to_sharegpt de Unsloth, para generar conversaciones largas combinando los inputs y outputs del dataset.\n",
        "\n",
        "Gracias a esto en vez de hacer el finetunning con preguntas sueltas, realizamos el entrenamiento con conversaciones simuladas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "dacc18e6fa914351b71468698aea27ff",
            "fd33365745b64ffe80031385cb1ae772",
            "a4b5c9120f6d484285cad69f75a4c4e7",
            "cd4a034727aa4a5eabe5d0160d2ca331",
            "22d6e496881c4d6d89d060a4c785c22c",
            "39ae9634aef84b949cef2f9107773320",
            "066a8a3400384015a2ab91be7925e856",
            "70c1b4c89dc04af4b88cec3d9fd76143",
            "4d5d028e5e304226b654fca8859514a4",
            "d7c0d3b5f98d4943aae99cb39a1a8974",
            "0ae11327bf7f40fa9a452c313fb4be49",
            "8879a8c8c73442f1913797a796271b23",
            "67f6191f7d934ba995d4deda864aa147",
            "f84e15027cc044b18041b2c14a6ca5b9",
            "c31c0805bee54f6e9d2ee25efb5455b3",
            "33f28884e71c4a3bba92ef0615ee3a48",
            "5dcf287617bb4db5ac7b65bcf81d1887",
            "c9bd4c42e09a459abd7574111b60812b",
            "f5943977ac2e47f79c80772f7aed2bd8",
            "53bd285ba55e428bbdcad85c808dca54",
            "d19d330b75eb4951ba2f4a65825d7eba",
            "08974acddcf14d3684295695bbe6fbac",
            "eaa5c7ea39344301bab459bc04d5716b",
            "2337590ca184415e9c27920a91f2bddb",
            "1a9d2ffe04bf4ba2a2f9c372307ab239",
            "c315c9e0934e4965ade0eddda9035103",
            "cdc832f653e340999903760c0fe93d51",
            "73d06604b4f540b58e9cf87c7f56152b",
            "3e53b04b3f754fc282692b31ea10baae",
            "9d4c6e36a1b8485d9f8371bd4e88e859",
            "7926b22c81d24b3faa488d7e8cd45e25",
            "7fdf5fdff9f64e749803ad4c4f4a5db7",
            "ac984ea7b70c4cda959655d375c978b3",
            "0067bf50ac57403d95bed599f67e05a2",
            "0aed6b8fffc9441dbe28059150085e39",
            "5650051de25b49b3a3cd2ec15dd6ef35",
            "e0b9c95bd1cb4c34ad24e7983d5fb019",
            "b2d07be45d1f4b1ea9845b302c32ef9e",
            "0104974262494a418b716c5fcbd36207",
            "dc68039b2b7840dabb1d39d90e90afd1",
            "6b61c3734c88480e84c4ca8d2ede49bf",
            "f6971aba51584d6e879ae4a026f71110",
            "f6a99c6a2a1d4de7b8ae59675718052f",
            "0dbbc41cb5da49c09557d77318e1d89a",
            "731686f94c174dc0b65556bca9eca0b1",
            "c01604f930e94bc79dc10369f36819b0",
            "931336892af0406cb159b2ebef825df3",
            "fa438c641a1e41c08b2ebbb4e87a2018",
            "8106f79da6974840a0c228354066bdac",
            "bf4f47b2da0344fe8f03eb26d982b96a",
            "9a665075297c4dd5b36cb32d5a4e76f8",
            "0d29128b3b8d42178f3ae6d6b4772d2f",
            "5095763539d846d39b9348e51bd7a627",
            "d34e92f473bf4d60924f5d9018132646",
            "fce4bd29180641d29cc7e6310706d109",
            "946f98b1fd3544509b709249cb96374b",
            "1843dac8987a4441819a184141373de2",
            "5c1ac9846bfb48d19298a110c29bf2e7",
            "278d6110c5d946748d5dd90256c326c9",
            "03ffb1d6834b444383139a123f373dd1",
            "5bd00d3d8b214109973049badc3cb4be",
            "39bc31df5c4947ae90a0bf862e739ae8",
            "4a8bbd1eb7a54f94a0933ea719e27a2e",
            "b5a109286ea842b0b2ba9534713c1ffe",
            "174183bcb63a4efa8f1420bfc4d0b5c8",
            "66106b47a3ba43ceb3b943dee3dbe3a0",
            "332acfff1e5e459c8d229d3a0d2169d1",
            "559cf60479a44452900421c985949a3a",
            "bdf00aa9b8da43b0b229c5a4566ff4bd",
            "fd633cad3d994284a8084f93c2668669",
            "6a329639b41149069cd87e5df3b7b5a9",
            "711c3e4a2fc94774931c340fcdd94039",
            "3cbca6985d8340a9b0e073a24622004f",
            "c7a644cb693544c9b42f2764cbd4abf8",
            "6e4fb3251724456fae6b6f13a6c12097",
            "05af9e6bb60047a3ad7e3142a177ca14",
            "1eabbd3c84064a2a86020c1c28b0426c",
            "a5fc373283e74ad38980e2a1c588619e",
            "a3b7c42a2be84209a595cc209ba12755",
            "69de33eb6326457298070f7fe84cfff1",
            "a8fb0593650f4007be70d0fbb0fdfced",
            "bdb97cdc500d4ef6be5175bdfd970914",
            "b5250b86c3a544929037a12dac2d898d",
            "887830af532541a88b552b499e751cf2",
            "743d80e6489542f7b9493fef1b9998a3",
            "2b400f884db74feb8be26093bdd792a2",
            "ab828a2583c44d1183da61e4663e1bba",
            "bb1368132ca94034bbae49c2150ad2e1"
          ]
        },
        "id": "U8H4EsAxxS1U",
        "outputId": "6986b6f9-f839-466e-d1c9-0ec7296cb4b5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dacc18e6fa914351b71468698aea27ff",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Merging columns:   0%|          | 0/796 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8879a8c8c73442f1913797a796271b23",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Converting to ShareGPT:   0%|          | 0/796 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eaa5c7ea39344301bab459bc04d5716b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Flattening the indices:   0%|          | 0/796 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0067bf50ac57403d95bed599f67e05a2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Flattening the indices:   0%|          | 0/796 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "731686f94c174dc0b65556bca9eca0b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Flattening the indices:   0%|          | 0/796 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "946f98b1fd3544509b709249cb96374b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Flattening the indices:   0%|          | 0/796 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "332acfff1e5e459c8d229d3a0d2169d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Flattening the indices:   0%|          | 0/796 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5fc373283e74ad38980e2a1c588619e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extending conversations:   0%|          | 0/796 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'conversations': [{'from': 'human',\n",
            "                    'value': \"La pregunta es: ('Si accedo a mi correo personal \"\n",
            "                             'desde el ordenador del trabajo, ¿puede mi '\n",
            "                             \"empleador leer mis mensajes?',).\"},\n",
            "                   {'from': 'gpt',\n",
            "                    'value': 'No, tu empleador no puede acceder a tus correos '\n",
            "                             'personales sin tu consentimiento, incluso si '\n",
            "                             'usas un dispositivo corporativo. Solo puede '\n",
            "                             'supervisar las comunicaciones laborales y debe '\n",
            "                             'informarte sobre cualquier política de '\n",
            "                             'monitoreo. Sin embargo, es recomendable no usar '\n",
            "                             'equipos de empresa para asuntos personales.'},\n",
            "                   {'from': 'human',\n",
            "                    'value': \"La pregunta es: ('¿Cómo puedo saber qué datos \"\n",
            "                             'personales recopila una aplicación móvil que he '\n",
            "                             \"instalado en mi móvil en España?',).\"},\n",
            "                   {'from': 'gpt',\n",
            "                    'value': 'Las aplicaciones móviles que operan en España '\n",
            "                             'deben informarte de manera clara y accesible '\n",
            "                             'sobre los datos personales que recopilan. Puedes '\n",
            "                             'encontrar esta información en la política de '\n",
            "                             'privacidad de la app, la cual suele estar '\n",
            "                             'disponible tanto en la tienda de aplicaciones '\n",
            "                             'antes de descargarla como dentro de la propia '\n",
            "                             'app (normalmente en secciones como \"Ajustes\", '\n",
            "                             '\"Información legal\" o \"Privacidad\"). Esta '\n",
            "                             'política debe detallar qué datos se recogen, con '\n",
            "                             'qué finalidad se usan y si se comparten con '\n",
            "                             'terceros.'},\n",
            "                   {'from': 'human',\n",
            "                    'value': 'La pregunta es: (\\'\"Una red social donde tengo '\n",
            "                             'cuenta ha sufrido una filtración que ha expuesto '\n",
            "                             'mis mensajes privados. ¿Qué derechos tengo como '\n",
            "                             'afectado y qué obligaciones tiene la '\n",
            "                             'plataforma?\"\\',).'},\n",
            "                   {'from': 'gpt',\n",
            "                    'value': '\"Como afectado, tienes derecho a ser informado '\n",
            "                             'de forma clara sobre el alcance de la filtración '\n",
            "                             'y los riesgos potenciales. La plataforma debe '\n",
            "                             'notificar el incidente a la AEPD en 72 horas si '\n",
            "                             'existe riesgo para los usuarios, tomar medidas '\n",
            "                             'técnicas para mitigar el daño y ofrecerte '\n",
            "                             'orientación sobre cómo protegerte. Puedes '\n",
            "                             'ejercer tus derechos de acceso, supresión o '\n",
            "                             'reclamación si consideras que no han actuado con '\n",
            "                             'la diligencia debida.\"'},\n",
            "                   {'from': 'human',\n",
            "                    'value': \"La pregunta es: ('¿Puede una aplicación móvil \"\n",
            "                             'recopilar mi ubicación en segundo plano sin mi '\n",
            "                             \"permiso explícito en España?',).\"},\n",
            "                   {'from': 'gpt',\n",
            "                    'value': 'No, en España una aplicación móvil necesita tu '\n",
            "                             'consentimiento explícito e informado para '\n",
            "                             'acceder a tu ubicación en segundo plano, es '\n",
            "                             'decir, cuando no la estás usando activamente. '\n",
            "                             'Deben explicarte claramente para qué necesitan '\n",
            "                             'esa información y sólo pueden recogerla si es '\n",
            "                             'estrictamente necesario para la finalidad que te '\n",
            "                             'han comunicado. Siempre debes tener la opción de '\n",
            "                             'retirar ese permiso o desactivar la '\n",
            "                             'geolocalización fácilmente desde los ajustes de '\n",
            "                             'la app o del propio dispositivo.'},\n",
            "                   {'from': 'human',\n",
            "                    'value': \"La pregunta es: ('Si una empresa en España usa \"\n",
            "                             'IA para procesar mis datos, ¿están obligados a '\n",
            "                             \"decírmelo y explicarme para qué?',).\"},\n",
            "                   {'from': 'gpt',\n",
            "                    'value': 'Sí, las organizaciones que usan inteligencia '\n",
            "                             'artificial para tratar tus datos personales en '\n",
            "                             'España deben informarte de forma transparente '\n",
            "                             'sobre ello. Esta información debe incluir '\n",
            "                             'claramente la finalidad para la que se usan tus '\n",
            "                             'datos y si la IA toma decisiones automatizadas '\n",
            "                             'que te afecten significativamente. Si esas '\n",
            "                             'decisiones tienen efectos importantes (legales o '\n",
            "                             'similares), tienes además derecho a obtener '\n",
            "                             'información significativa sobre la lógica '\n",
            "                             'utilizada, así como a impugnar la decisión y '\n",
            "                             'solicitar intervención humana.'}]}\n"
          ]
        }
      ],
      "source": [
        "from unsloth import to_sharegpt\n",
        "dataset = to_sharegpt(\n",
        "    dataset,\n",
        "    merged_prompt = \\\n",
        "        \"[[La pregunta es: {instruction}.]]\"\n",
        "        ,\n",
        "    conversation_extension = 5, # Este parámetro agrupa aleatoriamente hasta x preguntas y respuestas en una sola conversación. Esto es útil para simular conversaciones más largas.\n",
        "    output_column_name = \"output\",\n",
        ")\n",
        "#Para imprimir como se ve ahora el dataset\n",
        "from pprint import pprint\n",
        "pprint(dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXKKSvVgzYB-"
      },
      "source": [
        "El dataset ahora utiliza etiquetas como \"human\" y \"gpt\":\n",
        "\n",
        "```\n",
        "{'conversations': [{'from': 'human',\n",
        "                    'value': \"La pregunta es: ('Antonio instala cámaras en su \"\n",
        "                             'tienda sin avisar a empleados y clientes. ¿Qué '\n",
        "                             \"consecuencias puede tener?',).\"},\n",
        "                   {'from': 'gpt',\n",
        "                    'value': 'Según el Artículo 22 de la Ley Orgánica 3/2018 '\n",
        "                             'de Protección de Datos Personales y Garantía de '\n",
        "                             'los Derechos Digitales, debe informar a los '\n",
        "                             'afectados sobre la videovigilancia. Si no lo '\n",
        "                             'hace, puede enfrentar sanciones de la AEPD.'},\n",
        "```\n",
        "Sin embargo para un modelo de OpenAI o Hugging Face, se requieren etiquetas estándar como **user** para el usuario y **assistant** para el modelo.\n",
        "\n",
        "Para arreglar esto vamos a utilizar la libreria standardize_sharegpt de Unsloth que cambia todas las etiquetas como human, gpt, system, etc... a **user** y **assistant**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "34b786a9cd8d42b7b1c69ca91d3c9359",
            "ccdb1a7be02b4f77a4635f1612be9360",
            "6daa63e3012b435a887107e392aa6651",
            "a3ee21e71ae44f1ebbe0f6e2e06bcbcb",
            "36c4e3436ae64ee9bbf784aa8c39c06b",
            "014e363aae62423787d393d949df2c9b",
            "157dcb1f0bf242139f3546cc93b7b936",
            "c893c659fe674f73a391219bcc9ce9f2",
            "fe2cb4a4a87e425bb741dc2e96cb3c19",
            "9585c39899db42858cecf93aff9d5c15",
            "982efc40a70245b7ad8468c8bede757b"
          ]
        },
        "id": "4HR7_HxXysv7",
        "outputId": "3277c44c-6aae-486a-cd1f-ce1d9028eb70"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "34b786a9cd8d42b7b1c69ca91d3c9359",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Unsloth: Standardizing formats (num_proc=2):   0%|          | 0/796 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'conversations': [{'content': \"La pregunta es: ('Si accedo a mi correo \"\n",
            "                               'personal desde el ordenador del trabajo, '\n",
            "                               \"¿puede mi empleador leer mis mensajes?',).\",\n",
            "                    'role': 'user'},\n",
            "                   {'content': 'No, tu empleador no puede acceder a tus '\n",
            "                               'correos personales sin tu consentimiento, '\n",
            "                               'incluso si usas un dispositivo corporativo. '\n",
            "                               'Solo puede supervisar las comunicaciones '\n",
            "                               'laborales y debe informarte sobre cualquier '\n",
            "                               'política de monitoreo. Sin embargo, es '\n",
            "                               'recomendable no usar equipos de empresa para '\n",
            "                               'asuntos personales.',\n",
            "                    'role': 'assistant'},\n",
            "                   {'content': \"La pregunta es: ('¿Cómo puedo saber qué datos \"\n",
            "                               'personales recopila una aplicación móvil que '\n",
            "                               \"he instalado en mi móvil en España?',).\",\n",
            "                    'role': 'user'},\n",
            "                   {'content': 'Las aplicaciones móviles que operan en España '\n",
            "                               'deben informarte de manera clara y accesible '\n",
            "                               'sobre los datos personales que recopilan. '\n",
            "                               'Puedes encontrar esta información en la '\n",
            "                               'política de privacidad de la app, la cual '\n",
            "                               'suele estar disponible tanto en la tienda de '\n",
            "                               'aplicaciones antes de descargarla como dentro '\n",
            "                               'de la propia app (normalmente en secciones '\n",
            "                               'como \"Ajustes\", \"Información legal\" o '\n",
            "                               '\"Privacidad\"). Esta política debe detallar qué '\n",
            "                               'datos se recogen, con qué finalidad se usan y '\n",
            "                               'si se comparten con terceros.',\n",
            "                    'role': 'assistant'},\n",
            "                   {'content': 'La pregunta es: (\\'\"Una red social donde tengo '\n",
            "                               'cuenta ha sufrido una filtración que ha '\n",
            "                               'expuesto mis mensajes privados. ¿Qué derechos '\n",
            "                               'tengo como afectado y qué obligaciones tiene '\n",
            "                               'la plataforma?\"\\',).',\n",
            "                    'role': 'user'},\n",
            "                   {'content': '\"Como afectado, tienes derecho a ser informado '\n",
            "                               'de forma clara sobre el alcance de la '\n",
            "                               'filtración y los riesgos potenciales. La '\n",
            "                               'plataforma debe notificar el incidente a la '\n",
            "                               'AEPD en 72 horas si existe riesgo para los '\n",
            "                               'usuarios, tomar medidas técnicas para mitigar '\n",
            "                               'el daño y ofrecerte orientación sobre cómo '\n",
            "                               'protegerte. Puedes ejercer tus derechos de '\n",
            "                               'acceso, supresión o reclamación si consideras '\n",
            "                               'que no han actuado con la diligencia debida.\"',\n",
            "                    'role': 'assistant'},\n",
            "                   {'content': \"La pregunta es: ('¿Puede una aplicación móvil \"\n",
            "                               'recopilar mi ubicación en segundo plano sin mi '\n",
            "                               \"permiso explícito en España?',).\",\n",
            "                    'role': 'user'},\n",
            "                   {'content': 'No, en España una aplicación móvil necesita tu '\n",
            "                               'consentimiento explícito e informado para '\n",
            "                               'acceder a tu ubicación en segundo plano, es '\n",
            "                               'decir, cuando no la estás usando activamente. '\n",
            "                               'Deben explicarte claramente para qué necesitan '\n",
            "                               'esa información y sólo pueden recogerla si es '\n",
            "                               'estrictamente necesario para la finalidad que '\n",
            "                               'te han comunicado. Siempre debes tener la '\n",
            "                               'opción de retirar ese permiso o desactivar la '\n",
            "                               'geolocalización fácilmente desde los ajustes '\n",
            "                               'de la app o del propio dispositivo.',\n",
            "                    'role': 'assistant'},\n",
            "                   {'content': \"La pregunta es: ('Si una empresa en España usa \"\n",
            "                               'IA para procesar mis datos, ¿están obligados a '\n",
            "                               \"decírmelo y explicarme para qué?',).\",\n",
            "                    'role': 'user'},\n",
            "                   {'content': 'Sí, las organizaciones que usan inteligencia '\n",
            "                               'artificial para tratar tus datos personales en '\n",
            "                               'España deben informarte de forma transparente '\n",
            "                               'sobre ello. Esta información debe incluir '\n",
            "                               'claramente la finalidad para la que se usan '\n",
            "                               'tus datos y si la IA toma decisiones '\n",
            "                               'automatizadas que te afecten '\n",
            "                               'significativamente. Si esas decisiones tienen '\n",
            "                               'efectos importantes (legales o similares), '\n",
            "                               'tienes además derecho a obtener información '\n",
            "                               'significativa sobre la lógica utilizada, así '\n",
            "                               'como a impugnar la decisión y solicitar '\n",
            "                               'intervención humana.',\n",
            "                    'role': 'assistant'}]}\n"
          ]
        }
      ],
      "source": [
        "from unsloth import standardize_sharegpt\n",
        "dataset = standardize_sharegpt(dataset)\n",
        "\n",
        "#Para imprimir como se ve ahora el dataset\n",
        "from pprint import pprint\n",
        "pprint(dataset[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lo0Yv5Mt00NF"
      },
      "source": [
        "## Plantilla de conversación con el modelo\n",
        "\n",
        "Una plantilla de chat es útil para el fine-tuning porque proporciona una estructura coherente que enseña al modelo cómo interactuar de una mejor manera en las conversaciones, y por lo tanto mejorar la calidad de sus respuestas.\n",
        "\n",
        "Este es el formato de  un Prompt de Llama-3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "e0058619ab004c3891f1f211e7138bf2",
            "8521f1b081c8426ab052535bccb87a51",
            "777674f274264d22ade6581c5b9de8e6",
            "cecb244f1d094f188023e10ea1863471",
            "fda1be9042914fec8a145e0cac60ca01",
            "3d274df88eac4e16b4c220b1a6372a57",
            "e5caecafe5ba42fbbdeabcc688980df9",
            "d7e598dfeef2432eb628d35ff2a1628e",
            "9f97bbebf0e140fab4945e412b6ed2fa",
            "3e4bc9f043854488b878656483ad6688",
            "0561192c90dd4ea6a120a2b9940e5ab8"
          ]
        },
        "id": "J2SRFD38096V",
        "outputId": "689bd2bd-301d-4a5f-c54b-d55a6d55338a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e0058619ab004c3891f1f211e7138bf2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/796 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#chat_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "#{SYSTEM}<|end_of_text|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "#{INPUT}<|end_of_text|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "#{OUTPUT}<|end_of_text|>\"\"\"\n",
        "\n",
        "chat_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "{SYSTEM}<|eot_id|>\n",
        "<|start_header_id|>user<|end_header_id|>\n",
        "{INPUT}<|eot_id|>\n",
        "<|start_header_id|>assistant<|end_header_id|>\n",
        "{OUTPUT}<|eot_id|>\"\"\"\n",
        "\n",
        "from unsloth import apply_chat_template\n",
        "dataset = apply_chat_template(\n",
        "    dataset,\n",
        "    tokenizer = tokenizer,\n",
        "    chat_template = chat_template,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htc-yqdK6Ny_"
      },
      "source": [
        "## Entrenamiento del modelo\n",
        "\n",
        "Vamos a utilizar el Transformers Reinforcement Learning (TRL) de Hugginface **SFTTrainer** (Supervised Fine-Tuning) el cual permite entrenar modelos preexistentes con datos etiquetados para mejorar su desempeño en tareas específicas.\n",
        "\n",
        "Tambien se puede utilizar DPOTrainer: Utiliza el aprendizaje por refuerzo directo (Direct Preference Optimization) para mejorar las respuestas generadas por el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "6ce15f1736b343948196bb520cae9a84",
            "a42fb19dbd8f4557bf3bc62f79c729bd",
            "2b8e98da62ad465793e858bbdc7db9ab",
            "8bb5db854b6743218980a1909031f9d1",
            "6a4f221663c444b3a82a71f850162467",
            "de7427201d2d46b0a507f4e43dec20ed",
            "15a9b66c449a43ee9e759b29457e1df3",
            "40217752dc6245a1b2c67ec4e79e379c",
            "d9476b1342c74f95b72318a5b809402b",
            "f604e6f0f77c4d63bf49f4728d411f73",
            "4bebb5119aab4cd196509bd84f5e6015"
          ]
        },
        "id": "6R0i0OqB597z",
        "outputId": "f2472b7f-f5dc-4c65-d448-c1bef834c3cd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ce15f1736b343948196bb520cae9a84",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/796 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = Tesla T4. Max memory = 14.741 GB.\n",
            "7.117 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = 2048,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 40,\n",
        "        #max_steps=None,\n",
        "        num_train_epochs=4,\n",
        "        learning_rate =  5e-5,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 10,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")\n",
        "\n",
        "#Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rdUVpTcH7ThB"
      },
      "source": [
        "Es necesario tener una cuenta en https://wandb.ai/authorize y obtener la clave API para poder hacer el entrenamiento\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "F3SWM2RF7Vu7",
        "outputId": "a8b89a15-488a-4162-b940-753cce76f85e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 796 | Num Epochs = 4 | Total steps = 396\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 20,971,520/8,000,000,000 (0.26% trained)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mserdom02\u001b[0m (\u001b[33mserdom02-complutense-university-of-madrid\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250420_104748-ronsf90v</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/serdom02-complutense-university-of-madrid/huggingface/runs/ronsf90v' target=\"_blank\">outputs</a></strong> to <a href='https://wandb.ai/serdom02-complutense-university-of-madrid/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/serdom02-complutense-university-of-madrid/huggingface' target=\"_blank\">https://wandb.ai/serdom02-complutense-university-of-madrid/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/serdom02-complutense-university-of-madrid/huggingface/runs/ronsf90v' target=\"_blank\">https://wandb.ai/serdom02-complutense-university-of-madrid/huggingface/runs/ronsf90v</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='396' max='396' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [396/396 1:45:36, Epoch 3/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>2.698900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.139500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.624100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.315700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>1.149500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.028300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.977400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.931700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.898700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.865500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.781900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.750500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.720900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.674200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.627300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.592300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.568200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.546100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.510200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.472400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.401300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.360700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.355300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.334800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.314600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.301000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.273000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.257100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.247900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.233100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.192100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.178700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.172900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.162700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.160000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.154000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.154600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.145100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.143300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8iM7gau7ZYo"
      },
      "source": [
        "### Memoria Final y estadisticas de tiempo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EvKJml8M7eBY",
        "outputId": "5b397e9d-6d70-4ded-8104-4f8f7d634950"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4530.4901 seconds used for training.\n",
            "75.51 minutes used for training.\n",
            "Peak reserved memory = 7.23 GB.\n",
            "Peak reserved memory for training = 1.812 GB.\n",
            "Peak reserved memory % of max memory = 49.047 %.\n",
            "Peak reserved memory for training % of max memory = 12.292 %.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRBXK1wG7u4U"
      },
      "source": [
        "\n",
        "### Inferencia\n",
        "Vamos a ejecutar el modelo, Unsloth hace la inferencia de manera nativa 2 veces más rápida. Hay que usar promtp similares al finetunning para obtener buenos resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-i_hr1J73N0",
        "outputId": "0f1288b8-62d2-4938-938f-7dfaec7691d1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "El colegio necesita el consentimiento explícito de los padres o tutores legales para publicar imágenes de alumnos menores de edad en su web o redes sociales.<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "messages = [                    # Change below!\n",
        "    #{\"role\": \"user\", \"content\": '¿Cuál es el objeto de la Ley Orgánica 3/2018 según el Artículo 1? \\n'},\n",
        "    {\"role\": \"user\", \"content\": '\"¿Puede el colegio hacer fotos a los alumnos y publicarlas en la web del colegio?\"\\n'},\n",
        "    #{\"role\": \"user\", \"content\": '\"¿Cuál es el deber de confidencialidad según el Artículo 5 de la Ley Orgánica 3/2018?\"\\n'},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGohnNGT8cLm"
      },
      "source": [
        "## Guardar y Cargar el modelo finetuneado\n",
        "\n",
        "Para guardar el modelo final como adaptadores LoRA, utiliza `push_to_hub` de Huggingface para guardarlo en línea o `save_pretrained` para guardarlo localmente.\n",
        "\n",
        "[NOTA] Esto SOLO guarda los adaptadores LoRA, no el modelo completo. ¡Para guardarlo en 16bit o GGUF, baja más abajo!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXD4le6y94uc",
        "outputId": "575d74e3-332e-46f5-8e9e-4e7de86999a6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('lora_model/tokenizer_config.json',\n",
              " 'lora_model/special_tokens_map.json',\n",
              " 'lora_model/tokenizer.json')"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(\"lora_model\") # Local saving\n",
        "tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29AH6HEI98bC"
      },
      "source": [
        "Ahora, si deseas cargar los adaptadores LoRA que acabamos de guardar para inferencia, cambia False a True:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "QJma3s7D97yP",
        "outputId": "616afe56-d15e-4dca-d40e-b16b5f8049de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.3.19: Fast Llama patching. Transformers: 4.51.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-552d0118ec86>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0munsloth\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFastLanguageModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     model, tokenizer = FastLanguageModel.from_pretrained(\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"lora_model\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# YOUR MODEL YOU USED FOR TRAINING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmax_seq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/loader.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, load_in_8bit, full_finetuning, token, device_map, rope_scaling, fix_tokenizer, trust_remote_code, use_gradient_checkpointing, resize_model_vocab, revision, use_exact_model_name, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, *args, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         model, tokenizer = dispatch_model.from_pretrained(\n\u001b[0m\u001b[1;32m    364\u001b[0m             \u001b[0mmodel_name\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0mmax_seq_length\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/unsloth/models/llama.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(model_name, max_seq_length, dtype, load_in_4bit, token, device_map, rope_scaling, fix_tokenizer, model_patcher, tokenizer_name, trust_remote_code, fast_inference, gpu_memory_utilization, float8_kv_cache, random_state, max_lora_rank, disable_log_stats, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1779\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfast_inference\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1780\u001b[0;31m             model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m   1781\u001b[0m                 \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0mdevice_map\u001b[0m              \u001b[0;34m=\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    572\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4378\u001b[0m         \u001b[0;31m# Prepare the full device map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevice_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4380\u001b[0;31m             \u001b[0mdevice_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_in_fp32_regex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4382\u001b[0m         \u001b[0;31m# Finalize model weight initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_get_device_map\u001b[0;34m(model, device_map, max_memory, hf_quantizer, torch_dtype, keep_in_fp32_regex)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m             \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdevice_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"disk\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    105\u001b[0m                     \u001b[0;34m\"Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                     \u001b[0;34m\"quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
          ]
        }
      ],
      "source": [
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "pass\n",
        "\n",
        "messages = [                    # Change below!\n",
        "    {\"role\": \"user\", \"content\": '¿Puede un trabajador de un supermercado pedirme el DNI?\\n'\\\n",
        "                                '\\n'\\\n",
        "                                ''},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XnT-Voc_6Hr"
      },
      "source": [
        "A continuación, guardaremos el modelo en GGUF / llama.cpp.\n",
        "\n",
        "Clonamos llama.cpp y por defecto lo guardamos en q8_0. Permitimos todos los métodos como q4_k_m. Utiliza `save_pretrained_gguf` para guardarlo localmente y `push_to_hub_gguf` para subirlo a Hugging Face.\n",
        "\n",
        "Algunos métodos de cuantificación compatibles (lista completa en nuestra página de Wiki):\n",
        "\n",
        "- q8_0: Conversión rápida. Uso de recursos alto, pero generalmente aceptable.\n",
        "- q4_k_m: Recomendado. Utiliza Q6_K para la mitad de los tensores `attention.wv` y `feed_forward.w2`, el resto usa Q4_K.\n",
        "- q5_k_m: Recomendado. Utiliza Q6_K para la mitad de los tensores `attention.wv` y `feed_forward.w2`, el resto usa Q5_K.\n",
        "\n",
        "¡También soportamos guardar en múltiples opciones de GGUF en formato de lista! Esto puede acelerar el proceso en 10 minutos o más si deseas varios formatos de exportación.\n",
        "\n",
        "El siguiente codigo es para guardar el modelo en la carpeta /model, es necesario tener una api key en hugginface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "08f864ad78c8485bb45fa50dda7f188a",
            "6cbfef56108b4992b385d82ff36cdbc4",
            "1c7015ec55f545b8a1a26cbf8e32cc39",
            "a4e479e28ce94b0f9b63dd4c435e6e7a",
            "81acfacf75ef46d88c4b530896c1d6bb",
            "58e4b815ec4a4747a0b56e33a540c9bb",
            "51ff0c36700e4c92866ea7239e40e7f3",
            "741258f9827546319a1dc4d2f892210a",
            "e10e80363b754204887fffc4af7cd3d8",
            "08746819f1ba4895bcbc13fd0ecbd0ac",
            "fec29fe05b334fb488537ab92066ddb5",
            "968ab1d17382458daefc7e2724c6912e",
            "26d9440af08a4404abe52a76f096bb6e",
            "355096deee0842068a23019ab8d5ee1c",
            "c5f6ba1f51a1425b90b3946ca66cc685",
            "43ec4ef991ca471fb7b409eee3553ed0",
            "138f00d720ac475c9c752dd46e878db2",
            "3c65196ca805431ebb766846980c6d11",
            "0ae3f54842c04d6ea1b5fa1be3bfbc5e",
            "f7a60f172ea441df9fde0e5852305f57",
            "9392415e91b049ac9b173eaecb6e8c5f",
            "33893db8b278422ba6f508f5a9c78177"
          ]
        },
        "collapsed": true,
        "id": "0TIoxYH5__Au",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "1b5a38ee-6601-4ca0-8d51-bf382b357e86"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: ##### The current model auto adds a BOS token.\n",
            "Unsloth: ##### Your chat template has a BOS token. We shall remove it temporarily.\n",
            "Unsloth: You have 1 CPUs. Using `safe_serialization` is 10x slower.\n",
            "We shall switch to Pytorch saving, which might take 3 minutes and not 30 minutes.\n",
            "To force `safe_serialization`, set it to `None` instead.\n",
            "Unsloth: Kaggle/Colab has limited disk space. We need to delete the downloaded\n",
            "model which will save 4-16GB of disk space, allowing you to save on Kaggle/Colab.\n",
            "Unsloth: Will remove a cached repo with size 5.7G\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
            "Unsloth: Will use up to 4.87 out of 12.67 RAM for saving.\n",
            "Unsloth: Saving model... This might take 5 minutes ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  9%|▉         | 3/32 [00:00<00:04,  6.69it/s]\n",
            "We will save to Disk and not RAM now.\n",
            "100%|██████████| 32/32 [03:10<00:00,  5.96s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Saving tokenizer... Done.\n",
            "Unsloth: Saving serdom02/Leyeneitor_8bitQ8_0/pytorch_model-00001-of-00004.bin...\n",
            "Unsloth: Saving serdom02/Leyeneitor_8bitQ8_0/pytorch_model-00002-of-00004.bin...\n",
            "Unsloth: Saving serdom02/Leyeneitor_8bitQ8_0/pytorch_model-00003-of-00004.bin...\n",
            "Unsloth: Saving serdom02/Leyeneitor_8bitQ8_0/pytorch_model-00004-of-00004.bin...\n",
            "Done.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Converting llama model. Can use fast conversion = False.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth: Conversion from QLoRA to GGUF information\n",
            "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
            "O^O/ \\_/ \\    [1] Converting HF to GGUF 16bits might take 3 minutes.\n",
            "\\        /    [2] Converting GGUF 16bits to ['q8_0'] might take 10 minutes each.\n",
            " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
            "\n",
            "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
            "Unsloth: CMAKE detected. Finalizing some steps for installation.\n",
            "Unsloth: [1] Converting model at serdom02/Leyeneitor_8bitQ8_0 into q8_0 GGUF format.\n",
            "The output location will be /content/serdom02/Leyeneitor_8bitQ8_0/unsloth.Q8_0.gguf\n",
            "This might take 3 minutes...\n",
            "INFO:hf-to-gguf:Loading model: Leyeneitor_8bitQ8_0\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'pytorch_model.bin.index.json'\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00001-of-00004.bin'\n",
            "INFO:hf-to-gguf:token_embd.weight,           torch.float16 --> Q8_0, shape = {4096, 128256}\n",
            "INFO:hf-to-gguf:blk.0.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.0.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.0.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.0.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.0.ffn_down.weight,       torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.0.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.0.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.1.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.1.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.1.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.1.ffn_down.weight,       torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.1.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.1.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.2.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.2.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.2.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.2.ffn_down.weight,       torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.2.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.2.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.3.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.3.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.3.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.3.ffn_down.weight,       torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.3.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.3.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.4.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.4.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.4.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.4.ffn_down.weight,       torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.4.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.4.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.5.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.5.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.5.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.5.ffn_down.weight,       torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.5.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.5.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.6.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.6.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.6.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.6.ffn_down.weight,       torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.6.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.6.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.7.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.7.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.7.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.7.ffn_down.weight,       torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.7.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.7.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.8.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.8.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.8.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.8.ffn_down.weight,       torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.8.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.8.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00002-of-00004.bin'\n",
            "INFO:hf-to-gguf:blk.9.attn_q.weight,         torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_k.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_v.weight,         torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.9.attn_output.weight,    torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.9.ffn_gate.weight,       torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.9.ffn_up.weight,         torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.9.ffn_down.weight,       torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.9.attn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.9.ffn_norm.weight,       torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.10.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.10.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.10.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.10.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.10.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.10.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.11.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.11.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.11.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.11.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.11.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.11.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.12.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.12.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.12.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.12.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.12.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.12.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.13.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.13.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.13.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.13.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.13.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.13.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.14.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.14.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.14.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.14.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.14.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.14.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.15.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.15.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.15.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.15.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.15.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.15.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.16.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.16.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.16.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.16.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.16.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.16.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.17.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.17.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.17.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.17.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.17.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.17.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.18.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.18.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.18.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.18.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.18.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.18.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.19.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.19.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.19.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.19.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.19.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.19.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.20.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.20.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00003-of-00004.bin'\n",
            "INFO:hf-to-gguf:blk.20.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.20.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.20.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.20.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.21.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.21.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.21.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.21.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.21.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.21.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.22.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.22.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.22.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.22.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.22.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.22.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.23.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.23.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.23.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.23.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.23.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.23.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.24.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.24.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.24.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.24.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.24.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.24.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.25.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.25.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.25.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.25.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.25.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.25.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.26.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.26.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.26.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.26.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.26.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.26.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.27.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.27.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.27.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.27.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.27.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.27.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.28.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.28.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.28.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.28.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.28.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.28.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.28.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.29.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.29.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.29.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.29.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.29.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.29.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.29.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.30.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.30.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.30.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.30.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.30.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.30.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.30.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_q.weight,        torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_k.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.31.attn_v.weight,        torch.float16 --> Q8_0, shape = {4096, 1024}\n",
            "INFO:hf-to-gguf:blk.31.attn_output.weight,   torch.float16 --> Q8_0, shape = {4096, 4096}\n",
            "INFO:hf-to-gguf:blk.31.ffn_gate.weight,      torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:blk.31.ffn_up.weight,        torch.float16 --> Q8_0, shape = {4096, 14336}\n",
            "INFO:hf-to-gguf:gguf: loading model part 'pytorch_model-00004-of-00004.bin'\n",
            "INFO:hf-to-gguf:blk.31.ffn_down.weight,      torch.float16 --> Q8_0, shape = {14336, 4096}\n",
            "INFO:hf-to-gguf:blk.31.attn_norm.weight,     torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:blk.31.ffn_norm.weight,      torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:output_norm.weight,          torch.float16 --> F32, shape = {4096}\n",
            "INFO:hf-to-gguf:output.weight,               torch.float16 --> Q8_0, shape = {4096, 128256}\n",
            "INFO:hf-to-gguf:Set meta model\n",
            "INFO:hf-to-gguf:Set model parameters\n",
            "INFO:hf-to-gguf:gguf: context length = 8192\n",
            "INFO:hf-to-gguf:gguf: embedding length = 4096\n",
            "INFO:hf-to-gguf:gguf: feed forward length = 14336\n",
            "INFO:hf-to-gguf:gguf: head count = 32\n",
            "INFO:hf-to-gguf:gguf: key-value head count = 8\n",
            "INFO:hf-to-gguf:gguf: rope theta = 500000.0\n",
            "INFO:hf-to-gguf:gguf: rms norm epsilon = 1e-05\n",
            "INFO:hf-to-gguf:gguf: file type = 7\n",
            "INFO:hf-to-gguf:Set model tokenizer\n",
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n",
            "INFO:gguf.vocab:Adding 280147 merge(s).\n",
            "INFO:gguf.vocab:Setting special token type bos to 128000\n",
            "INFO:gguf.vocab:Setting special token type eos to 128009\n",
            "INFO:gguf.vocab:Setting special token type pad to 128255\n",
            "INFO:gguf.vocab:Setting chat_template to {% if messages[0]['role'] == 'system' %}{{ '<|start_header_id|>system<|end_header_id|>\n",
            "' + messages[0]['content'] + '<|eot_id|>' }}{% set loop_messages = messages[1:] %}{% else %}{{ '<|start_header_id|>system<|end_header_id|>\n",
            "Below are some instructions that describe some tasks. Write responses that appropriately complete each request.<|eot_id|>' }}{% set loop_messages = messages %}{% endif %}{% for message in loop_messages %}{% if message['role'] == 'user' %}{{ '\n",
            "<|start_header_id|>user<|end_header_id|>\n",
            "' + message['content'] + '<|eot_id|>' }}{% elif message['role'] == 'assistant' %}{{ '\n",
            "<|start_header_id|>assistant<|end_header_id|>\n",
            "' + message['content'] + '<|eot_id|>' }}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '\n",
            "<|start_header_id|>assistant<|end_header_id|>\n",
            "' }}{% endif %}\n",
            "INFO:hf-to-gguf:Set model quantization version\n",
            "INFO:gguf.gguf_writer:Writing the following files:\n",
            "INFO:gguf.gguf_writer:/content/serdom02/Leyeneitor_8bitQ8_0/unsloth.Q8_0.gguf: n_tensors = 291, total_size = 8.5G\n",
            "Writing: 100%|██████████| 8.53G/8.53G [03:59<00:00, 35.7Mbyte/s]\n",
            "INFO:hf-to-gguf:Model successfully exported to /content/serdom02/Leyeneitor_8bitQ8_0/unsloth.Q8_0.gguf\n",
            "Unsloth: Conversion completed! Output location: /content/serdom02/Leyeneitor_8bitQ8_0/unsloth.Q8_0.gguf\n",
            "Unsloth: Uploading GGUF to Huggingface Hub...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "08f864ad78c8485bb45fa50dda7f188a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "968ab1d17382458daefc7e2724c6912e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "unsloth.Q8_0.gguf:   0%|          | 0.00/8.54G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: ##### The current model auto adds a BOS token.\n",
            "Unsloth: ##### We removed it in GGUF's chat template for you.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved GGUF to https://huggingface.co/serdom02/Leyeneitor_8bitQ8_0\n"
          ]
        }
      ],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if True: model.push_to_hub_gguf(\"serdom02/Leyeneitor_8bitQ8_0\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"serdom02/model_16bitGGUF\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"serdom02/model_q4_k_mGGUF\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"serdom02/model\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTjohupyAgKP"
      },
      "source": [
        "## Si no subimos el modelo a Hugging Face tenemos que crear el modelo de Ollama\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cXYtIa2EMH3"
      },
      "source": [
        "Ollama necesita un archivo de modelo (Modelfile), que especifica el formato del prompt del modelo. Vamos a imprimir el generado automáticamente por Unsloth:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "vmgNLwsbERBS",
        "outputId": "225ab7e3-56bc-409a-86ed-336bfc122672"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'tokenizer' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-e1f679953d38>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ollama_modelfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ],
      "source": [
        "print(tokenizer._ollama_modelfile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5P8Ddl2E5wL"
      },
      "source": [
        "Ahora crearemos un modelo de Ollama llamado `unsloth_model` utilizando el archivo de modelo (Modelfile) que generamos automáticamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBlAipXXE6C3"
      },
      "outputs": [],
      "source": [
        "!ollama create unsloth_model -f ./model/Modelfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KGlR5y7pBkQ0"
      },
      "source": [
        "## Descargar el modelo a nuestro ordenador\n",
        "\n",
        "Con este codigo creamos un zip con el modelo para poder descargarlo todo junto a nuestro ordenador local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7GhWuu_nBnQ3"
      },
      "outputs": [],
      "source": [
        "!zip -r /content/file.zip /content/model\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"/content/file.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3I0aeb-eD26h"
      },
      "source": [
        "Si se ejecuta en el ordenador personal en vez de en colab hay que cambiar la ruta dentro del archivo Modelfile: ![Sin título.png](data:image/png;base64,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)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJdUc4ji-Q8c"
      },
      "source": [
        "# Ollama\n",
        "\n",
        "Unsloth ahora permite ajustar el modelo automáticamente y crear un archivo de modelo, exportando a Ollama.\n",
        "\n",
        "Primero vamos a installar Ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O15au36B-Vyp",
        "outputId": "75742f0a-c04f-4774-dd23-0ec9b1bf9fa2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsSmwZzNCQeU"
      },
      "source": [
        "### Usar ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPbHMS-6CmS4"
      },
      "source": [
        "Usamos `subprocess` para iniciar Ollama de forma no bloqueante. En tu propio escritorio, simplemente puedes abrir una nueva terminal y escribir `ollama serve`, pero en Colab, necesitamos usar este truco."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wloHHZfCKKz"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3) # Esperar a que Ollama se cargue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuCqqUq4AMLO"
      },
      "source": [
        "### Usar Modelo subido a Hugginface\n",
        "Si hemos subido el modelo a hugging face podemos descargarlo con el siguiente comando (si el modelo se llama model como este caso, si no habria que sustituir el nombre del modelo por el que sea)\n",
        "Una vez termine de descargar el modelo hay que parar la ejecucion de la celda para usar el resto del cuaderno"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqEM3Lbl_x2t",
        "outputId": "9068702a-1976-46c4-cdc0-7e4b1c0e7788"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "#!ollama run hf.co/serdom02/model_8bitQ8_0\n",
        "#!ollama run hf.co/serdom02/model_16bitGGUF\n",
        "#!ollama run hf.co/serdom02/model_q4_k_mGGUF\n",
        "!ollama run llama3:8b\n",
        "#!ollama run hf.co/serdom02/Leyeneitor_8bitQ8_0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zUqVHKrBhC7"
      },
      "source": [
        "Hablar con el modelo de Hugging Face (Solo cambiamos el nombre del modelo)\n",
        "\n",
        "Hay que detener la celda anterior y volver a iniciar Ollama para poder hablar con el modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MN-zen9pXhXg"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3) # Esperar a que Ollama se cargue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8X3VD1EaAtnL"
      },
      "source": [
        "### Hablar con el modelo usando ollama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICFGEuz2GpH3"
      },
      "source": [
        "Una vez ya tenemos ollama instalado y el modelo descargado podemos interactuar con el modelo usando el siguiente codigo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSjE5z7fGpyH",
        "outputId": "e099d929-9c5a-4564-c4f5-783681972c31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Puedes presentar una denuncia ante la Agencia Española de Protección de Datos (AEPD) o el juez competente por el delito de vulneración de privacidad. Si el vídeo se ha publicado en redes sociales, también puedes ejercer el derecho al olvido solicitando su retirada. En España, grabar a alguien desnudo en su propia casa sin su consentimiento es una violación grave de la privacidad, incluso si ocurre en un lugar público como un balcón. Si la grabación se ha difundido y perjudica tu reputación o intimidad, puedes exigir que la eliminen inmediatamente. Si no estabas al tanto de la grabación y no hay prueba de que consentiste, es probable que sea ilegal. En este caso, podrías presentar una denuncia ante la policía por el delito de vulneración de privacidad, ya que se ha afectado a tus derechos fundamentales. Para protegerte aún más, considera cambiar las contraseñas de tus redes sociales y revisar cualquier publicación tuya que pueda estar relacionada con este incidente.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import json\n",
        "import re\n",
        "\n",
        "url = \"http://localhost:11434/api/chat\"\n",
        "data = {\n",
        "    \"model\": \"hf.co/serdom02/Leyeneitor_8bitQ8_0\", #Solo cambiamos esta parte\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"El vecino del bloque de enfrente me ha grabado desde su casa y me ha pillado desnudo, creo que lo ha publicado en las redes, puedo denunciar?\"}] #Aqui ponemos el mensaje\n",
        "}\n",
        "\n",
        "response = requests.post(url, json=data, stream=True)\n",
        "\n",
        "# Concatenar la respuesta completa y limpiar espacios\n",
        "full_response = \"\"\n",
        "for line in response.iter_lines():\n",
        "    if line:\n",
        "        decoded_line = json.loads(line)\n",
        "        if \"message\" in decoded_line and \"content\" in decoded_line[\"message\"]:\n",
        "            # Limpiar espacios dobles y unir las partes\n",
        "            full_response += decoded_line[\"message\"][\"content\"]\n",
        "\n",
        "# Eliminar saltos de línea y limpiar los espacios extras\n",
        "full_response = re.sub(r'\\s+', ' ', full_response).strip()\n",
        "\n",
        "print(full_response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s20qCTdICnt2"
      },
      "source": [
        "Si ollama se queda congelado podemos usar kill para matar el proceso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upycRraGCsvP"
      },
      "outputs": [],
      "source": [
        "#Para resetear ollama\n",
        "!kill -9 $(ps aux | grep '[o]llama' | awk '{print $2}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQVgDT5zH57k"
      },
      "source": [
        "# Rag con Llama 3\n",
        "\n",
        "Fuente: https://medium.com/@danushidk507/rag-with-llama-using-ollama-a-deep-dive-into-retrieval-augmented-generation-c58b9a1cfcd3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiPLtIAUIFBw",
        "outputId": "38b8179f-def9-44c5-a288-5af4e67abb7b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.3.25-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting langchain-huggingface\n",
            "  Downloading langchain_huggingface-0.1.2-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Collecting langchain-core<1.0.0,>=0.3.58 (from langchain)\n",
            "  Downloading langchain_core-0.3.58-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.39)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.30.2)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.0 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (3.4.1)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.21.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->langchain-huggingface) (4.13.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.4.26)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.15.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (11.2.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.6.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.0.2)\n",
            "Downloading langchain-0.3.25-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_huggingface-0.1.2-py3-none-any.whl (21 kB)\n",
            "Downloading langchain_core-0.3.58-py3-none-any.whl (437 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.6/437.6 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, langchain-core, langchain-huggingface, langchain\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.56\n",
            "    Uninstalling langchain-core-0.3.56:\n",
            "      Successfully uninstalled langchain-core-0.3.56\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.3.24\n",
            "    Uninstalling langchain-0.3.24:\n",
            "      Successfully uninstalled langchain-0.3.24\n",
            "Successfully installed langchain-0.3.25 langchain-core-0.3.58 langchain-huggingface-0.1.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Collecting langchain-ollama\n",
            "  Downloading langchain_ollama-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting ollama<1,>=0.4.4 (from langchain-ollama)\n",
            "  Downloading ollama-0.4.8-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.52 in /usr/local/lib/python3.11/dist-packages (from langchain-ollama) (0.3.58)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (0.3.39)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (4.13.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (2.11.4)\n",
            "Requirement already satisfied: httpx<0.29,>=0.27 in /usr/local/lib/python3.11/dist-packages (from ollama<1,>=0.4.4->langchain-ollama) (0.28.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (3.10.18)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (2.4.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.3.1)\n",
            "Downloading langchain_ollama-0.3.2-py3-none-any.whl (20 kB)\n",
            "Downloading ollama-0.4.8-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: ollama, langchain-ollama\n",
            "Successfully installed langchain-ollama-0.3.2 ollama-0.4.8\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.4.26)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-5.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Downloading pypdf-5.4.0-py3-none-any.whl (302 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.3/302.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-5.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain langchain-huggingface transformers\n",
        "!pip install -U langchain-ollama\n",
        "!pip install sentence-transformers\n",
        "!pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Geh84e61IG8K"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain-community faiss-cpu\n",
        "from langchain.vectorstores import FAISS\n",
        "#print(faiss.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0txzLu3IvgQ"
      },
      "source": [
        "## Ingesta de Datos\n",
        "Comenzamos cargando y dividiendo los documentos. Utilizamos PyPDFLoader para cargar un archivo PDF y dividirlo en fragmentos más pequeños y superpuestos, lo que mejora la precisión de la recuperación de información."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kpz4Ar6cT7-"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "#from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# Lista de rutas de los PDFs\n",
        "pdf_paths = [\n",
        "    \"BOE-A-1978-31229-consolidado.pdf\",\n",
        "    \"BOE-A-1995-25444-consolidado_CodigoPenal.pdf\",\n",
        "    \"Protección de Datos Personales y garantia de los derechos digitales.pdf\",\n",
        "    \"RGPD_boe.pdf\"\n",
        "]\n",
        "\n",
        "# Cargar documentos de todos los PDFs\n",
        "documents = []\n",
        "for path in pdf_paths:\n",
        "    try:\n",
        "        loader = PyPDFLoader(path)\n",
        "        documents.extend(loader.load())\n",
        "    except Exception as e:\n",
        "        print(f\"Error cargando {path}: {e}\")\n",
        "\n",
        "for doc in documents:\n",
        "    doc.page_content = doc.page_content.replace('\\r\\n', '\\n').replace('\\n\\n', '\\n').strip()\n",
        "\n",
        "\n",
        "# Dividir los documentos en fragmentos\n",
        "#text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=30, separator=\"\\n\")\n",
        "#docs = text_splitter.split_documents(documents=documents)\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\")\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=800,\n",
        "    chunk_overlap=200,\n",
        "    separators=[\"\\n\\n\\n\", \"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
        "    tokenizer_name=\"cl100k_base\"  # Compatible con modelos más modernos\n",
        ")\n",
        "\n",
        "\n",
        "docs = text_splitter.split_documents(documents=documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wmAhvLleAo3"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksK6pmIeJait"
      },
      "source": [
        "## Embeddings de Datos y Almacenamiento con FAISS\n",
        "\n",
        "FAISS (Facebook AI Similarity Search) es una biblioteca versátil y eficiente para la búsqueda de similitudes en vectores. Permite la recuperación escalable y rápida de embeddings.\n",
        "Elección del Modelo de Embedding\n",
        "\n",
        "Utilizamos sentence-transformers/all-mpnet-base-v2, conocido por su rendimiento robusto en diversas tareas de procesamiento de texto. Alternativas como BGE o MiniLM pueden ser utilizadas para equilibrar la velocidad y la precisión según el caso específico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNUGuqwXJn6u"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import faiss\n",
        "\n",
        "# Load embedding model\n",
        "#embedding_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "embedding_model_name = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
        "#model_kwargs = {\"device\": \"cuda\"}\n",
        "model_kwargs = {\"device\": \"cpu\"}\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=embedding_model_name,\n",
        "    model_kwargs=model_kwargs\n",
        ")\n",
        "\n",
        "# Create FAISS vector store\n",
        "vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "# Save and reload the vector store\n",
        "vectorstore.save_local(\"faiss_index_\")\n",
        "persisted_vectorstore = FAISS.load_local(\"faiss_index_\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "# Create a retriever\n",
        "retriever = persisted_vectorstore.as_retriever()\n",
        "\n",
        "print(f\"Se indexaron {len(docs)} fragmentos.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZ3LCsD0JplS"
      },
      "source": [
        "## Seleccionamos el modelo\n",
        "\n",
        "Aqui podemos probar como responde el modelo sin RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cI3DdntQT9EB",
        "outputId": "86df65ab-32fd-4adf-de0f-c3b8788c513b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**\"La web de la seguridad\"**\n",
            "\n",
            "María era una estudiante que había creado una cuenta en la web de un concurso de programación para practicar sus habilidades. Sin embargo, meses después, comenzó a recibir publicidad en su correo electrónico sobre productos financieros que nunca había solicitado. Al investigar, descubrió que el concurso había compartido sus datos personales con empresas de marketing sin informarle previamente.\n",
            "\n",
            "María decidió ejercer sus derechos y contactó con la organización para reclamar la eliminación de estos datos y solicitar información sobre qué se habían enviado. La respuesta inicial fue evasiva, pero después de insistir, la empresa confirmó que había sufrido una filtración de datos y que algunos registros habían sido compartidos con terceros sin el consentimiento explícito del participante.\n",
            "\n",
            "Este incidente le recordó la importancia de leer los términos y condiciones antes de registrarse en cualquier web. Además, se aseguró de cambiar sus credenciales y desactivar las notificaciones no esenciales para minimizar el riesgo de nuevas filtraciones.\n",
            "\n",
            "**¿Qué podía hacer María?**\n",
            "\n",
            "* Ejercer su derecho de supresión (derecho al olvido) para que eliminaran sus datos personales de los sistemas del concurso, incluyendo las envíos publicitarios.\n",
            "* Solicitar a la organización detalles sobre qué datos se habían compartido y con qué finalidad.\n",
            "* Presentar una denuncia ante la autoridad de protección de datos si consideraba que había existido una vulneración grave de las normas de protección.\n",
            "\n",
            "Este caso ilustra la importancia de transparencia en el tratamiento de datos personales, especialmente cuando se comparten con terceros. Las organizaciones deben informar claramente a los usuarios sobre el uso de sus datos y ofrecerles herramientas para ejercer sus derechos.\n"
          ]
        }
      ],
      "source": [
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# Initialize the LLaMA model\n",
        "llm = OllamaLLM(model=\"hf.co/serdom02/Leyeneitor_8bitQ8_0\", base_url=\"http://127.0.0.1:11434\") #aqui ponemos el modelo finetuneado\n",
        "\n",
        "# Test with a sample prompt\n",
        "response = llm.invoke(\"Un cuento sobre proteccion de datos\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taF4AoOWMhqh"
      },
      "source": [
        "## Ahora podemos interactuar con el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yYF2u4QOa8T-"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3) # Esperar a que Ollama se cargue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "wuv0msS5i3ww",
        "outputId": "c39db064-30c8-4edc-a364-454806e41b06",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\sdominguez\\AppData\\Local\\Temp\\ipykernel_17028\\10883580.py:34: LangChainDeprecationWarning: This class is deprecated. See the following migration guides for replacements based on `chain_type`:\n",
            "stuff: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain\n",
            "map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\n",
            "refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\n",
            "map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\n",
            "\n",
            "See also guides on retrieval and question-answering here: https://python.langchain.com/docs/how_to/#qa-with-rag\n",
            "  chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=chat_prompt)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "Asistente legal de protección de datos listo (usando load_qa_chain con prompt separado). Escribe 'Exit' para salir.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stdin",
          "output_type": "stream",
          "text": [
            "\n",
            "Pregunta:  Hola me llamo sergio\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Respuesta:\n",
            "**Problema:** Un colegio desea enviar notificaciones a padres y alumnos mediante una plataforma de mensajería móvil, almacenando datos personales y actividades de los usuarios.\n",
            "\n",
            "**Análisis:** La normativa española exige que el tratamiento de datos en esta plataforma cumpla con el principio de minimización. Para cumplir con este requisito, la escuela debe informar claramente a los padres y alumnos sobre qué información se almacena, con quién se comparte y para qué finalidad. Además, debe ofrecer la posibilidad de ejercer los derechos de acceso, rectificación o supresión de estos datos.\n",
            "\n",
            "**Conclusión:** El colegio debe informar de forma clara y transparente sobre el tratamiento de datos personales en la plataforma de mensajería móvil, obteniendo el consentimiento adecuado para su uso.\n",
            "\n",
            "Fuentes Recuperadas:\n",
            "  [1] Fuente: BOE-A-1995-25444-consolidado_CodigoPenal.pdf, Página: 93\n",
            "     Fragmento: comunicación interior con él, y con el cual formen una unidad física.\n",
            "BOLETÍN OFICIAL DEL ESTADO\n",
            "LEGISLACIÓN CONSOLIDADA\n",
            "Página 94...\n",
            "  [2] Fuente: RGPD_boe.pdf, Página: 38\n",
            "     Fragmento: tratamient o, en f or ma concisa, transparente, inteligible y de f ácil acceso, con un lenguaj e claro y sencillo, en par ticular \n",
            "cualquier inf or ma...\n",
            "  [3] Fuente: RGPD_boe.pdf, Página: 21\n",
            "     Fragmento: desempe ñar las funciones que se le conf ieran de conf or midad con el presente Reg lamento. Lo anter ior debe \n",
            "4.5.2016 L 119/22 Diar io Oficial de l...\n",
            "  [4] Fuente: RGPD_boe.pdf, Página: 75\n",
            "     Fragmento: ar tículos 64 y 65, sin per juicio de las funciones de las autori dades de control nacionales; \n",
            "4.5.2016 L 119/76 Diar io Oficial de la U nión Europea...\n",
            "  [5] Fuente: BOE-A-1995-25444-consolidado_CodigoPenal.pdf, Página: 55\n",
            "     Fragmento: relevante de todos ellos.\n",
            "BOLETÍN OFICIAL DEL ESTADO\n",
            "LEGISLACIÓN CONSOLIDADA\n",
            "Página 56...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# --- 3. Bucle de Consulta Interactivo ---\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 42\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPregunta: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     44\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1263\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1266\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1267\u001b[0m )\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "# Importación correcta para versiones recientes de Langchain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "import torch\n",
        "\n",
        "# --- 1. Separación de roles con ChatPromptTemplate ---\n",
        "system_prompt = \"\"\"\n",
        "Eres un experto legal en protección de datos en España.\n",
        "Tu tarea es analizar la legalidad de las situaciones planteadas por el usuario.\n",
        "La respuesta debe ser clara, rigurosa y formal, como si fuera escrita por un profesional del derecho.\n",
        "No hagas suposiciones. No generalices. No repitas ideas.\n",
        "\"\"\"\n",
        "\n",
        "user_prompt = \"\"\"\n",
        "Fragmentos recuperados:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "A continuación, presenta el análisis legal y la conclusión en tres partes:\n",
        "1. **Introducción breve** del problema legal.\n",
        "2. **Análisis jurídico** apoyado en los fragmentos recuperados, citando tus fuentes.\n",
        "3. **Conclusión clara**, con un juicio legal concreto.\n",
        "\"\"\"\n",
        "\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
        "    HumanMessagePromptTemplate.from_template(user_prompt)\n",
        "])\n",
        "\n",
        "# --- 2. Crear la cadena usando load_qa_chain con el prompt de chat ---\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=chat_prompt)\n",
        "\n",
        "print(\"-\"*50)\n",
        "print(\"Asistente legal de protección de datos listo (usando load_qa_chain con prompt separado). Escribe 'Exit' para salir.\")\n",
        "print(\"-\"*50)\n",
        "\n",
        "# --- 3. Bucle de Consulta Interactivo ---\n",
        "while True:\n",
        "    query = input(\"\\nPregunta: \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "    if not query.strip():\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        retriever.search_kwargs = {'k': 5}\n",
        "        docs_retrieved = retriever.invoke(query)\n",
        "\n",
        "        input_data = {\n",
        "            \"input_documents\": docs_retrieved,\n",
        "            \"question\": query\n",
        "        }\n",
        "\n",
        "        result = chain.invoke(input_data)\n",
        "\n",
        "        if isinstance(result, dict) and 'output_text' in result:\n",
        "            print(\"\\nRespuesta:\")\n",
        "            print(result['output_text'])\n",
        "        elif isinstance(result, str):\n",
        "            print(\"\\nRespuesta:\")\n",
        "            print(result)\n",
        "        else:\n",
        "            print(\"\\nRespuesta recibida (formato inesperado):\")\n",
        "            print(result)\n",
        "\n",
        "        print(\"\\nFuentes Recuperadas:\")\n",
        "        for i, doc in enumerate(docs_retrieved):\n",
        "            source = doc.metadata.get('source', 'N/A')\n",
        "            page = doc.metadata.get('page', 'N/A')\n",
        "            print(f\"  [{i+1}] Fuente: {source}, Página: {page}\")\n",
        "            print(f\"     Fragmento: {doc.page_content[:150]}...\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError durante la ejecución de la cadena: {e}\")\n",
        "\n",
        "print(\"\\nSaliendo del asistente.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M6vxs-EuSUW"
      },
      "source": [
        "Plantillas para Propmt Template:\n",
        "\n",
        "**Plantilla1 Muy restrictiva, se centra en usar solo datos sacados del RAG:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqmjISNbvMYZ"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"\n",
        "Eres un asistente legal experto en protección de datos en España.\n",
        "Tu tarea es analizar la legalidad de la situación descrita en la 'Pregunta' basándote ÚNICA y EXCLUSIVAMENTE en los siguientes 'Textos Legales Recuperados'.\n",
        "NO uses ningún conocimiento externo. Si la información en los textos no es suficiente para dar una respuesta fundada, indícalo claramente.\n",
        "Justifica tu respuesta paso a paso, haciendo referencia explícita a los artículos o secciones relevantes de los textos proporcionados si es posible.\n",
        "\n",
        "Textos Legales Recuperados:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "Análisis Legal y Conclusión (Basado SÓLO en los textos recuperados):\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFL_ls9VvI8j"
      },
      "source": [
        "**Plantilla 2 Menos restrictiva, permite al modelo usar sus conocimientos adquiridos del finetunning y razonar un poco fuera de los contenidos del RAG**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPlb7geDvH70"
      },
      "outputs": [],
      "source": [
        "prompt_template = \"\"\"\n",
        "Eres un asistente legal experto en protección de datos en España.\n",
        "Tu tarea es analizar la legalidad de la situación descrita en la 'Pregunta' basándote **principalmente** en los siguientes 'Textos Legales Recuperados'.\n",
        "Si los textos recuperados contienen suficiente información, responde únicamente basándote en ellos.\n",
        "Si los textos no contienen una respuesta clara pero la pregunta se refiere a conceptos básicos del RGPD, usa lo aprendido en el entrenamiento para dar una respuesta general, especificando que no proviene de los textos recuperados.\n",
        "Si la pregunta requiere información específica que no está en los textos recuperados ni en tu entrenamiento, indícalo claramente.\n",
        "\n",
        "Textos Legales Recuperados:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "Análisis Legal y Conclusión:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9GEM9djZltvB"
      },
      "source": [
        "## Clasificación de consultas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWFAIrWxjGWa"
      },
      "source": [
        "El Problema de la función anterior es que si el usuario intenta interactuar con el modelo para temas no legales, por ejemplo, saludar, al incluir los resultados del RAG en el propmt el modelo siempre va a intentar responder usando el contexto legal que recibe del RAG, por ejemplo:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgdnHyrdjjlJ"
      },
      "source": [
        "\n",
        "\n",
        "> Pregunta: Hola me llamo sergio\n",
        "\n",
        "> Respuesta:\n",
        "\"La pregunta plantea si el usuario puede solicitar a una empresa que elimine sus datos personales,\n",
        "aunque la normativa oblige a conservarlos. Como experto en protección de datos,\n",
        "debes explicar que solo se pueden conservar los datos estrictamente necesarios para cumplir con obligaciones legales,\n",
        "y que el usuario debe ser informado sobre el plazo de conservación.\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwWDkkm6jtVJ"
      },
      "source": [
        "Para solucionar esto hay que analizar la entrada del usuario para verificar si es una consulta legal que necesite de RAG o simplemente esta interactuando con el modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4-fTIcUsDe6"
      },
      "outputs": [],
      "source": [
        "llm_evaluador = OllamaLLM(model=\"llama3:8b\",base_url=\"http://127.0.0.1:11434\") #aqui ponemos el modelo finetuneado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xqpL5-k9km_e",
        "outputId": "3171be9f-cda3-4d23-b448-8970d71d7237",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\sdominguez\\AppData\\Local\\Temp\\ipykernel_9964\\334576307.py:87: LangChainDeprecationWarning: This class is deprecated. See the following migration guides for replacements based on `chain_type`:\n",
            "stuff: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain\n",
            "map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\n",
            "refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\n",
            "map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\n",
            "\n",
            "See also guides on retrieval and question-answering here: https://python.langchain.com/docs/how_to/#qa-with-rag\n",
            "  chain_rag = load_qa_chain(llm, chain_type=\"stuff\", prompt=chat_prompt_rag)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "Asistente listo. Escribe 'Exit' para salir.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[15], line 96\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 96\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mPregunta: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     98\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1263\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1266\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1267\u001b[0m )\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain  # Necesario para la clasificación\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "import torch\n",
        "\n",
        "# --- 1. Función de Clasificación de Intención (Modelo de Clasificación Separado) ---\n",
        "def classify_intent_with_classifier(query, llm_evaluador):\n",
        "    \"\"\"Clasifica si se necesita usar RAG para generar la respuesta.\"\"\"\n",
        "    classification_prompt_template = \"\"\"\n",
        "    Analiza la siguiente pregunta del usuario. Determina si la pregunta requiere un análisis legal sobre protección de datos en España (como RGPD, LOPDGDD) o si se puede responder sin el uso de RAG, es decir, si el modelo tiene suficiente conocimiento para dar una respuesta sin fragmentos adicionales.\n",
        "\n",
        "    Responde únicamente con la palabra \"USAR_RAG\" si la pregunta requiere fragmentos recuperados para una respuesta precisa.\n",
        "    Responde únicamente con la palabra \"NO_RAG\" si la pregunta no requiere fragmentos adicionales y puede ser respondida directamente con el conocimiento general del modelo.\n",
        "\n",
        "    Pregunta del usuario: \"{user_query}\"\n",
        "\n",
        "    Clasificación (USAR_RAG o NO_RAG):\"\"\"\n",
        "\n",
        "    prompt = PromptTemplate(template=classification_prompt_template, input_variables=[\"user_query\"])\n",
        "\n",
        "    classification_runnable = prompt | llm_evaluador\n",
        "\n",
        "    try:\n",
        "        result = classification_runnable.invoke({\"user_query\": query})\n",
        "\n",
        "        # Extraer texto. Ajusta la clave ('text') si tu LLM devuelve otra cosa.\n",
        "        if isinstance(result, dict):\n",
        "            answer_text = result.get('text', '').strip().upper()\n",
        "        elif isinstance(result, str):\n",
        "            answer_text = result.strip().upper()\n",
        "        else:\n",
        "            answer_text = str(result).strip().upper()\n",
        "\n",
        "        # Debug de la clasificación\n",
        "        print(f\"--- DEBUG: Clasificación LLM respondió: '{answer_text}' ---\")\n",
        "\n",
        "        if \"USAR_RAG\" in answer_text:\n",
        "            return \"USAR_RAG\"\n",
        "        elif \"NO_RAG\" in answer_text:\n",
        "            return \"NO_RAG\"\n",
        "        else:\n",
        "            print(\"--- WARN: Respuesta de clasificación no clara, asumiendo NO_RAG por seguridad ---\")\n",
        "            return \"NO_RAG\"  # Default seguro\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error en la clasificación de intención con LLM: {e}\")\n",
        "        return \"USAR_RAG\"  # Default seguro en caso de error\n",
        "\n",
        "# --- 2. Definiciones para el prompt de RAG ---\n",
        "system_prompt_rag = \"\"\"\n",
        "Eres un experto legal en protección de datos en España.\n",
        "Tu tarea es analizar la legalidad de las situaciones planteadas por el usuario utilizando fragmentos recuperados.\n",
        "La respuesta debe ser clara, rigurosa y formal, como si fuera escrita por un profesional del derecho.\n",
        "No hagas suposiciones. No generalices. No repitas ideas.\n",
        "\"\"\"\n",
        "\n",
        "user_prompt_rag = \"\"\"\n",
        "Fragmentos recuperados:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "A continuación, presenta el análisis legal y la conclusión en tres partes:\n",
        "1. **Introducción breve** del problema legal.\n",
        "2. **Análisis jurídico** apoyado en los fragmentos recuperados, citando tus fuentes.\n",
        "3. **Conclusión clara**, con un juicio legal concreto.\n",
        "\"\"\"\n",
        "\n",
        "chat_prompt_rag = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_prompt_rag),\n",
        "    HumanMessagePromptTemplate.from_template(user_prompt_rag)\n",
        "])\n",
        "\n",
        "# --- 3. Definiciones para la respuesta conversacional ---\n",
        "conversational_prompt_template = \"\"\"\n",
        "Ya no eres un asistente legal, ahora eres un asistente de IA amigable y conversador. Responde directamente al usuario de forma natural.\n",
        "Usuario: {user_input}\n",
        "Asistente:\"\"\"\n",
        "PROMPT_CONVERSATIONAL = PromptTemplate(template=conversational_prompt_template, input_variables=[\"user_input\"])\n",
        "\n",
        "# --- 4. Crear la cadena usando load_qa_chain para el modelo de RAG ---\n",
        "chain_rag = load_qa_chain(llm, chain_type=\"stuff\", prompt=chat_prompt_rag)\n",
        "\n",
        "# --- 5. Bucle de Consulta Interactivo ---\n",
        "\n",
        "print(\"-\"*50)\n",
        "print(\"Asistente listo. Escribe 'Exit' para salir.\")\n",
        "print(\"-\"*50)\n",
        "\n",
        "while True:\n",
        "    query = input(\"\\nPregunta: \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "    if not query.strip():\n",
        "        continue\n",
        "\n",
        "    # --- Paso 1: Clasificar la intención del usuario ---\n",
        "    intent = classify_intent_with_classifier(query, llm_evaluador)\n",
        "    print(f\"--- Intención Detectada: {intent} ---\")\n",
        "\n",
        "    # --- Ejecutar flujo según la intención ---\n",
        "    if intent == \"USAR_RAG\":\n",
        "        print(\"--- Ejecutando RAG para consulta legal ---\")\n",
        "        try:\n",
        "            # 1. Recuperar documentos (RAG)\n",
        "            retriever.search_kwargs = {'k': 3}  # Ajusta 'k' si es necesario\n",
        "            docs_retrieved = retriever.invoke(query)\n",
        "\n",
        "            # 2. Preparar input para la cadena RAG\n",
        "            input_data = {\n",
        "                \"input_documents\": docs_retrieved,\n",
        "                \"question\": query\n",
        "            }\n",
        "\n",
        "            # 3. Ejecutar la cadena RAG\n",
        "            result = chain_rag.invoke(input_data)\n",
        "\n",
        "            # 4. Imprimir resultado RAG\n",
        "            if isinstance(result, dict) and 'output_text' in result:\n",
        "                print(\"\\nRespuesta (Legal):\")\n",
        "                print(result['output_text'])\n",
        "            elif isinstance(result, str):\n",
        "                print(\"\\nRespuesta (Legal):\")\n",
        "                print(result)\n",
        "            else:\n",
        "                print(\"\\nRespuesta RAG recibida (formato inesperado):\")\n",
        "                print(result)\n",
        "\n",
        "            # 5. Imprimir fuentes (Opcional)\n",
        "            print(\"\\nFuentes Recuperadas:\")\n",
        "            for i, doc in enumerate(docs_retrieved):\n",
        "                source = doc.metadata.get('source', 'N/A')\n",
        "                page = doc.metadata.get('page', 'N/A')\n",
        "                print(f\"  [{i+1}] Fuente: {source}, Página: {page}\")\n",
        "                print(f\"     Fragmento: {doc.page_content[:150]}...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError durante la ejecución de la cadena RAG: {e}\")\n",
        "\n",
        "    elif intent == \"NO_RAG\":\n",
        "        print(\"--- Generando respuesta sin usar RAG ---\")\n",
        "        try:\n",
        "            # 1. Formatear prompt conversacional simple\n",
        "            simple_prompt = PROMPT_CONVERSATIONAL.format(user_input=query)\n",
        "\n",
        "            # 2. Llamar directamente al LLM (sin RAG)\n",
        "            response = llm.invoke(simple_prompt)\n",
        "\n",
        "            # 3. Extraer y limpiar la respuesta\n",
        "            if isinstance(response, dict):\n",
        "                 # Adapta la clave si 'invoke' devuelve un dict con otra clave para el texto\n",
        "                 response_text = response.get('text', str(response))\n",
        "            elif isinstance(response, str):\n",
        "                 response_text = response\n",
        "            else:\n",
        "                 response_text = str(response)\n",
        "\n",
        "            # Limpieza: quitar el prompt si el LLM lo repite\n",
        "            print(\"\\nRespuesta (Conversacional):\")\n",
        "            print(response_text)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError durante la llamada conversacional al LLM: {e}\")\n",
        "    else:\n",
        "        print(f\"--- ERROR: Intención desconocida '{intent}' recibida de la clasificación ---\")\n",
        "\n",
        "print(\"\\nSaliendo del asistente.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHWFiDgzo35z"
      },
      "source": [
        "Vemos que ahora si le decimos algo que no es claramente algo relacionado con temas legales, como \"Hola me llamo Sergio\" se detecta que es una conversación normal y no se activa el RAG:\n",
        "\n",
        "\n",
        "> Pregunta: Hola me llamo sergio\n",
        "\n",
        "\n",
        "> Respuesta (Conversacional):\n",
        "Hola Sergio, ¿en qué puedo ayudarte hoy?\n",
        "\n",
        "¿Pero que pasa si hacemos una pregunta legal y luego seguimos intentando consultar al modelo sobre la pregunta inicial?\n",
        "\n",
        "\n",
        "\n",
        "> Pregunta:  Tengo una consulta sobre protección de datos, mi gimnasio me pide mi huella dactilar para poder entrar al gimnasio, ¿pueden exirgirmelo?\n",
        "\n",
        "\n",
        "> Respuesta (Legal):\n",
        "**Introducción**: Un gimnasio quiere usar datos biométricos (como la huella dactilar) para controlar el acceso a sus instalaciones. Este tratamiento de datos personales requiere una base legal clara y garantías de seguridad adecuadas.\n",
        "**Análisis**: El uso de datos biométricos para fines como el control de acceso debe basarse en un principio de consentimiento explícito, salvo que exista una justificación legal específica (como la seguridad pública). En este caso, se deben aplicar medidas adicionales para proteger estos datos sensibles, ya que permiten identificar de forma única a cada persona. Además, el gimnasio debe informar claramente sobre el propósito del tratamiento y las garantías de seguridad adoptadas.\n",
        "**Conclusión**: Para que este tratamiento sea lícito, debe existir una base legal clara (como el consentimiento informado o una obligación legal) y se deben aplicar medidas adecuadas para proteger estos datos sensibles.\n",
        "\n",
        "\n",
        "\n",
        "> Pregunta: ¿Puedo negarme?\n",
        "\n",
        "\n",
        "> Respuesta (Legal):\n",
        "Una empresa farmacéutica me ha solicitado información personal para estudiar la eficacia de un medicamento, pero prefiero no participar. ¿Tengo derecho a negarme?\n",
        "Análisis legal: Por ser un estudio clínico, se requiere tu consentimiento explícito. Si no das tu autorización, no pueden procesar tus datos. Sin embargo, si la investigación tiene relevancia para el bienestar público o hay una obligación legal, podrían excepcionalmente utilizar los datos anónimos o con medidas de protección específicas.\n",
        "Conclusión: Tienes derecho a negarte si el estudio clínico no cumple con tus derechos y garantías de privacidad.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Podemos observar que el modelo al activarse el RAG pierde el contexto de la pregunta anterior y responde algo totalmente diferente, seguramente debido a que RAG ha recuperado textos donde se incluya \"¿puedo negarme?\" que no estan relacionados con el contexto anterior, por lo que se inyecta contexto erroneo en el propmt.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8LOwn4wq58V"
      },
      "source": [
        "## Memoria Conversacional con RAG\n",
        "\n",
        "Para solucionar el problema de continuidad del contexto en la conversación vamos a añadir memoria al RAG, para ello, LangChain tiene una variedad de herramientas que permite implementarlo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8mERu0HnsDe6",
        "outputId": "617c97bd-3ea1-483c-9950-ac15de5290d0",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "Asistente listo. Escribe 'Exit' para salir.\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stdin",
          "output_type": "stream",
          "text": [
            "\n",
            "Pregunta:  Hola me llamo sergio\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 11:42:53,890 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- DEBUG: Clasificación LLM respondió: 'NO_RAG' ---\n",
            "--- Intención Detectada: NO_RAG ---\n",
            "--- Generando respuesta sin usar RAG ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 11:43:00,024 - asyncio - ERROR - Task exception was never retrieved\n",
            "future: <Task finished name='Task-1' coro=<Server.serve() done, defined at C:\\ProgramData\\anaconda3\\Lib\\site-packages\\uvicorn\\server.py:68> exception=KeyboardInterrupt()>\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\uvicorn\\main.py\", line 580, in run\n",
            "    server.run()\n",
            "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\uvicorn\\server.py\", line 66, in run\n",
            "    return asyncio.run(self.serve(sockets=sockets))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\nest_asyncio.py\", line 30, in run\n",
            "    return loop.run_until_complete(task)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\nest_asyncio.py\", line 92, in run_until_complete\n",
            "    self._run_once()\n",
            "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\nest_asyncio.py\", line 133, in _run_once\n",
            "    handle._run()\n",
            "  File \"C:\\ProgramData\\anaconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"C:\\ProgramData\\anaconda3\\Lib\\asyncio\\tasks.py\", line 396, in __wakeup\n",
            "    self.__step()\n",
            "  File \"C:\\ProgramData\\anaconda3\\Lib\\asyncio\\tasks.py\", line 303, in __step\n",
            "    self.__step_run_and_handle_result(exc)\n",
            "  File \"C:\\ProgramData\\anaconda3\\Lib\\asyncio\\tasks.py\", line 314, in __step_run_and_handle_result\n",
            "    result = coro.send(None)\n",
            "             ^^^^^^^^^^^^^^^\n",
            "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\uvicorn\\server.py\", line 69, in serve\n",
            "    with self.capture_signals():\n",
            "  File \"C:\\ProgramData\\anaconda3\\Lib\\contextlib.py\", line 144, in __exit__\n",
            "    next(self.gen)\n",
            "  File \"C:\\ProgramData\\anaconda3\\Lib\\site-packages\\uvicorn\\server.py\", line 330, in capture_signals\n",
            "    signal.raise_signal(captured_signal)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[14], line 176\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    171\u001b[0m     prompt_with_memory \u001b[38;5;241m=\u001b[39m PROMPT_CONVERSATIONAL\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    172\u001b[0m         chat_history\u001b[38;5;241m=\u001b[39mmemory\u001b[38;5;241m.\u001b[39mget_memory_as_text(),\n\u001b[0;32m    173\u001b[0m         user_input\u001b[38;5;241m=\u001b[39mquery\n\u001b[0;32m    174\u001b[0m     )\n\u001b[1;32m--> 176\u001b[0m     response \u001b[38;5;241m=\u001b[39m llm\u001b[38;5;241m.\u001b[39minvoke(prompt_with_memory)\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    179\u001b[0m         response_text \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mstr\u001b[39m(response))\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\language_models\\llms.py:387\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    384\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    385\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m--> 387\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    388\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    389\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    390\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    391\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    392\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    393\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    394\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    395\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    396\u001b[0m         )\n\u001b[0;32m    397\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    398\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[0;32m    399\u001b[0m     )\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\language_models\\llms.py:764\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    756\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    757\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    761\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    762\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    763\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_strings, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\language_models\\llms.py:971\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m    957\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    958\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    959\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    969\u001b[0m         )\n\u001b[0;32m    970\u001b[0m     ]\n\u001b[1;32m--> 971\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_helper(\n\u001b[0;32m    972\u001b[0m         prompts,\n\u001b[0;32m    973\u001b[0m         stop,\n\u001b[0;32m    974\u001b[0m         run_managers,\n\u001b[0;32m    975\u001b[0m         new_arg_supported\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m(new_arg_supported),\n\u001b[0;32m    976\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    977\u001b[0m     )\n\u001b[0;32m    978\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    979\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    980\u001b[0m         callback_managers[idx]\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[0;32m    981\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_serialized,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    988\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m missing_prompt_idxs\n\u001b[0;32m    989\u001b[0m     ]\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\langchain_core\\language_models\\llms.py:790\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[1;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    781\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    787\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    788\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    789\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 790\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m    791\u001b[0m                 prompts,\n\u001b[0;32m    792\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    793\u001b[0m                 \u001b[38;5;66;03m# TODO: support multiple run managers\u001b[39;00m\n\u001b[0;32m    794\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    795\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    796\u001b[0m             )\n\u001b[0;32m    797\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    798\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[0;32m    799\u001b[0m         )\n\u001b[0;32m    800\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    801\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\langchain_ollama\\llms.py:290\u001b[0m, in \u001b[0;36mOllamaLLM._generate\u001b[1;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    288\u001b[0m generations \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    289\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[1;32m--> 290\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stream_with_aggregation(\n\u001b[0;32m    291\u001b[0m         prompt,\n\u001b[0;32m    292\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    293\u001b[0m         run_manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[0;32m    294\u001b[0m         verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    295\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    296\u001b[0m     )\n\u001b[0;32m    297\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([final_chunk])\n\u001b[0;32m    298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\langchain_ollama\\llms.py:258\u001b[0m, in \u001b[0;36mOllamaLLM._stream_with_aggregation\u001b[1;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stream_with_aggregation\u001b[39m(\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    251\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    255\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    256\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GenerationChunk:\n\u001b[0;32m    257\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 258\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_generate_stream(prompt, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stream_resp, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    260\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m GenerationChunk(\n\u001b[0;32m    261\u001b[0m                 text\u001b[38;5;241m=\u001b[39mstream_resp[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m stream_resp \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    262\u001b[0m                 generation_info\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    263\u001b[0m                     \u001b[38;5;28mdict\u001b[39m(stream_resp) \u001b[38;5;28;01mif\u001b[39;00m stream_resp\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdone\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    264\u001b[0m                 ),\n\u001b[0;32m    265\u001b[0m             )\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\langchain_ollama\\llms.py:213\u001b[0m, in \u001b[0;36mOllamaLLM._create_generate_stream\u001b[1;34m(self, prompt, stop, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_generate_stream\u001b[39m(\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    209\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    210\u001b[0m     stop: Optional[\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    212\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[Union[Mapping[\u001b[38;5;28mstr\u001b[39m, Any], \u001b[38;5;28mstr\u001b[39m]]:\n\u001b[1;32m--> 213\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m    214\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_params(prompt, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    215\u001b[0m     )\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\ollama\\_client.py:163\u001b[0m, in \u001b[0;36mClient._request.<locals>.inner\u001b[1;34m()\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m():\n\u001b[1;32m--> 163\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mstream(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m r:\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m       r\u001b[38;5;241m.\u001b[39mraise_for_status()\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\contextlib.py:137\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgen)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:870\u001b[0m, in \u001b[0;36mClient.stream\u001b[1;34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[0m\n\u001b[0;32m    847\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    848\u001b[0m \u001b[38;5;124;03mAlternative to `httpx.request()` that streams the response body\u001b[39;00m\n\u001b[0;32m    849\u001b[0m \u001b[38;5;124;03minstead of loading it into memory at once.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    855\u001b[0m \u001b[38;5;124;03m[0]: /quickstart#streaming-responses\u001b[39;00m\n\u001b[0;32m    856\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    857\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_request(\n\u001b[0;32m    858\u001b[0m     method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    859\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    868\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mextensions,\n\u001b[0;32m    869\u001b[0m )\n\u001b[1;32m--> 870\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(\n\u001b[0;32m    871\u001b[0m     request\u001b[38;5;241m=\u001b[39mrequest,\n\u001b[0;32m    872\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[0;32m    873\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[0;32m    874\u001b[0m     stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    875\u001b[0m )\n\u001b[0;32m    876\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    877\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m response\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[1;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[0;32m    906\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[0;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[0;32m    910\u001b[0m )\n\u001b[0;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[1;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_auth(\n\u001b[0;32m    915\u001b[0m     request,\n\u001b[0;32m    916\u001b[0m     auth\u001b[38;5;241m=\u001b[39mauth,\n\u001b[0;32m    917\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[0;32m    918\u001b[0m     history\u001b[38;5;241m=\u001b[39m[],\n\u001b[0;32m    919\u001b[0m )\n\u001b[0;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[1;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[0;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[0;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_handling_redirects(\n\u001b[0;32m    943\u001b[0m         request,\n\u001b[0;32m    944\u001b[0m         follow_redirects\u001b[38;5;241m=\u001b[39mfollow_redirects,\n\u001b[0;32m    945\u001b[0m         history\u001b[38;5;241m=\u001b[39mhistory,\n\u001b[0;32m    946\u001b[0m     )\n\u001b[0;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[1;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    977\u001b[0m     hook(request)\n\u001b[1;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_single_request(request)\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpx\\_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m   1010\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1011\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1012\u001b[0m     )\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[1;32m-> 1015\u001b[0m     response \u001b[38;5;241m=\u001b[39m transport\u001b[38;5;241m.\u001b[39mhandle_request(request)\n\u001b[0;32m   1017\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[0;32m   1019\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpx\\_transports\\default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    220\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[0;32m    221\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[0;32m    222\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    230\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    231\u001b[0m )\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[1;32m--> 233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool\u001b[38;5;241m.\u001b[39mhandle_request(req)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[0;32m    238\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[0;32m    239\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[0;32m    240\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[0;32m    241\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[0;32m    242\u001b[0m )\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:268\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ShieldCancellation():\n\u001b[0;32m    267\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse_closed(status)\n\u001b[1;32m--> 268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\connection_pool.py:251\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    248\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 251\u001b[0m     response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;66;03m# The ConnectionNotAvailable exception is a special case, that\u001b[39;00m\n\u001b[0;32m    254\u001b[0m     \u001b[38;5;66;03m# indicates we need to retry the request on a new connection.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[38;5;66;03m# might end up as an HTTP/2 connection, but which actually ends\u001b[39;00m\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# up as HTTP/1.1.\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pool_lock:\n\u001b[0;32m    261\u001b[0m         \u001b[38;5;66;03m# Maintain our position in the request queue, but reset the\u001b[39;00m\n\u001b[0;32m    262\u001b[0m         \u001b[38;5;66;03m# status so that the request becomes queued again.\u001b[39;00m\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\connection.py:103\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m    101\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ConnectionNotAvailable()\n\u001b[1;32m--> 103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connection\u001b[38;5;241m.\u001b[39mhandle_request(request)\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:133\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[1;32m--> 133\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:111\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[0;32m    105\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[0;32m    106\u001b[0m     (\n\u001b[0;32m    107\u001b[0m         http_version,\n\u001b[0;32m    108\u001b[0m         status,\n\u001b[0;32m    109\u001b[0m         reason_phrase,\n\u001b[0;32m    110\u001b[0m         headers,\n\u001b[1;32m--> 111\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_response_headers(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    112\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    113\u001b[0m         http_version,\n\u001b[0;32m    114\u001b[0m         status,\n\u001b[0;32m    115\u001b[0m         reason_phrase,\n\u001b[0;32m    116\u001b[0m         headers,\n\u001b[0;32m    117\u001b[0m     )\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[0;32m    120\u001b[0m     status\u001b[38;5;241m=\u001b[39mstatus,\n\u001b[0;32m    121\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m     },\n\u001b[0;32m    128\u001b[0m )\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:176\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    173\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 176\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_receive_event(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[0;32m    178\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpcore\\_sync\\http11.py:212\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    209\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[1;32m--> 212\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\u001b[38;5;241m.\u001b[39mread(\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mREAD_NUM_BYTES, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    214\u001b[0m     )\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\httpcore\\_backends\\sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[1;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[1;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39mrecv(max_bytes)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain  # Necesario para la clasificación\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "import torch\n",
        "\n",
        "# Añadimos una clase de memoria conversacional\n",
        "class ConversationMemory:\n",
        "    def __init__(self, max_turns=10):\n",
        "        self.max_turns = max_turns\n",
        "        self.history = []\n",
        "\n",
        "    def add_turn(self, user_input, assistant_response):\n",
        "        self.history.append((user_input, assistant_response))\n",
        "        if len(self.history) > self.max_turns:\n",
        "            self.history.pop(0)\n",
        "\n",
        "    def get_memory_as_text(self):\n",
        "        return \"\\n\".join([f\"Usuario: {u}\\nAsistente: {a}\" for u, a in self.history])\n",
        "\n",
        "# --- 1. Función de Clasificación de Intención (Modelo de Clasificación Separado) ---\n",
        "def classify_intent_with_classifier(query, llm_evaluador):\n",
        "    \"\"\"Clasifica si se necesita usar RAG para generar la respuesta.\"\"\"\n",
        "    classification_prompt_template = \"\"\"\n",
        "        Analiza la siguiente consulta del usuario. Decide si es necesario usar RAG (fragmentos legales recuperados) o si el modelo puede responder directamente.\n",
        "\n",
        "        Solo debes responder con:\n",
        "        - \"USAR_RAG\" → si se requiere un análisis legal riguroso, citas precisas o fundamentación jurídica específica.\n",
        "        - \"NO_RAG\" → si el modelo puede dar una respuesta general, introductoria o basada en conocimiento común sin necesidad de fragmentos legales.\n",
        "\n",
        "        Ejemplos:\n",
        "        - Pregunta: \"¿Puede una empresa ceder mis datos sin consentimiento?\" → NO_RAG\n",
        "        - Pregunta: \"¿Qué dice el artículo 6 del RGPD sobre el consentimiento?\" → USAR_RAG\n",
        "        - Pregunta: \"¿Puedes citar el artículo 13 del RGPD?\" → USAR_RAG\n",
        "        - Pregunta: \"¿En qué artículo se regula el consentimiento explícito?\" → USAR_RAG\n",
        "        - Pregunta: \"Puedes decirme en qué artículos te basas?\" → USAR_RAG\n",
        "        - Pregunta: \"Cita textualmente en qué artículos te basas\" → USAR_RAG\n",
        "        - Pregunta: \"¿Qué derechos tengo como interesado?\" → NO_RAG\n",
        "        - Pregunta: \"¿Cómo elimino mis datos personales?\" → NO_RAG\n",
        "\n",
        "\n",
        "        Pregunta del usuario: \"{user_query}\"\n",
        "\n",
        "        Clasificación (USAR_RAG o NO_RAG):\"\"\"\n",
        "\n",
        "    prompt = PromptTemplate(template=classification_prompt_template, input_variables=[\"user_query\"])\n",
        "\n",
        "    classification_runnable = prompt | llm_evaluador\n",
        "\n",
        "    try:\n",
        "        result = classification_runnable.invoke({\"user_query\": query})\n",
        "\n",
        "        # Extraer texto. Ajusta la clave ('text') si tu LLM devuelve otra cosa.\n",
        "        if isinstance(result, dict):\n",
        "            answer_text = result.get('text', '').strip().upper()\n",
        "        elif isinstance(result, str):\n",
        "            answer_text = result.strip().upper()\n",
        "        else:\n",
        "            answer_text = str(result).strip().upper()\n",
        "\n",
        "        # Debug de la clasificación\n",
        "        print(f\"--- DEBUG: Clasificación LLM respondió: '{answer_text}' ---\")\n",
        "\n",
        "        if \"USAR_RAG\" in answer_text.upper():\n",
        "            return \"USAR_RAG\"\n",
        "        elif \"NO_RAG\" in answer_text:\n",
        "            return \"NO_RAG\"\n",
        "        else:\n",
        "            print(\"--- WARN: Respuesta de clasificación no clara, asumiendo NO_RAG por seguridad ---\")\n",
        "            return \"NO_RAG\"  # Default seguro\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error en la clasificación de intención con LLM: {e}\")\n",
        "        return \"USAR_RAG\"  # Default seguro en caso de error\n",
        "\n",
        "# --- 2. Definiciones para el prompt de RAG ---\n",
        "system_prompt_rag = \"\"\"\n",
        "Eres un experto legal en protección de datos en España.\n",
        "Tu tarea es analizar la legalidad de las situaciones planteadas por el usuario utilizando fragmentos recuperados.\n",
        "La respuesta debe ser clara, rigurosa y formal, como si fuera escrita por un profesional del derecho.\n",
        "No hagas suposiciones. No generalices. No repitas ideas.\n",
        "\"\"\"\n",
        "\n",
        "user_prompt_rag = \"\"\"\n",
        "Fragmentos recuperados:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "A continuación, presenta el análisis legal y la conclusión en tres partes:\n",
        "1. **Introducción breve** del problema legal.\n",
        "2. **Análisis jurídico** apoyado en los fragmentos recuperados, citando tus fuentes.\n",
        "3. **Conclusión clara**, con un juicio legal concreto.\n",
        "\"\"\"\n",
        "\n",
        "chat_prompt_rag = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_prompt_rag),\n",
        "    HumanMessagePromptTemplate.from_template(user_prompt_rag)\n",
        "])\n",
        "\n",
        "# --- 3. Definiciones para la respuesta conversacional ---\n",
        "conversational_prompt_template = \"\"\"\n",
        "Eres un asistente de IA amigable y conversador. Tu tarea es mantener una conversación fluida y coherente con el usuario.\n",
        "Responde de forma natural y cercana, teniendo en cuenta lo que ya se ha dicho.\n",
        "\n",
        "Historial reciente de la conversación:\n",
        "{chat_history}\n",
        "\n",
        "Usuario: {user_input}\n",
        "Asistente:\"\"\"\n",
        "\n",
        "PROMPT_CONVERSATIONAL = PromptTemplate(\n",
        "    template=conversational_prompt_template,\n",
        "    input_variables=[\"chat_history\", \"user_input\"]\n",
        ")\n",
        "\n",
        "# --- 4. Crear la cadena usando load_qa_chain para el modelo de RAG ---\n",
        "chain_rag = load_qa_chain(llm, chain_type=\"stuff\", prompt=chat_prompt_rag)\n",
        "\n",
        "# --- 5. Bucle de Consulta Interactivo ---\n",
        "\n",
        "print(\"-\"*50)\n",
        "print(\"Asistente listo. Escribe 'Exit' para salir.\")\n",
        "print(\"-\"*50)\n",
        "memory = ConversationMemory(max_turns=6)\n",
        "\n",
        "while True:\n",
        "    query = input(\"\\nPregunta: \")\n",
        "    if query.lower() == \"exit\":\n",
        "        break\n",
        "    if not query.strip():\n",
        "        continue\n",
        "\n",
        "    intent = classify_intent_with_classifier(query, llm_evaluador)\n",
        "    print(f\"--- Intención Detectada: {intent} ---\")\n",
        "\n",
        "    if intent == \"USAR_RAG\":\n",
        "        print(\"--- Ejecutando RAG para consulta legal ---\")\n",
        "        try:\n",
        "            retriever.search_kwargs = {'k': 3}\n",
        "            docs_retrieved = retriever.invoke(query)\n",
        "\n",
        "            input_data = {\n",
        "                \"input_documents\": docs_retrieved,\n",
        "                \"question\": query\n",
        "            }\n",
        "\n",
        "            result = chain_rag.invoke(input_data)\n",
        "            response_text = result.get('output_text', result if isinstance(result, str) else str(result))\n",
        "\n",
        "            print(\"\\nRespuesta (Legal):\")\n",
        "            print(response_text)\n",
        "\n",
        "            memory.add_turn(query, response_text)\n",
        "\n",
        "            print(\"\\nFuentes Recuperadas:\")\n",
        "            for i, doc in enumerate(docs_retrieved):\n",
        "                source = doc.metadata.get('source', 'N/A')\n",
        "                page = doc.metadata.get('page', 'N/A')\n",
        "                print(f\"  [{i+1}] Fuente: {source}, Página: {page}\")\n",
        "                print(f\"     Fragmento: {doc.page_content[:150]}...\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError durante la ejecución de la cadena RAG: {e}\")\n",
        "\n",
        "    elif intent == \"NO_RAG\":\n",
        "        print(\"--- Generando respuesta sin usar RAG ---\")\n",
        "        try:\n",
        "            prompt_with_memory = PROMPT_CONVERSATIONAL.format(\n",
        "                chat_history=memory.get_memory_as_text(),\n",
        "                user_input=query\n",
        "            )\n",
        "\n",
        "            response = llm.invoke(prompt_with_memory)\n",
        "\n",
        "            if isinstance(response, dict):\n",
        "                response_text = response.get('text', str(response))\n",
        "            elif isinstance(response, str):\n",
        "                response_text = response\n",
        "            else:\n",
        "                response_text = str(response)\n",
        "\n",
        "            if \"Asistente:\" in response_text:\n",
        "                response_text = response_text.split(\"Asistente:\", 1)[-1].strip()\n",
        "\n",
        "            print(\"\\nRespuesta (Conversacional):\")\n",
        "            print(response_text)\n",
        "\n",
        "            memory.add_turn(query, response_text)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError durante la llamada conversacional al LLM: {e}\")\n",
        "    else:\n",
        "        print(f\"--- ERROR: Intención desconocida '{intent}' recibida de la clasificación ---\")\n",
        "\n",
        "print(\"\\nSaliendo del asistente.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdnreWql09_8"
      },
      "source": [
        "# APLICACION FINAL MEJORADA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qoRlZhzgsDe7",
        "outputId": "dfd58074-501e-4e15-c1ed-d99f7e4138bd",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: langchain in c:\\users\\sdominguez\\appdata\\roaming\\python\\python312\\site-packages (0.3.25)\n",
            "Requirement already satisfied: langchain-huggingface in c:\\users\\sdominguez\\appdata\\roaming\\python\\python312\\site-packages (0.2.0)\n",
            "Requirement already satisfied: transformers in c:\\programdata\\anaconda3\\lib\\site-packages (4.51.3)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in c:\\users\\sdominguez\\appdata\\roaming\\python\\python312\\site-packages (from langchain) (0.3.59)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (0.3.33)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (2.10.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: requests<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (2.32.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: tokenizers>=0.19.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-huggingface) (0.21.1)\n",
            "Requirement already satisfied: sentence-transformers>=2.6.0 in c:\\users\\sdominguez\\appdata\\roaming\\python\\python312\\site-packages (from langchain-huggingface) (4.1.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.30.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-huggingface) (0.30.2)\n",
            "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2023.10.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (2024.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (4.12.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (8.2.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.27.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: torch>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (2.7.0)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\sdominguez\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.6.1)\n",
            "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (1.13.1)\n",
            "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers>=2.6.0->langchain-huggingface) (10.3.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.1)\n",
            "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: anyio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.2.0)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.2)\n",
            "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\programdata\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (2.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.13.3)\n",
            "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (3.1.4)\n",
            "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (69.5.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sdominguez\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn->sentence-transformers>=2.6.0->langchain-huggingface) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers>=2.6.0->langchain-huggingface) (2.1.3)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: langchain-ollama in c:\\programdata\\anaconda3\\lib\\site-packages (0.3.2)\n",
            "Requirement already satisfied: ollama<1,>=0.4.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-ollama) (0.4.8)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.52 in c:\\users\\sdominguez\\appdata\\roaming\\python\\python312\\site-packages (from langchain-ollama) (0.3.59)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (0.3.33)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (8.2.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (6.0.1)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (23.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (4.12.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.52->langchain-ollama) (2.10.3)\n",
            "Requirement already satisfied: httpx<0.29,>=0.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from ollama<1,>=0.4.4->langchain-ollama) (0.27.0)\n",
            "Requirement already satisfied: anyio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (4.2.0)\n",
            "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.0.2)\n",
            "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (3.7)\n",
            "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.3.0)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in c:\\programdata\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (2.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (3.10.16)\n",
            "Requirement already satisfied: requests<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (2.32.2)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (2.27.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (2.0.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.52->langchain-ollama) (2.2.2)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: sentence-transformers in c:\\users\\sdominguez\\appdata\\roaming\\python\\python312\\site-packages (4.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.51.3)\n",
            "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.7.0)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\sdominguez\\appdata\\roaming\\python\\python312\\site-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.30.2)\n",
            "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (10.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.3.1)\n",
            "Requirement already satisfied: packaging>=20.9 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.2)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (69.5.1)\n",
            "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.10.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sdominguez\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: pypdf in c:\\users\\sdominguez\\appdata\\roaming\\python\\python312\\site-packages (3.17.1)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\sdominguez\\appdata\\roaming\\python\\python312\\site-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sdominguez\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn) (3.6.0)\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: tiktoken in c:\\users\\sdominguez\\appdata\\roaming\\python\\python312\\site-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in c:\\programdata\\anaconda3\\lib\\site-packages (from tiktoken) (2023.10.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tiktoken) (2.32.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 14:59:06,685 - asistente_legal - INFO - Iniciando asistente legal de protección de datos\n"
          ]
        }
      ],
      "source": [
        "!pip install -U langchain langchain-huggingface transformers\n",
        "!pip install -U langchain-ollama\n",
        "!pip install sentence-transformers\n",
        "!pip install pypdf\n",
        "!pip install -qU langchain-community faiss-cpu\n",
        "!pip install -U scikit-learn\n",
        "!pip install tiktoken\n",
        "import logging\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "# Configuración de logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler(\"asistente_legal.log\"),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(\"asistente_legal\")\n",
        "logger.info(\"Iniciando asistente legal de protección de datos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXAmHnOPsDe7",
        "outputId": "5856a89a-3e32-40c0-9d45-7f4a9752deed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 14:59:12,976 - numexpr.utils - INFO - Note: NumExpr detected 24 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
            "2025-05-08 14:59:12,978 - numexpr.utils - INFO - NumExpr defaulting to 8 threads.\n",
            "2025-05-08 14:59:13,919 - asistente_legal - INFO - Cargando documento: BOE-A-1978-31229-consolidado.pdf\n",
            "2025-05-08 14:59:14,826 - asistente_legal - INFO - Cargando documento: BOE-A-1995-25444-consolidado_CodigoPenal.pdf\n",
            "2025-05-08 14:59:18,609 - asistente_legal - INFO - Cargando documento: Protección de Datos Personales y garantia de los derechos digitales.pdf\n",
            "2025-05-08 14:59:19,780 - asistente_legal - INFO - Cargando documento: RGPD_boe.pdf\n",
            "2025-05-08 14:59:22,168 - asistente_legal - INFO - Documentos cargados: 404\n",
            "2025-05-08 14:59:23,325 - asistente_legal - INFO - Documentos divididos en 798 fragmentos\n"
          ]
        }
      ],
      "source": [
        "# Celda 2: Carga y procesamiento de documentos mejorado\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import re\n",
        "import tiktoken\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Lista de rutas de los PDFs\n",
        "pdf_paths = [\n",
        "    \"BOE-A-1978-31229-consolidado.pdf\",\n",
        "    \"BOE-A-1995-25444-consolidado_CodigoPenal.pdf\",\n",
        "    \"Protección de Datos Personales y garantia de los derechos digitales.pdf\",\n",
        "    \"RGPD_boe.pdf\"\n",
        "]\n",
        "\n",
        "# Función para identificar tipo de documento y mejorar metadatos\n",
        "def enrich_metadata(doc):\n",
        "    filename = doc.metadata.get('source', '')\n",
        "\n",
        "    # Añadir información sobre tipo de documento\n",
        "    if \"RGPD\" in filename:\n",
        "        doc.metadata['tipo'] = 'RGPD'\n",
        "        doc.metadata['jerarquia'] = 'Reglamento Europeo'\n",
        "    elif \"BOE-A-1978\" in filename:\n",
        "        doc.metadata['tipo'] = 'Constitución'\n",
        "        doc.metadata['jerarquia'] = 'Constitución Española'\n",
        "    elif \"BOE-A-1995\" in filename:\n",
        "        doc.metadata['tipo'] = 'Código Penal'\n",
        "        doc.metadata['jerarquia'] = 'Ley Orgánica'\n",
        "    elif \"Protección de Datos\" in filename:\n",
        "        doc.metadata['tipo'] = 'LOPDGDD'\n",
        "        doc.metadata['jerarquia'] = 'Ley Orgánica'\n",
        "\n",
        "    # Extraer información de artículos si está disponible\n",
        "    article_match = re.search(r'Artículo (\\d+)', doc.page_content)\n",
        "    if article_match:\n",
        "        doc.metadata['tipo_contenido'] = 'artículo'\n",
        "        doc.metadata['num_articulo'] = article_match.group(1)\n",
        "\n",
        "    return doc\n",
        "\n",
        "# Cargar documentos de todos los PDFs con manejo de errores\n",
        "documents = []\n",
        "for path in pdf_paths:\n",
        "    try:\n",
        "        logger.info(f\"Cargando documento: {path}\")\n",
        "        loader = PyPDFLoader(path)\n",
        "        documents.extend(loader.load())\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error cargando {path}: {e}\")\n",
        "\n",
        "# Limpiar y normalizar contenido\n",
        "for doc in documents:\n",
        "    doc.page_content = doc.page_content.replace('\\r\\n', '\\n').replace('\\n\\n', '\\n').strip()\n",
        "    doc = enrich_metadata(doc)\n",
        "\n",
        "logger.info(f\"Documentos cargados: {len(documents)}\")\n",
        "\n",
        "# Splitter optimizado para textos legales\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    chunk_size=800,  # Tamaño optimizado para capturar contexto legal completo\n",
        "    chunk_overlap=200,  # Mayor overlap para mantener coherencia\n",
        "    separators=[\"\\n\\n\\n\", \"\\n\\n\", \"\\n\", \".\", \" \", \"\"],\n",
        "    encoding_name=\"cl100k_base\"  # Compatible con modelos modernos\n",
        ")\n",
        "\n",
        "docs = text_splitter.split_documents(documents=documents)\n",
        "logger.info(f\"Documentos divididos en {len(docs)} fragmentos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 563,
          "referenced_widgets": [
            "cac764d083e34e8798681ea14fcf1a75",
            "dde8564ed950447490393f433add95a6",
            "d5aa82b997ab4be189285e681efc564d",
            "2227fdd27eaa4ce49be2a0f852496be2",
            "99f72a35e9b84bb391be822e751193ab",
            "0487c8e4a08f4e20b33aae61a069b7d7",
            "6cea37a6ac1244faa10da603cf53c985",
            "6f342ebd95af4f33b51649261a89bea2",
            "d1e7a5f5fb554318a2dcf99b68077cf4",
            "f4d842b8447c4d2db29a2a5646be53db",
            "16d2bef217f04594bbc94acd7971ddac",
            "9dd150275e074a99813d6d33f497a678",
            "49c87fcf94e74d7da5dc14803b102cb3",
            "08ac4c49dee94f17a6ef197a985fd780",
            "f3e1221f87d3405f9e0a2626f2202dfc",
            "d35613bcb73e4faeb93e74fd71441c48",
            "b950535e3e7347a0af7628d5bf7b80ef",
            "b82314f359324390b94eb921847d5e08",
            "90dbffdf96bf4d609c59d0c631d6fdfd",
            "7d238b16dac94154bd3e24df5636bc65",
            "db9b43bd16f541f5bfca796e23391575",
            "b85cbf9a65f54e699e1c539938fe5a6f",
            "1bed7cd946514427849fee72d8762fb4",
            "ff68046ed0bd417e92e303c15c43d7f2",
            "b7c8a149304948f7850e24b3b6e2f3b4",
            "9402beee1ca44853b81ca1425b38bdad",
            "709541f6add042f9942ece0d18d0ed8b",
            "ba9223c462864718968d1d2399fba000",
            "64d9434b4d4c43bcab5188ef0cd22ccc",
            "782afad7b7474bc09650a78759a916e9",
            "e4cf5cf7bdda4c4b8785ed121d800531",
            "3cdf4dc180b64a54844353e64725cf3e",
            "382bbae83bbc416395edb127b6bec9ec",
            "ac6bd87456c84942b74b461d8a8126b6",
            "c1b94c562bc04a9197388bbe47290df8",
            "c543d5b573f746ac8cbf54af455a242c",
            "1a2674c45ca046cba90f0040eef6fe84",
            "0fcfe5f97c8147a3a375cec97fecd359",
            "b252adcd86424e549429e91c42e4231c",
            "c58bab0f2bd546bcb962fb45a3d6de17",
            "b3b7b90e3607427f94cffed566a51bcb",
            "c2e2e5027bff43b5b47f07cd7b80ff66",
            "f1e88ba228d844f697ac66ab1f11a0f5",
            "2d9361af973f426aaf8b6e7816c18f80",
            "0b98782b5d1d4862af17ae67cc5777ca",
            "ed1a808877b54b14a01c0d4ab757dc81",
            "5462e6730819423292ff22ba556578d2",
            "148d32d501274b418bb8d63f2831da34",
            "9dff1428059d4bc1b396338372a0ac66",
            "97a26d010d1340b2811b8cdaf4c8bce9",
            "e06d7dd1a809445db2ae8996af6dabf9",
            "a47b8d1c755f486686afbd01f85dff34",
            "32840c7d1b414841bc28c7a9fd81be33",
            "adc72504dd424f948cc7117553212d57",
            "0e7b0815f4424615866b745392e1348c",
            "121e5097c4da4938af588656ee8a14dc",
            "9013e44d4b234355bf5cb5ab2cf282a1",
            "54421f39df1a41efacb0b907c57922f6",
            "23ca931599514777b22a9c78d175ed21",
            "5ca0d161ddec4450bec2a0cb8d0ad683",
            "0015e48e30504dcfa20e224ddfcf823d",
            "4e81edbb12584ac9b2bcc3d34aff7e8f",
            "92e8fb8fd761435b8873b6ea8bf8fe91",
            "83bd0985ce934f8e90925148a2fb29c2",
            "24e1e0ebc9f04fadbae02b192b57bfa4",
            "07e44da4ada941eb8fc5b45251049802",
            "7232fabaecbe44f0bfb85efabc852adf",
            "285d4440076a4c68a6f8317266e880c7",
            "ddf79a29dab14cc9916571d5ef10b224",
            "283f2a5610894deb9a7b10c6403cd4bb",
            "2e39a170df4e413f960ec363bbea2095",
            "5808bf7c644e4228ae669343d44c59bd",
            "4a2150ec026648bcba1c75a31618131d",
            "dacd4821451644cebf3cdc739762698f",
            "c418be902e11469cbb66deb117f9362d",
            "289b9bc44ac44e6b9876b95b2f0a3133",
            "20e48edeeb834c16a0e592889bfb33a6",
            "22ffe8f6ecc941b492179e53a8ad6928",
            "1cd4963370fd42d8beae2e798ffee9a3",
            "b5d505d5789f43cca86cb1a621745015",
            "2676c6f283104ded9ae4c7f9420f1473",
            "7ae263e1a157415b989d86b39dc86ab9",
            "ec0f34d02eee4fcf9d7f51d86b768b46",
            "30ed64dac9cc4a218734f8678d6b35b4",
            "365a847d2fb949b08ad2799ddb9bdfe8",
            "9932f74e242f4119bf2ef6c3c9306087",
            "5dd53b147d08431e8ecc73aa798ad889",
            "615c3c243a214a27a95a1b3cc807bc21",
            "bf90afd20c1f4717910955a9d481ae9b",
            "84ded545991a482ea540a08ec0efac93",
            "e218685f2f2a403daeaf63c34204d6e5",
            "dee2d0ee41014773b887fb8365d9506d",
            "5d9c82cc676d40c9a92d9ccca8a4e3e7",
            "4d0f2246d36943cfab3dd60260c92912",
            "86f61fb933494d148da6d6b0d64cf56c",
            "bfbaf1e01bc34bb9a3d7a11d47fdc683",
            "adb37cae3e0c4ab986c5c0704becf8f3",
            "7df0ca10a6b64cf49549a592019744af",
            "38053e33cafd4207bc3d5ed0537b92ea",
            "866d6d319be342cc9c9c1584d79e276f",
            "f03102b868c54d0aa2e47d4971d01cd0",
            "8a74df7879114abf83c02f59b86179e3",
            "e1897a57fea34737ac7848789f951bae",
            "34d9097b358f4066b81610a643a91367",
            "2ce45738eaed494288408212e78724f2",
            "9bb6e499ffb0400e9abfb4c4df69ad10",
            "04df9580b1ec4ca79d282383eafabaa2",
            "b75d3f266ade490b8b49393883fad4c5",
            "193259b79e294176983888af51b51103",
            "dac4ebc417c14936af453c15258c605e",
            "14d56ccfd0904350abe91735f301aab7",
            "da6f107176a642e3bf5d4620ae011d03",
            "ed9e04c3c7f84a649a4c36b8f517ca90",
            "e093a79cbce24644ac2f510187798adf",
            "4adaeabf499941c1b5c3ca13bccda9f8",
            "005b82a3932a4ecb9770921e61eee168",
            "d3d61388da654567bcf61577a65f60a5",
            "bae2e5076ac64f6f9b89e73197d2de1f",
            "50efde67ef904d4693da167df766d993",
            "cf1622cec12146bdac9caa903985e340",
            "15614a90630f48f28e345ee0e71d4306"
          ]
        },
        "id": "2SkrBhqrsDe7",
        "outputId": "e1a3bf72-96a1-4385-ff00-1fa3c369c1fa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 14:59:26,985 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n",
            "2025-05-08 14:59:29,788 - asistente_legal - INFO - Creando índice vectorial FAISS\n",
            "2025-05-08 14:59:41,540 - faiss.loader - INFO - Loading faiss with AVX512 support.\n",
            "2025-05-08 14:59:41,541 - faiss.loader - INFO - Could not load library with AVX512 support due to:\n",
            "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx512'\")\n",
            "2025-05-08 14:59:41,541 - faiss.loader - INFO - Loading faiss with AVX2 support.\n",
            "2025-05-08 14:59:41,808 - faiss.loader - INFO - Successfully loaded faiss with AVX2 support.\n",
            "2025-05-08 14:59:41,817 - faiss - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n",
            "2025-05-08 14:59:41,917 - asistente_legal - INFO - Se indexaron 798 fragmentos\n"
          ]
        }
      ],
      "source": [
        "# Celda 3: Embeddings mejorados y sistema de recuperación\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "\n",
        "# Configuración de embeddings para español y textos jurídicos\n",
        "embedding_model_name = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
        "model_kwargs = {\"device\": \"cpu\"}\n",
        "\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=embedding_model_name,\n",
        "    model_kwargs=model_kwargs\n",
        ")\n",
        "\n",
        "# Crear vector store\n",
        "logger.info(\"Creando índice vectorial FAISS\")\n",
        "vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "# Guardar y recargar el vector store\n",
        "vectorstore.save_local(\"faiss_index_\")\n",
        "persisted_vectorstore = FAISS.load_local(\"faiss_index_\", embeddings, allow_dangerous_deserialization=True)\n",
        "\n",
        "# Crear retriever base con MMR para diversidad de resultados\n",
        "base_retriever = persisted_vectorstore.as_retriever(\n",
        "    search_type=\"mmr\",  # Maximum Marginal Relevance para diversidad\n",
        "    search_kwargs={\n",
        "        \"k\": 5,  # Recuperar más documentos inicialmente\n",
        "        \"fetch_k\": 10,  # Considerar más candidatos\n",
        "        \"lambda_mult\": 0.7  # Balance entre relevancia y diversidad\n",
        "    }\n",
        ")\n",
        "\n",
        "logger.info(f\"Se indexaron {len(docs)} fragmentos\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53Lwzj1esDe7",
        "outputId": "015f25c0-79e1-4d12-ed55-512c09713e9a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 14:59:48,623 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test del modelo exitoso:\n",
            "**Ejemplo: Una clínica dental utiliza un servicio en la nube para almacenar historiales médicos, pero no informa a los pacientes sobre cómo se protegen sus datos.**\n",
            "\n",
            "**Problema:** La clínica no está cumpliendo con la normativa de protección de datos, ya que no informa sobre el tratamiento de los datos ni toma medidas de seguridad adecuadas para un servicio en la nube.\n",
            "\n",
            "**Solución:** La clínica debe informar a los pacientes sobre cómo se almacenan y protegen sus datos en la nube, incluyendo quién es el responsable del tratamiento. Además, debe implementar medidas de seguridad como cifrado y acceso restringido para garantizar la integridad de los historiales médicos.\n"
          ]
        }
      ],
      "source": [
        "# Celda 4: Configuración del modelo LLM\n",
        "from langchain_ollama import OllamaLLM\n",
        "import time\n",
        "\n",
        "# Inicializar el modelo LLaMA con timeout\n",
        "llm_legal = OllamaLLM(\n",
        "    model=\"hf.co/serdom02/Leyeneitor_8bitQ8_0\",\n",
        "    base_url=\"http://127.0.0.1:11434\",\n",
        "    temperature=0.3,  # Menor temperatura para respuestas legales más precisas\n",
        "    timeout=60  # Timeout de 60 segundos para evitar bloqueos\n",
        ")\n",
        "llm_conversacion = OllamaLLM(\n",
        "    model=\"llama3:8b\",\n",
        "    base_url=\"http://127.0.0.1:11434\",\n",
        "    temperature=0.6, # temperatura más alta para conversación\n",
        "    timeout=60\n",
        ")\n",
        "# Inicializar un modelo para evaluación/clasificación (puede ser el mismo)\n",
        "llm_evaluador = llm_conversacion\n",
        "\n",
        "llm=llm_legal #El modelo principal va a ser nuestro LLM Finetuneado\n",
        "\n",
        "# Test rápido del modelo\n",
        "try:\n",
        "    response = llm_legal.invoke(\"Un breve ejemplo sobre protección de datos\")\n",
        "    print(\"Test del modelo exitoso:\")\n",
        "    print(response)\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error al probar el modelo: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lgzgx2VYsDe7"
      },
      "outputs": [],
      "source": [
        "# --- 7. Prompts mejorados según tipología de consulta ---\n",
        "\n",
        "# Prompt base/genérico (fallback)\n",
        "system_prompt_rag = \"\"\"\n",
        "Sistema: Eres un asistente legal especializado en protección de datos en España (RGPD y LOPDGDD). Tu objetivo es proporcionar respuestas claras, precisas y fundamentadas en la ley.\n",
        "\n",
        "Contexto legal relevante extraído de la documentación:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Consulta del usuario: {question}\n",
        "\n",
        "Instrucciones:\n",
        "1. Basa tu respuesta EXCLUSIVAMENTE en el contexto legal proporcionado arriba.\n",
        "2. Si el contexto no es suficiente para responder, indica que la información no se encuentra en los documentos proporcionados.\n",
        "3. Responde de forma directa y estructurada a la consulta del usuario.\n",
        "4. Cita las fuentes (ej. \"Según el Artículo X del RGPD...\") si es posible basándote en el contexto.\n",
        "5. Evita dar opiniones personales o información no verificada.\n",
        "\"\"\"\n",
        "\n",
        "# Prompt para citas legales\n",
        "legal_citation_prompt = \"\"\"\n",
        "Sistema: Eres un asistente jurídico experto en el Reglamento General de Protección de Datos (RGPD). Tu tarea es citar artículos legales de forma textual y precisa cuando el usuario lo solicita.\n",
        "\n",
        "---------------------\n",
        "Documentos disponibles:\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Consulta del usuario:\n",
        "{question}\n",
        "\n",
        "Instrucciones:\n",
        "1. Extrae **todos los artículos del RGPD** que traten sobre el tema mencionado en la consulta.\n",
        "2. Para cada artículo encontrado, incluye:\n",
        "   - El número del artículo (ej. Artículo 6 del RGPD)\n",
        "   - El **texto literal más relevante** (puede ser un apartado si es muy largo)\n",
        "   - La **fuente** (ej. “Fuente: RGPD, pág. 12”)\n",
        "3. Usa este formato:\n",
        "   Artículo X del RGPD:\n",
        "   “Texto legal...”\n",
        "   Fuente: RGPD, pág. X\n",
        "\n",
        "4. Si hay varios artículos relevantes, enuméralos claramente.\n",
        "5. No inventes. Si el texto literal no está en el contexto, indícalo explícitamente.\n",
        "6. No incluyas artículos de otras leyes (como LOPDGDD o Constitución) salvo que se mencione expresamente.\n",
        "\n",
        "Ejemplo:\n",
        "Artículo 6.1 del RGPD:\n",
        "“El tratamiento será lícito solo si se cumple al menos una de las siguientes condiciones: [...]”\n",
        "Fuente: RGPD, pág. 8\n",
        "\"\"\"\n",
        "\n",
        "legal_multi_citation_prompt = \"\"\"\n",
        "Eres un asistente jurídico experto en el RGPD. El usuario solicita una lista de artículos relacionados con un tema específico.\n",
        "\n",
        "Instrucciones:\n",
        "1. Identifica todos los artículos del RGPD que se relacionen con el tema de la consulta.\n",
        "2. Si el artículo aparece en el contexto, cita su número y el **texto literal completo** relevante.\n",
        "3. Si no aparece el texto completo, menciona el número del artículo y una breve descripción basada en el título o lo que sepas del RGPD.\n",
        "4. No inventes textos. Si hay dudas, di que no tienes acceso al contenido exacto.\n",
        "5. Formatea la respuesta como una lista clara y numerada.\n",
        "\n",
        "Consulta: {question}\n",
        "Fragmentos del RGPD disponibles:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "Respuesta:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Prompt para análisis legal\n",
        "legal_analysis_prompt = \"\"\"\n",
        "Sistema: Eres un jurista experto analizando situaciones bajo la ley de protección de datos española (RGPD, LOPDGDD). Razonas jurídicamente paso a paso.\n",
        "\n",
        "Contexto legal relevante extraído de la documentación:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Consulta del usuario que requiere análisis legal: {question}\n",
        "\n",
        "Instrucciones para tu análisis:\n",
        "1. **Identifica la cuestión jurídica principal** planteada en la consulta.\n",
        "2. **Selecciona las normas aplicables** del contexto legal proporcionado que sean pertinentes para la cuestión.\n",
        "3. **Analiza los hechos implícitos o explícitos** en la consulta a la luz de las normas seleccionadas.\n",
        "4. **Aplica las normas a los hechos**, explicando tu razonamiento paso a paso basado únicamente en el contexto proporcionado.\n",
        "5. **Formula una conclusión jurídica** clara y fundamentada en el análisis anterior y el contexto. Si el contexto es insuficiente, señala las limitaciones.\n",
        "\n",
        "Estructura tu respuesta:\n",
        "* **Cuestión planteada:** (Resume la pregunta legal)\n",
        "* **Normativa aplicable (según contexto):** (Menciona artículos/disposiciones relevantes del contexto)\n",
        "* **Análisis jurídico:** (Desarrolla tu razonamiento aquí, conectando contexto y consulta)\n",
        "* **Conclusión:** (Respuesta final basada en el análisis)\n",
        "\"\"\"\n",
        "\n",
        "# Prompt para consultas procedimentales\n",
        "procedural_prompt = \"\"\"\n",
        "Sistema: Eres un consultor experto en los procedimientos y trámites relacionados con la protección de datos en España (AEPD, derechos ARSULIPO, etc.). Proporcionas información práctica.\n",
        "\n",
        "Contexto legal relevante extraído de la documentación (puede contener información sobre procedimientos):\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Consulta del usuario sobre un procedimiento o trámite: {question}\n",
        "\n",
        "Instrucciones:\n",
        "1. Identifica claramente el **procedimiento o trámite** consultado.\n",
        "2. Busca en el contexto información relevante sobre los **pasos a seguir, plazos, requisitos, o autoridad competente**.\n",
        "3. Explica el procedimiento de forma **clara, secuencial y práctica**, basándote en la información del contexto.\n",
        "4. Si el contexto menciona la base legal, puedes indicarla brevemente, pero prioriza la **descripción del proceso**.\n",
        "5. Si la información específica sobre el procedimiento no está en el contexto, indica que no se puede detallar con la documentación disponible. NO inventes pasos o plazos.\n",
        "\"\"\"\n",
        "\n",
        "# Prompt para información general\n",
        "general_info_prompt = \"\"\"\n",
        "Sistema: Eres un asistente informativo sobre protección de datos. Explicas conceptos generales de forma clara y sencilla.\n",
        "\n",
        "Contexto legal relevante extraído de la documentación:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Consulta general del usuario: {question}\n",
        "\n",
        "Instrucciones:\n",
        "1. Identifica el concepto o tema general sobre el que pregunta el usuario.\n",
        "2. Busca definiciones, explicaciones o principios relevantes en el contexto proporcionado.\n",
        "3. Explica el concepto de forma clara y concisa, utilizando la información del contexto.\n",
        "4. Puedes usar ejemplos si el contexto los proporciona o si son derivados directos de la explicación legal.\n",
        "5. Cita la fuente si es relevante (ej. \"El RGPD define X como...\").\n",
        "6. Si la información no está en el contexto, indícalo.\n",
        "\"\"\"\n",
        "\n",
        "# Prompt para conversación general (cuando no se usa RAG)\n",
        "conversation_prompt = \"\"\"\n",
        "Sistema: Eres un asistente legal amable y conversacional llamado Leyeneitor. Tu especialidad es la protección de datos, PERO ahora estás en modo conversacional. Ignora tu rol legal por un momento y responde directamente a la pregunta o comentario del usuario de forma natural y breve como un asistente general.\n",
        "NO des respuestas sobre protección de datos o leyes a menos que la pregunta sea específicamente sobre eso. Sé breve y directo.\n",
        "\n",
        "Historial reciente de la conversación (para contexto):\n",
        "{memory}\n",
        "Asistente:\"\"\"\n",
        "\n",
        "action_oriented_prompt = \"\"\"\n",
        "Sistema: Eres un asesor jurídico especializado en protección de datos (RGPD, LOPDGDD) y derechos digitales. Tu tarea es explicar de manera clara qué **acciones, derechos o reclamaciones** puede ejercer el usuario en la situación planteada.\n",
        "\n",
        "Contexto legal relevante extraído de la documentación:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Nueva consulta (enfocada en qué puede hacer el usuario): {question}\n",
        "\n",
        "Instrucciones específicas para tu respuesta:\n",
        "1. **Identifica los derechos relevantes** (acceso, oposición, supresión, portabilidad, etc.) que el usuario podría ejercer según el contexto.\n",
        "2. **Describe las acciones prácticas** que puede realizar el usuario, incluyendo:\n",
        "   - Cómo ejercer su derecho (por ejemplo: “solicitarlo por escrito”, “presentar reclamación ante la AEPD”).\n",
        "   - Ante quién debe dirigirse (empresa, delegado de protección de datos, AEPD...).\n",
        "   - Qué requisitos o pasos debe seguir.\n",
        "3. **Si procede,** menciona los artículos legales que respaldan las acciones propuestas (sin recargar la respuesta).\n",
        "4. Explica de forma **estructurada y práctica**, usando pasos o listas si facilita la comprensión.\n",
        "5. Si no hay suficiente información en el contexto para dar un procedimiento concreto, **indícalo claramente**. No inventes.\n",
        "\n",
        "Estructura sugerida para tu respuesta:\n",
        "* **Derechos aplicables:** (Enumera los derechos relevantes)\n",
        "* **Acciones que puede realizar:** (Pasos claros y prácticos)\n",
        "* **Normativa de respaldo:** (Artículos relevantes, si es aplicable)\n",
        "* **Notas importantes:** (Advertencias o limitaciones si las hubiera)\n",
        "\n",
        "Evita explicaciones teóricas largas. Sé claro, útil y orientado a lo que el usuario puede **hacer**.\n",
        "\"\"\"\n",
        "refinement_module_prompt = \"\"\"\n",
        "Eres un jurista experto en derecho de protección de datos.\n",
        "Tu tarea es **corregir y mejorar** una respuesta legal manteniendo su estructura original.\n",
        "\n",
        "\n",
        "Basándote en los siguientes fallos identificados por un validador, **ajusta cada sección que lo requiera** para mejorar la precisión y completitud jurídica.\n",
        "No elimines secciones ni cambies su orden. Si una sección no requiere corrección, déjala intacta.\n",
        "Si alguno de los fallos detectados se refiere a consentimiento, derechos fundamentales o proporcionalidad, asegúrate de explicarlos de forma técnica y precisa en la sección correspondiente.\n",
        "\n",
        "--- FALLAS DETECTADAS ---\n",
        "{fallos}\n",
        "\n",
        "Es obligatorio que sigas la Estructura de la respuesta original\n",
        "\n",
        "--- RESPUESTA ORIGINAL ---\n",
        "{respuesta}\n",
        "\n",
        "--- INSTRUCCIONES ---\n",
        "Corrige las secciones necesarias dentro de la estructura, sin añadir otras partes nuevas. Usa lenguaje jurídico claro y fundamentado.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ds2AfDdrsDe7",
        "outputId": "47e26145-25f2-4f45-b40d-6d0a0f5cc3f3",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo LLM y retriever listos.\n",
            "\n",
            "Bienvenido al Asistente Legal de Protección de Datos.\n",
            "Escribe 'salir' para terminar.\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 868\u001b[0m\n\u001b[0;32m    865\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEscribe \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msalir\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m para terminar.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    867\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 868\u001b[0m     user_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTu consulta: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m user_query\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msalir\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    870\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1262\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1260\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1263\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1264\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1266\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1267\u001b[0m )\n",
            "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\ipykernel\\kernelbase.py:1305\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1303\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1304\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1305\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1306\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1307\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ],
      "source": [
        "# Celda 5: Sistema completo con mejoras y correcciones\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
        "from langchain.chains import LLMChain # Aunque no se usa directamente, puede ser útil\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import re\n",
        "import unicodedata\n",
        "import logging # Asegúrate de que el logger está configurado\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "# Obtener el logger configurado en la Celda 1\n",
        "logger = logging.getLogger(\"asistente_legal\")\n",
        "\n",
        "# --- 0. Para que cada usuario tenga su memoria ---\n",
        "class UsuarioSession:\n",
        "    def __init__(self):\n",
        "        self.memory = ImprovedMemory(max_turns=4)\n",
        "        self.last_query = None\n",
        "\n",
        "# --- 1. Memoria Conversacional Mejorada ---\n",
        "class ImprovedMemory:\n",
        "    def __init__(self, max_turns=6):\n",
        "        self.summary = \"\"\n",
        "        self.recent_turns = []\n",
        "        self.max_turns = max_turns\n",
        "        self.entities = {}  # Seguimiento de entidades legales\n",
        "\n",
        "    def get_all_turns(self):\n",
        "        \"\"\"Devuelve todos los turnos almacenados en la memoria\"\"\"\n",
        "        return self.recent_turns\n",
        "\n",
        "    def get_relevant_memory(self, query):\n",
        "        \"\"\"Devuelve memoria relevante para la consulta actual\"\"\"\n",
        "        # Versión sencilla: devolver todo el historial\n",
        "        return self.get_memory_as_text()\n",
        "        # Versión avanzada (requiere embeddings): pendiente\n",
        "\n",
        "    def add_turn(self, user_input, assistant_response, llm=None):\n",
        "        self.recent_turns.append((user_input, assistant_response))\n",
        "\n",
        "        # Extraer y rastrear entidades legales mencionadas\n",
        "        self._extract_entities(user_input + \" \" + assistant_response)\n",
        "\n",
        "        if len(self.recent_turns) > self.max_turns:\n",
        "            if llm_evaluador:  # Si tenemos acceso al LLM, generamos resumen, no usamos el modelo finetuneado para evitar sus sesgos legales\n",
        "                try:\n",
        "                    # Generar resumen del contexto anterior (turnos más antiguos) para ahorrar espacio del contexto\n",
        "                    context_to_summarize = \"\\n\".join([f\"U: {u}\\nA: {a}\" for u, a in self.recent_turns[:-self.max_turns]]) # Corregido: resumir los que se van a quitar\n",
        "                    if context_to_summarize: # Solo si hay algo que resumir\n",
        "                         prompt = f\"Resume brevemente los puntos legales clave de esta conversación anterior:\\n{context_to_summarize}\"\n",
        "                         new_summary = llm.invoke(prompt)\n",
        "                         # Concatenar resumen nuevo con el anterior si existe\n",
        "                         self.summary = f\"{self.summary}\\n{new_summary}\".strip()\n",
        "                         logger.info(\"Resumen de memoria actualizado.\")\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Error al generar resumen de memoria: {e}\")\n",
        "\n",
        "            # Mantener solo los turnos más recientes\n",
        "            self.recent_turns = self.recent_turns[-self.max_turns:]\n",
        "\n",
        "    def _extract_entities(self, text):\n",
        "        # Detección simple de entidades legales\n",
        "        patterns = {\n",
        "            'articulos': r'art(?:ículo|\\.)\\s+(\\d+)',\n",
        "            'leyes': r'(?:RGPD|LOPDGDD|Reglamento|Ley Orgánica|Constitución|Código Penal)'\n",
        "        }\n",
        "\n",
        "        for entity_type, pattern in patterns.items():\n",
        "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "            for match in matches:\n",
        "                entity = f\"{entity_type}_{match.upper()}\" # Normalizar nombre\n",
        "                self.entities[entity] = self.entities.get(entity, 0) + 1\n",
        "\n",
        "    def get_memory_as_text(self):\n",
        "        memory_text = \"\"\n",
        "        if self.summary:\n",
        "            memory_text += f\"Resumen de puntos legales anteriores:\\n{self.summary}\\n\\n\"\n",
        "\n",
        "        if self.recent_turns:\n",
        "             memory_text += \"Historial reciente de la conversación:\\n\"\n",
        "             memory_text += \"\\n\".join([f\"Usuario: {u}\\nAsistente: {a}\" for u, a in self.recent_turns])\n",
        "\n",
        "        # Añadir entidades más relevantes si existen\n",
        "        if self.entities:\n",
        "            top_entities = sorted(self.entities.items(), key=lambda x: x[1], reverse=True)[:3]\n",
        "            if top_entities:\n",
        "                memory_text += \"\\n\\nTemas legales clave mencionados: \" + \", \".join([e[0].replace('_', ' ') for e in top_entities])\n",
        "\n",
        "        return memory_text.strip()\n",
        "\n",
        "# --- 2. Sistema de Clasificación Avanzado (Versión única y completa) ---\n",
        "def classify_intent_advanced(query, llm_evaluador):\n",
        "\n",
        "    \"\"\"Sistema de clasificación avanzado con múltiples categorías y confianza\"\"\"\n",
        "    #Sistema cache para no reclasificar preguntas parecidas\n",
        "    cache_key = ' '.join(query.lower().split()[:10])  # Simplificamos clave: primeras 10 palabras\n",
        "    if cache_key in intent_classification_cache:\n",
        "        logger.info(f\"Resultado de clasificación obtenido de caché para: {cache_key}\")\n",
        "        return intent_classification_cache[cache_key]\n",
        "\n",
        "\n",
        "    classification_prompt_text = \"\"\"\n",
        "    Analiza la siguiente consulta legal y clasifícala en UNA de estas categorías:\n",
        "\n",
        "    1. LEGAL_CITATION: Requiere citar artículos específicos o textos legales exactos (e.g., \"¿Qué dice el artículo 5 del RGPD?\", \"Cita el artículo 18.4 de la Constitución\")\n",
        "    2. LEGAL_ANALYSIS: Requiere análisis jurídico basado en leyes/normativas (e.g., \"¿Es legal tratar datos de salud sin consentimiento explícito?\", \"¿Qué implicaciones tiene la sentencia X?\")\n",
        "    3. GENERAL_INFO: Información general sobre protección de datos (e.g., \"¿Qué es el RGPD?\", \"¿Cuáles son los derechos de los ciudadanos?\")\n",
        "    4. PROCEDURAL: Preguntas sobre procedimientos o trámites (e.g., \"¿Cómo puedo ejercer mi derecho de acceso?\", \"¿Qué pasos seguir para una reclamación en la AEPD?\")\n",
        "    5. CONVERSATION: Diálogo general, saludos, agradecimientos o consulta no legal (e.g., \"Hola\", \"Gracias\", \"¿Qué tiempo hace?\")\n",
        "\n",
        "    Responde SOLO con la categoría y un número del 1-100 que indique tu confianza, separados por un guion.\n",
        "    Formato: [CATEGORÍA] - [CONFIANZA]\n",
        "\n",
        "    Ejemplos:\n",
        "    Consulta: \"¿Qué dice el artículo 6 del RGPD?\" -> Respuesta: LEGAL_CITATION - 95\n",
        "    Consulta: \"¿Es legal que una empresa comparta mis datos sin permiso?\" -> Respuesta: LEGAL_ANALYSIS - 85\n",
        "    Consulta: \"¿Qué es la protección de datos?\" -> Respuesta: GENERAL_INFO - 90\n",
        "    Consulta: \"¿Cómo presento una reclamación a la AEPD?\" -> Respuesta: PROCEDURAL - 88\n",
        "    Consulta: \"Gracias por tu ayuda\" -> Respuesta: CONVERSATION - 92\n",
        "    Consulta: \"Hola buenos días\" -> Respuesta: CONVERSATION - 99\n",
        "    Consulta: \"Qué tal estás?\" -> Respuesta: CONVERSATION - 95\n",
        "    Consulta: \"Me llamo Paula\" -> Respuesta: CONVERSATION - 98\n",
        "    Consulta: \"Ok gracias\" -> Respuesta: CONVERSATION - 90\n",
        "    Consulta: \"¿Recuerdas mi nombre?\" -> Respuesta: CONVERSATION - 98\n",
        "    Consulta: \"¿De qué hemos hablado antes?\" -> Respuesta: CONVERSATION - 95\n",
        "    Consulta: \"¿Puedes resumir nuestra conversación?\" -> Respuesta: CONVERSATION - 90\n",
        "    Consulta: \"¿Eres un abogado?\" -> Respuesta: CONVERSATION - 92\n",
        "\n",
        "    Consulta del usuario: \"{user_query}\"\n",
        "    Clasificación y confianza:\"\"\"\n",
        "\n",
        "    prompt = PromptTemplate(template=classification_prompt_text, input_variables=[\"user_query\"])\n",
        "    # No se necesita crear una cadena completa aquí, solo invocar el LLM con el prompt formateado.\n",
        "    # classification_runnable = prompt | llm_evaluador # Esto crea una cadena, innecesario si solo invocas\n",
        "\n",
        "    try:\n",
        "        formatted_prompt = prompt.format(user_query=query)\n",
        "        result = llm_evaluador.invoke(formatted_prompt) # Invocar directamente\n",
        "\n",
        "        # Extraer texto y procesar la respuesta\n",
        "        answer_text = result.strip() if isinstance(result, str) else str(result).strip()\n",
        "\n",
        "        # Intentar extraer categoría y confianza\n",
        "        pattern = r'([A-Z_]+)\\s*-\\s*(\\d+)'\n",
        "        match = re.search(pattern, answer_text)\n",
        "\n",
        "        if match:\n",
        "            category = match.group(1)\n",
        "            confidence = int(match.group(2))\n",
        "            logger.info(f\"Clasificación: {category}, Confianza: {confidence}\")\n",
        "\n",
        "            # Determinar si usar RAG (basado en categoría O baja confianza)\n",
        "            # Ajusta esta lógica si prefieres usar RAG para GENERAL_INFO o PROCEDURAL también\n",
        "            use_rag = category in [\"LEGAL_CITATION\", \"LEGAL_ANALYSIS\", \"PROCEDURAL\"] or confidence < 75\n",
        "\n",
        "            return {\n",
        "                \"category\": category,\n",
        "                \"confidence\": confidence,\n",
        "                \"use_rag\": use_rag\n",
        "            }\n",
        "        else:\n",
        "            logger.warning(f\"No se pudo extraer categoría/confianza de la respuesta del clasificador: '{answer_text}'. Se usará RAG por defecto.\")\n",
        "            return {\n",
        "                \"category\": \"UNKNOWN\",\n",
        "                \"confidence\": 0,\n",
        "                \"use_rag\": True # Default seguro\n",
        "            }\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error en clasificación avanzada: {e}\")\n",
        "        return {\n",
        "            \"category\": \"ERROR\",\n",
        "            \"confidence\": 0,\n",
        "            \"use_rag\": True # Default seguro\n",
        "        }\n",
        "\n",
        "# --- 3. Estrategia de retrieval adaptativo ---\n",
        "def get_adaptive_retriever(query, base_retriever, llm_evaluador):\n",
        "    \"\"\"Devuelve un retriever configurado dinámicamente según la consulta\"\"\"\n",
        "\n",
        "    # Analizar complejidad y especificidad de la consulta\n",
        "    complexity_prompt = \"\"\"\n",
        "    Evalúa la complejidad y especificidad de esta consulta legal:\n",
        "    \"{query}\"\n",
        "\n",
        "    Responde solo con una de estas opciones:\n",
        "    - SIMPLE: Consulta general o introductoria\n",
        "    - MEDIA: Consulta moderadamente específica\n",
        "    - COMPLEJA: Consulta muy específica o técnica\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        complexity_result = llm_evaluador.invoke(complexity_prompt.replace(\"{query}\", query))\n",
        "        complexity = complexity_result.strip().upper()\n",
        "\n",
        "        filter_dict = None # Inicializar filtro\n",
        "\n",
        "        # Ajustar parámetros según complejidad\n",
        "        if \"SIMPLE\" in complexity:\n",
        "            k_value = 3 # Aumentado ligeramente para consultas simples\n",
        "            fetch_k = 6\n",
        "            lambda_mult = 0.6\n",
        "        elif \"MEDIA\" in complexity:\n",
        "            k_value = 5 # Aumentado\n",
        "            fetch_k = 10\n",
        "            lambda_mult = 0.7\n",
        "        else:  # COMPLEJA\n",
        "            k_value = 7 # Aumentado\n",
        "            fetch_k = 15\n",
        "            lambda_mult = 0.8\n",
        "\n",
        "            # Intentar determinar qué ley es más relevante para filtrar (si es compleja)\n",
        "            law_prompt = \"\"\"\n",
        "            Para esta consulta compleja: \"{query}\"\n",
        "            ¿Qué normativa parece MÁS relevante? Responde solo con UNA palabra clave:\n",
        "            - RGPD\n",
        "            - LOPDGDD\n",
        "            - CONSTITUCION\n",
        "            - CODIGO_PENAL\n",
        "            - OTRO (si no encaja claramente o requiere múltiples)\n",
        "            \"\"\"\n",
        "\n",
        "            law_result = llm_evaluador.invoke(law_prompt.replace(\"{query}\", query)).strip().upper()\n",
        "\n",
        "            # Configurar filtro si hay una ley específica y clara\n",
        "            if law_result == \"RGPD\":\n",
        "                filter_dict = {\"tipo\": \"RGPD\"}\n",
        "            elif law_result == \"LOPDGDD\":\n",
        "                filter_dict = {\"tipo\": \"LOPDGDD\"}\n",
        "            elif law_result == \"CONSTITUCION\":\n",
        "                filter_dict = {\"tipo\": \"Constitución\"}\n",
        "            elif law_result == \"CODIGO_PENAL\":\n",
        "                filter_dict = {\"tipo\": \"Código Penal\"}\n",
        "            # else: filtro sigue siendo None\n",
        "\n",
        "        # Configurar retriever con los parámetros adaptativos\n",
        "        # ¡Importante! Crear una *nueva* instancia o clonar para no modificar el base_retriever original globalmente\n",
        "        # Nota: as_retriever() crea una nueva instancia configurada\n",
        "        adaptive_retriever = persisted_vectorstore.as_retriever( # Asume persisted_vectorstore es global o pasado como argumento\n",
        "            search_type=\"mmr\",\n",
        "            search_kwargs={\n",
        "                'k': k_value,\n",
        "                'fetch_k': fetch_k,\n",
        "                'lambda_mult': lambda_mult,\n",
        "                # Aplicar filtro si se determinó uno\n",
        "                **({'filter': filter_dict} if filter_dict else {})\n",
        "            }\n",
        "        )\n",
        "\n",
        "        logger.info(f\"Retriever adaptativo configurado: k={k_value}, fetch_k={fetch_k}, lambda={lambda_mult}, filtro={filter_dict}\")\n",
        "        return adaptive_retriever\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.warning(f\"Error configurando retriever adaptativo: {e}. Usando configuración por defecto.\")\n",
        "        # Devolver el retriever base original o uno con config por defecto\n",
        "        return base_retriever # O reconfigurar base_retriever con valores por defecto\n",
        "\n",
        "\n",
        "# --- 4. Sistema de validación y reintento ---\n",
        "def validate_legal_response(response, query, docs_used, llm): #Aqui usamos el modelo finetuneado porque es mas preciso para temas legales\n",
        "    \"\"\"Valida la calidad de una respuesta legal\"\"\"\n",
        "\n",
        "    validation_prompt = f\"\"\"\n",
        "    Evalúa la calidad de esta respuesta legal:\n",
        "\n",
        "    Consulta: {query}\n",
        "    Respuesta: {response}\n",
        "\n",
        "    Verifica SOLO estos 3 aspectos:\n",
        "    1. ¿Es jurídicamente precisa según la legislación española y europea de protección de datos? (Sí/No)\n",
        "    2. ¿Responde completamente a la consulta realizada? (Sí/No)\n",
        "    3. ¿Contiene contradicciones internas o errores evidentes? (Sí/No)\n",
        "\n",
        "    Responde estrictamente en este formato: [PRECISIÓN: Sí/No], [COMPLETITUD: Sí/No], [ERRORES: Sí/No]\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        result = llm.invoke(validation_prompt)\n",
        "\n",
        "        # Patrones de validación más robustos (ignorando mayúsculas/minúsculas y espacios)\n",
        "        precision_match = re.search(r'PRECISIÓN:\\s*(Sí|No)', result, re.IGNORECASE)\n",
        "        completitud_match = re.search(r'COMPLETITUD:\\s*(Sí|No)', result, re.IGNORECASE)\n",
        "        errores_match = re.search(r'ERRORES:\\s*(Sí|No)', result, re.IGNORECASE) # Busca 'No' para sin_errores\n",
        "\n",
        "        # Extraer resultados\n",
        "        precision_ok = precision_match and \"sí\" in precision_match.group(1).lower()\n",
        "        completitud_ok = completitud_match and \"sí\" in completitud_match.group(1).lower()\n",
        "        sin_errores = errores_match and \"no\" in errores_match.group(1).lower() # Es bueno si NO hay errores\n",
        "\n",
        "        # Calcular validez general\n",
        "        is_valid = precision_ok and completitud_ok and sin_errores\n",
        "\n",
        "        validation_result = {\n",
        "            \"valid\": is_valid,\n",
        "            \"precision\": precision_ok,\n",
        "            \"completitud\": completitud_ok,\n",
        "            \"sin_errores\": sin_errores,\n",
        "            \"raw_validation_output\": result.strip() # Guardar la salida cruda para depuración\n",
        "        }\n",
        "\n",
        "        logger.info(f\"Validación: {validation_result}\")\n",
        "        return validation_result\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error en validación de respuesta: {e}\")\n",
        "        return {\"valid\": True, \"error\": str(e)} # Asumir validez en caso de error para no bloquear\n",
        "\n",
        "# --- 6. Sistema de reordenamiento de documentos ---\n",
        "def rerank_documents(query, docs, llm, top_n=5):\n",
        "    \"\"\"Reordena documentos por relevancia usando LLM, devuelve los top_n\"\"\"\n",
        "    if not docs:\n",
        "        return []\n",
        "\n",
        "    logger.info(f\"Iniciando reranking para {len(docs)} documentos recuperados...\")\n",
        "    try:\n",
        "        results = []\n",
        "        # Limitar el número de documentos a reordenar para eficiencia\n",
        "        docs_to_rerank = docs[:min(len(docs), 8)] # Reordenar hasta 8 documentos\n",
        "\n",
        "        for i, doc in enumerate(docs_to_rerank):\n",
        "            # Truncar contenido del documento para el prompt\n",
        "            content_preview = doc.page_content[:500] # Usar un fragmento más largo para evaluación\n",
        "\n",
        "            prompt = f\"\"\"\n",
        "            Evalúa la relevancia de este fragmento de texto legal para responder a la siguiente consulta específica:\n",
        "\n",
        "            Consulta del usuario: \"{query}\"\n",
        "\n",
        "            Fragmento del documento ({doc.metadata.get('source', 'N/A')} - Pág. {doc.metadata.get('page', 'N/A')}):\n",
        "            \"{content_preview}...\"\n",
        "\n",
        "            Asigna una puntuación de relevancia del 0 al 10, donde 10 es extremadamente relevante y 0 es irrelevante.\n",
        "            Responde SOLAMENTE con el número de la puntuación:\"\"\"\n",
        "\n",
        "            score_text = llm.invoke(prompt).strip()\n",
        "\n",
        "            # Extraer puntuación de forma más robusta\n",
        "            score_match = re.search(r'\\b(10|[0-9])\\b', score_text) # Busca un número del 0-10 como palabra completa\n",
        "            if score_match:\n",
        "                score = float(score_match.group(0))\n",
        "            else:\n",
        "                logger.warning(f\"No se pudo extraer puntuación de reranking de: '{score_text}'. Usando 5.0 por defecto.\")\n",
        "                score = 5.0 # Valor neutral por defecto\n",
        "\n",
        "            results.append((doc, score))\n",
        "            logger.debug(f\"Doc {i} puntuado con {score}\")\n",
        "\n",
        "        # Ordenar por puntuación descendente\n",
        "        sorted_results = sorted(results, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Devolver los top_n documentos reordenados\n",
        "        reranked_docs = [doc for doc, score in sorted_results[:top_n]]\n",
        "        logger.info(f\"Reranking completado. {len(reranked_docs)} documentos seleccionados.\")\n",
        "\n",
        "        # Opcional: Añadir documentos no reordenados si top_n es mayor que los reordenados\n",
        "        # if len(reranked_docs) < top_n:\n",
        "        #     remaining_docs = docs[len(docs_to_rerank):]\n",
        "        #     reranked_docs.extend(remaining_docs[:top_n - len(reranked_docs)])\n",
        "\n",
        "        return reranked_docs\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error durante el reranking: {e}\")\n",
        "        return docs[:top_n] # Devolver los primeros N documentos originales en caso de error\n",
        "\n",
        "\n",
        "# --- 9. Selección de prompt según clasificación ---\n",
        "def get_prompt_by_category(category, memory_text=\"\", query=\"\"):\n",
        "    \"\"\"Devuelve el ChatPromptTemplate adecuado según la categoría de consulta\"\"\"\n",
        "    system_template = system_prompt_rag # Default\n",
        "    human_template = \"{question}\" # Input directo del usuario para RAG\n",
        "    prompt_used =\"\"\n",
        "    patterns_multi = [\"todos los artículos\", \"artículos relacionados\", \"varios artículos\", \"artículos del rgpd\", \"citame los artículos\", \"cítame varios\", \"cítame todos\"]\n",
        "\n",
        "    if is_follow_up_query(query) and is_action_request(query):\n",
        "        logger.info(\"Consulta de seguimiento orientada a acciones detectada. Usando action_oriented_prompt.\")\n",
        "        system_template = action_oriented_prompt\n",
        "    else:\n",
        "        if category == \"LEGAL_CITATION\" and any(pat in query.lower() for pat in patterns_multi):\n",
        "            system_template = legal_citation_prompt\n",
        "            prompt_used = \"legal_citation_prompt\"\n",
        "        elif category == \"LEGAL_CITATION\":\n",
        "            system_template = legal_multi_citation_prompt\n",
        "            prompt_used = \"legal_multi_citation_prompt\"\n",
        "        elif category == \"LEGAL_ANALYSIS\" and is_action_request(query):\n",
        "            system_template = action_oriented_prompt\n",
        "            prompt_used = \"action_oriented_prompt\"\n",
        "        elif category == \"LEGAL_ANALYSIS\":\n",
        "            system_template = legal_analysis_prompt\n",
        "            prompt_used = \"legal_analysis_prompt\"\n",
        "        elif category == \"PROCEDURAL\":\n",
        "            system_template = procedural_prompt\n",
        "            prompt_used = \"procedural_prompt\"\n",
        "        elif category == \"GENERAL_INFO\":\n",
        "            system_template = general_info_prompt # Usar prompt específico para info general\n",
        "            prompt_used = \"general_info_prompt\"\n",
        "        elif category == \"CONVERSATION\":\n",
        "            # Para conversación, no usamos contexto RAG, usamos memoria\n",
        "            system_template = conversation_prompt.format(memory=memory_text) # Inyectar memoria aquí\n",
        "            human_template = \"{question}\" # Sigue siendo la pregunta del usuario\n",
        "            # Devolver directamente el prompt formateado para conversación, ya que no pasará por load_qa_chain\n",
        "            # OJO: Esto requiere que el flujo principal maneje esto diferente.\n",
        "            # Por simplicidad ahora, devolvemos estructura similar, pero el flujo debe saber NO usar RAG.\n",
        "            # Alternativa: devolver None o un identificador especial.\n",
        "            # Vamos a devolver la estructura estándar por ahora, asumiendo que el flujo principal lo maneja.\n",
        "            # PERO, el prompt de conversación NO tiene variable {context}.\n",
        "            # => Mejor devolver None para indicar que no se use RAG/load_qa_chain.\n",
        "            logger.info(\"Categoría CONVERSATION: No se usará RAG. Se generará respuesta directa.\")\n",
        "            prompt_used = \"conversation_prompt\"\n",
        "            # Construir un prompt simple para LLM directo\n",
        "            return ChatPromptTemplate.from_messages([\n",
        "                 (\"system\", system_template), # Ya formateado con memoria\n",
        "                 (\"human\", human_template)\n",
        "            ])\n",
        "\n",
        "    logger.info(f\"Usando el prompt: {prompt_used}\")\n",
        "    # Para las categorías que usan RAG (con contexto)\n",
        "    # El human_template debe incluir la pregunta del usuario\n",
        "    # El system_template incluye {context} y {question}\n",
        "    # load_qa_chain se encargará de llenar {context} y {question}\n",
        "    return ChatPromptTemplate.from_messages([\n",
        "        SystemMessagePromptTemplate.from_template(system_template),\n",
        "        HumanMessagePromptTemplate.from_template(human_template) # Solo {question} aquí, load_qa_chain lo maneja\n",
        "    ])\n",
        "\n",
        "\n",
        "# --- ¡¡¡DEFINICIÓN NECESARIA DE LA CADENA RAG!!! ---\n",
        "# Necesitamos definir cómo se combinarán el LLM y el Prompt con los documentos.\n",
        "# Usamos load_qa_chain. El tipo de cadena (\"stuff\", \"map_reduce\", etc.) puede variar.\n",
        "# \"stuff\" es simple pero puede exceder el límite de tokens si hay muchos documentos.\n",
        "# \"map_reduce\" o \"refine\" son más robustos para contextos largos.\n",
        "# Probemos con \"stuff\" inicialmente dado el chunk_size de 800.\n",
        "\n",
        "# Nota: La cadena se crea aquí, pero el *prompt específico* se pasará en cada invocación.\n",
        "# Esto es más flexible que crear una cadena diferente cada vez.\n",
        "# OJO: load_qa_chain espera un prompt específico en su creación.\n",
        "# Vamos a crear una función que genere la cadena CON el prompt adecuado CADA VEZ.\n",
        "\n",
        "def create_rag_chain(llm, prompt):\n",
        "    \"\"\"Crea la cadena load_qa_chain con el LLM y el prompt específicos.\"\"\"\n",
        "    # El prompt debe ser un BasePromptTemplate (como ChatPromptTemplate)\n",
        "    if not isinstance(prompt, (PromptTemplate, ChatPromptTemplate)):\n",
        "         logger.error(\"El prompt proporcionado a create_rag_chain no es válido.\")\n",
        "         # Se puede lanzar un error o devolver None/cadena por defecto\n",
        "         return None\n",
        "\n",
        "    # Selecciona el tipo de cadena. 'stuff' es bueno para empezar.\n",
        "    # Ajusta 'chain_type' si tienes problemas de longitud de contexto.\n",
        "    qa_chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=prompt, verbose=False) # verbose=True para debug\n",
        "    return qa_chain\n",
        "\n",
        "def is_follow_up_query(query):\n",
        "    \"\"\"Detecta si la nueva pregunta es una continuación (seguimiento) del contexto anterior.\"\"\"\n",
        "    query_lower = query.strip().lower()\n",
        "    follow_up_starts = [\n",
        "        \"entonces\", \"y\", \"pero\", \"por qué\", \"qué pueden hacer\",\n",
        "        \"qué derechos tienen\", \"cómo reclamar\", \"porque\",\"qué consecuencias\",\n",
        "        \"qué opciones tienen\", \"cómo actuar\", \"qué recursos tienen\"\n",
        "    ]\n",
        "    return any(query_lower.startswith(start) for start in follow_up_starts) or len(query.split()) <= 8\n",
        "\n",
        "\n",
        "def build_contextual_query(last_query, current_query):\n",
        "    \"\"\"Construye una consulta combinando la anterior y la nueva\"\"\"\n",
        "    return f\"Respecto a la situación planteada previamente: {last_query}\\nNueva pregunta: {current_query}\"\n",
        "\n",
        "def classify_intent_with_cache(query, llm_evaluador):\n",
        "    \"\"\"\n",
        "    Clasifica la intención de una consulta usando caché para evitar llamadas repetidas al LLM.\n",
        "    \"\"\"\n",
        "    cache_key = ' '.join(query.lower().split()[:10])  # Usamos las primeras 10 palabras para normalizar claves\n",
        "\n",
        "    if cache_key in intent_classification_cache:\n",
        "        logger.info(f\"Resultado de clasificación obtenido de caché para: {cache_key}\")\n",
        "        return intent_classification_cache[cache_key]\n",
        "\n",
        "    # No está en caché: clasificamos\n",
        "    classification_result = classify_intent_advanced(query, llm_evaluador)\n",
        "\n",
        "    # Guardamos el resultado en caché\n",
        "    intent_classification_cache[cache_key] = classification_result\n",
        "\n",
        "    return classification_result\n",
        "\n",
        "def adapt_retriever(base_retriever, attempt):\n",
        "    \"\"\"Adapta el recuperador según el intento para mejorar recuperación de documentos.\"\"\"\n",
        "    if attempt == 1:\n",
        "        logger.info(\"Reintento 1: aumentando k y activando reranking.\")\n",
        "        return persisted_vectorstore.as_retriever(\n",
        "            search_type=\"mmr\",\n",
        "            search_kwargs={'k': 8, 'fetch_k': 16, 'lambda_mult': 0.75}\n",
        "        )\n",
        "    elif attempt == 2:\n",
        "        logger.info(\"Reintento 2: creando un retriever más flexible (MMR + reformulación posible).\")\n",
        "        return persisted_vectorstore.as_retriever(\n",
        "            search_type=\"mmr\",\n",
        "            search_kwargs={'k': 10, 'fetch_k': 20, 'lambda_mult': 0.6}\n",
        "        )\n",
        "    else:\n",
        "        return base_retriever\n",
        "\n",
        "\n",
        "# --- 5. Sistema de reintento con estrategias alternativas ---\n",
        "def get_response_with_retry(query, llm, base_retriever, memory: ImprovedMemory, last_query=None, max_attempts=3):\n",
        "    \"\"\"Obtiene respuesta con sistema de reintentos, clasificación con caché, y detección de continuidad conversacional.\"\"\"\n",
        "    from sentence_transformers import SentenceTransformer, util\n",
        "    reformulation_embedder = SentenceTransformer(\"paraphrase-multilingual-mpnet-base-v2\")\n",
        "\n",
        "    memory_text = memory.get_relevant_memory(query)\n",
        "\n",
        "    # --- Detectar si la pregunta depende de la anterior ---\n",
        "    if is_follow_up_query(query) and last_query:\n",
        "        logger.info(\"Detectada pregunta de seguimiento. Incorporando última consulta como contexto.\")\n",
        "        query = f\"Contexto anterior: {last_query}\\n\\nNueva consulta: {query}\"\n",
        "\n",
        "    # --- 3. Clasificar la intención usando caché ---\n",
        "    classifier_llm = llm_evaluador if 'llm_evaluador' in globals() and llm_evaluador else llm\n",
        "    classification_result = classify_intent_with_cache(query, classifier_llm)\n",
        "    category = classification_result[\"category\"]\n",
        "    confidence = classification_result[\"confidence\"]\n",
        "\n",
        "    use_rag = category in [\"LEGAL_CITATION\", \"LEGAL_ANALYSIS\", \"PROCEDURAL\"] or (category == \"GENERAL_INFO\" and confidence < 85) or confidence < 75\n",
        "    if category == \"CONVERSATION\":\n",
        "        use_rag = False\n",
        "\n",
        "    logger.info(f\"Intención clasificada como: {category} (Confianza: {confidence}%) - Usar RAG: {use_rag}\")\n",
        "\n",
        "    response_data = None\n",
        "\n",
        "    # --- 4. Generar respuesta directa o RAG ---\n",
        "    if not use_rag:\n",
        "        # --- RESPUESTA DIRECTA ---\n",
        "        try:\n",
        "            system_prompt = conversation_prompt.format(memory=memory_text) if category == \"CONVERSATION\" else f\"\"\"\n",
        "            Sistema: Eres un asistente experto que responde preguntas con precisión y amabilidad.\n",
        "            Contexto conversacional previo:\n",
        "            {memory_text}\n",
        "\n",
        "            Usuario: {{question}}\n",
        "            \"\"\"\n",
        "            prompt = ChatPromptTemplate.from_messages([\n",
        "                (\"system\", system_prompt),\n",
        "                (\"human\", \"{question}\")\n",
        "            ])\n",
        "            llm_chain = LLMChain(llm=llm_conversacion, prompt=prompt, output_key='text')\n",
        "            result = llm_chain.invoke({\"question\": query})\n",
        "            response_text = result.get('text', str(result)).strip()\n",
        "\n",
        "            response_data = {\"response\": response_text, \"docs\": [], \"attempt\": 1, \"category\": category, \"validation\": {\"valid\": True}}\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generando respuesta directa: {e}\", exc_info=True)\n",
        "            response_data = {\"response\": f\"Lo siento, no pude generar una respuesta directa.\", \"docs\": [], \"attempt\": 1, \"category\": category, \"error\": str(e)}\n",
        "\n",
        "    else:\n",
        "        # --- RESPUESTA RAG ---\n",
        "        selected_prompt_template = get_prompt_by_category(category, memory_text, query)\n",
        "\n",
        "        for attempt in range(max_attempts):\n",
        "            try:\n",
        "                logger.info(f\"Intento RAG {attempt+1} para consulta: '{query}' (Categoría: {category})\")\n",
        "\n",
        "                retriever = adapt_retriever(base_retriever, attempt)\n",
        "                retrieved_docs = retriever.get_relevant_documents(query)\n",
        "                logger.info(f\"Recuperados {len(retrieved_docs)} documentos.\")\n",
        "\n",
        "                if attempt > 0 and retrieved_docs:\n",
        "                    retrieved_docs = rerank_documents(query, retrieved_docs, llm, top_n=4)\n",
        "                    logger.info(f\"Documentos después de reranking: {len(retrieved_docs)}\")\n",
        "\n",
        "                if not retrieved_docs:\n",
        "                    logger.warning(\"No se encontraron documentos relevantes.\")\n",
        "\n",
        "                    if attempt == max_attempts - 1:\n",
        "                        response_data = {\n",
        "                            \"response\": \"No he encontrado información específica para responder con seguridad, pero puedo intentar darte una orientación general.\",\n",
        "                            \"docs\": [],\n",
        "                            \"attempt\": attempt + 1,\n",
        "                            \"category\": category,\n",
        "                            \"validation\": {\"valid\": False, \"error\": \"Sin documentos relevantes\"}\n",
        "                        }\n",
        "                        break  # Salir del bucle\n",
        "                    else:\n",
        "                        continue\n",
        "\n",
        "                chain = create_rag_chain(llm, selected_prompt_template)\n",
        "                result = chain.invoke({\"input_documents\": retrieved_docs, \"question\": query})\n",
        "                response_text = result.get('output_text', result if isinstance(result, str) else str(result)).strip()\n",
        "\n",
        "                validation = validate_legal_response(response_text, query, retrieved_docs, llm)\n",
        "                if validation.get(\"valid\", False):\n",
        "                    logger.info(f\"Respuesta validada en intento {attempt+1}.\")\n",
        "                    response_data = {\n",
        "                        \"response\": response_text,\n",
        "                        \"docs\": retrieved_docs,\n",
        "                        \"attempt\": attempt+1,\n",
        "                        \"category\": category,\n",
        "                        \"validation\": validation\n",
        "                    }\n",
        "                    break\n",
        "                else:\n",
        "                    logger.warning(f\"Respuesta no válida en intento {attempt+1}: {validation}\")\n",
        "\n",
        "                    # Intentar mejorar el prompt si no es el último intento\n",
        "                    if attempt < max_attempts - 1:\n",
        "                        # Extraer sugerencias de mejora\n",
        "                        hints = extract_improvement_hints(validation.get(\"raw_validation_output\", \"\"))\n",
        "                        reformulated_query = reformulate_user_query(query, hints)\n",
        "                        # Volver a obtener el prompt base original\n",
        "                        selected_prompt_template = get_prompt_by_category(category, memory_text, reformulated_query)\n",
        "\n",
        "                        # Si es un prompt estándar con {context}, entonces podemos modificarlo\n",
        "                        if isinstance(selected_prompt_template, ChatPromptTemplate):\n",
        "                            try:\n",
        "                                # Tomar solo el system prompt como texto\n",
        "                                original_system_msg = selected_prompt_template.messages[0].prompt.template\n",
        "                                new_system_msg = augment_prompt_with_validation(original_system_msg, hints)\n",
        "\n",
        "                                selected_prompt_template = ChatPromptTemplate.from_messages([\n",
        "                                    SystemMessagePromptTemplate.from_template(new_system_msg),\n",
        "                                    HumanMessagePromptTemplate.from_template(\"{question}\")\n",
        "                                ])\n",
        "\n",
        "                                logger.info(\"Prompt adaptado con sugerencias del validador.\")\n",
        "                            except Exception as e:\n",
        "                                logger.warning(f\"No se pudo adaptar el prompt con sugerencias: {e}\")\n",
        "\n",
        "                    if attempt == max_attempts - 1:\n",
        "                        response_data = {\n",
        "                            \"response\": response_text,\n",
        "                            \"docs\": retrieved_docs,\n",
        "                            \"attempt\": attempt+1,\n",
        "                            \"category\": category,\n",
        "                            \"validation\": validation\n",
        "                        }\n",
        "                    debug=1\n",
        "                    if attempt == max_attempts - 1 and not validation.get(\"valid\", False) and debug ==1:\n",
        "                        logger.info(\"Iniciando auto-refinamiento de respuesta con feedback del validador.\")\n",
        "\n",
        "                        refinement_prompt = ChatPromptTemplate.from_messages([\n",
        "                            SystemMessagePromptTemplate.from_template(\n",
        "                                refinement_module_prompt\n",
        "                            ),\n",
        "                            HumanMessagePromptTemplate.from_template(\"Corrige la respuesta anterior respetando la estructura exacta y ajustándola según los fallos detectados.\")\n",
        "                        ])\n",
        "\n",
        "                        refinement_chain = LLMChain(llm=llm, prompt=refinement_prompt)\n",
        "                        refined_result = refinement_chain.invoke({\n",
        "                            \"fallos\": validation.get(\"raw_validation_output\", \"\"),\n",
        "                            \"respuesta\": response_text\n",
        "                        })\n",
        "\n",
        "                        response_data[\"response\"] = refined_result.get(\"text\", \"\").strip()\n",
        "                        response_data[\"validation\"][\"refinado\"] = True\n",
        "\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error en intento RAG {attempt+1}: {e}\", exc_info=True)\n",
        "                if attempt == max_attempts-1:\n",
        "                    response_data = {\"response\": f\"Ocurrió un error grave al procesar tu consulta.\", \"docs\": [], \"attempt\": attempt+1, \"category\": category, \"error\": str(e)}\n",
        "\n",
        "            query = reformulated_query\n",
        "\n",
        "    # --- 5. Guardar turno en la memoria ---\n",
        "    if response_data and \"response\" in response_data:\n",
        "        memory.add_turn(query, response_data[\"response\"], llm)\n",
        "    else:\n",
        "        memory.add_turn(query, \"[Error al generar respuesta]\", llm=None)\n",
        "\n",
        "    return response_data\n",
        "\n",
        "# --- 8. Sistema de feedback y evaluación ---\n",
        "def collect_feedback(query, response):\n",
        "    \"\"\"Solicita feedback al usuario sobre la respuesta\"\"\"\n",
        "    print(\"\\n\" + \"=\"*20 + \" FEEDBACK \" + \"=\"*20)\n",
        "    print(f\"Consulta: '{query}'\")\n",
        "    print(f\"\\nRespuesta proporcionada:\\n{response}\")\n",
        "    print(\"=\"*50)\n",
        "    while True:\n",
        "        try:\n",
        "            rating = input(\"¿Qué tan útil fue esta respuesta? (1=Nada útil, 5=Muy útil, 0=Saltar): \")\n",
        "            rating = int(rating)\n",
        "            if 0 <= rating <= 5:\n",
        "                 return rating if rating > 0 else None # Devolver None si es 0\n",
        "            else:\n",
        "                 print(\"Por favor, introduce un número entre 0 y 5.\")\n",
        "        except ValueError:\n",
        "            print(\"Entrada inválida. Por favor, introduce un número.\")\n",
        "\n",
        "def extract_improvement_hints(validation_output):\n",
        "    # Extrae frases después de los puntos de fallo\n",
        "    hints = []\n",
        "    for match in re.finditer(r'\\[\\w+:\\s*No\\](.*?)\\n', validation_output, re.IGNORECASE):\n",
        "        hint = match.group(1).strip()\n",
        "        if hint:\n",
        "            hints.append(hint)\n",
        "    return hints\n",
        "\n",
        "def augment_prompt_with_validation(prompt_text, improvement_hints):\n",
        "    if not improvement_hints:\n",
        "        return prompt_text\n",
        "    hint_block = \"\\nIMPORTANTE: Al responder, asegúrate de abordar también los siguientes aspectos:\\n\"\n",
        "    for h in improvement_hints:\n",
        "        hint_block += f\"- {h}\\n\"\n",
        "    return prompt_text + hint_block\n",
        "\n",
        "def reformulate_user_query(original_query: str, improvement_hints: list[str]) -> str:\n",
        "    \"\"\"Añade instrucciones explícitas a la consulta original usando sugerencias del validador.\"\"\"\n",
        "    if not improvement_hints:\n",
        "        return original_query\n",
        "    reformulation = \"\\n\\n También responde específicamente a:\\n\"\n",
        "    for hint in improvement_hints:\n",
        "        reformulation += f\"- {hint.strip()}\\n\"\n",
        "    return original_query + reformulation\n",
        "\n",
        "def save_interaction(query, result_data, feedback=None):\n",
        "    \"\"\"Guarda la interacción completa para análisis y mejora\"\"\"\n",
        "    try:\n",
        "        interaction = {\n",
        "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            \"query\": query,\n",
        "            \"response\": result_data.get(\"response\"),\n",
        "            \"category\": result_data.get(\"category\"),\n",
        "            \"attempt\": result_data.get(\"attempt\"),\n",
        "            \"validation\": result_data.get(\"validation\"),\n",
        "            \"error\": result_data.get(\"error\"),\n",
        "            \"feedback\": feedback, # Añadir feedback del usuario\n",
        "            \"retrieved_docs\": [\n",
        "                {\n",
        "                    \"content_preview\": doc.page_content[:200] + \"...\", # Preview\n",
        "                    \"metadata\": doc.metadata,\n",
        "                    #\"score\": doc.score # Añadir si el retriever devuelve score\n",
        "                }\n",
        "                # Limitar el número de documentos guardados para no hacer el log enorme\n",
        "                for doc in result_data.get(\"docs\", [])[:5] # Guardar metadata de los primeros 5 docs usados\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        with open(\"interacciones_legales.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(json.dumps(interaction, ensure_ascii=False, default=str) + \"\\n\") # default=str para manejar tipos no serializables\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error guardando interacción: {e}\")\n",
        "\n",
        "def evaluate_response_quality(query, response, llm):\n",
        "    \"\"\"Evalúa la calidad de una respuesta utilizando el LLM\"\"\"\n",
        "    # (Se mantiene la estructura de evaluación con prompts, pero se simplifica la llamada)\n",
        "    metrics = {}\n",
        "    prompts = {\n",
        "        \"legal_accuracy\": \"\"\"\n",
        "        Evalúa la PRECISIÓN JURÍDICA de esta respuesta sobre protección de datos (España/UE):\n",
        "        Consulta: {query}\n",
        "        Respuesta: {response}\n",
        "        ¿La respuesta es correcta según RGPD/LOPDGDD? ¿Interpreta bien las normas?\n",
        "        Puntuación (0-10, solo número):\"\"\",\n",
        "        \"relevance\": \"\"\"\n",
        "        Evalúa la RELEVANCIA de la respuesta respecto a la consulta:\n",
        "        Consulta: {query}\n",
        "        Respuesta: {response}\n",
        "        ¿Responde directamente a lo preguntado? ¿Evita información superflua?\n",
        "        Puntuación (0-10, solo número):\"\"\",\n",
        "        \"completeness\": \"\"\"\n",
        "        Evalúa la COMPLETITUD de esta respuesta legal:\n",
        "        Consulta: {query}\n",
        "        Respuesta: {response}\n",
        "        ¿Cubre todos los aspectos clave? ¿Ofrece suficiente detalle/fundamento?\n",
        "        Puntuación (0-10, solo número):\"\"\"\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        for metric, prompt_template in prompts.items():\n",
        "            eval_prompt = prompt_template.format(query=query, response=response)\n",
        "            eval_resp = llm.invoke(eval_prompt)\n",
        "            score_match = re.search(r'\\b(10|[0-9])\\b', eval_resp)\n",
        "            if score_match:\n",
        "                metrics[metric] = float(score_match.group(0)) / 10.0\n",
        "            else:\n",
        "                logger.warning(f\"No se pudo extraer puntuación para métrica '{metric}' de: '{eval_resp}'\")\n",
        "                metrics[metric] = 0.5 # Default neutral\n",
        "\n",
        "        # Calcular métrica general (si todas las métricas están presentes)\n",
        "        if len(metrics) == 3:\n",
        "             metrics[\"overall\"] = sum(metrics.values()) / 3.0\n",
        "        else:\n",
        "             metrics[\"overall\"] = 0.5 # Default si faltan métricas\n",
        "\n",
        "        logger.info(f\"Evaluación de calidad automática: {metrics}\")\n",
        "        return metrics\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error evaluando calidad de respuesta: {e}\")\n",
        "        return {\"overall\": 0.5, \"error\": str(e)} # Default en caso de error\n",
        "\n",
        "def save_interaction_for_training(query, response, feedback_score, docs_retrieved):\n",
        "     \"\"\"Guarda interacción con feedback positivo para posible entrenamiento futuro.\"\"\"\n",
        "     # Solo guardar si el feedback es bueno (ej. 4 o 5)\n",
        "     if feedback_score is None or feedback_score < 4:\n",
        "         return\n",
        "\n",
        "     try:\n",
        "         training_example = {\n",
        "             \"query\": query,\n",
        "             \"positive_response\": response, # Marcado como positivo por el feedback\n",
        "             \"feedback_score\": feedback_score,\n",
        "             # Opcional: incluir contexto relevante si se quiere entrenar RAG-finetuning\n",
        "             \"relevant_context\": [doc.page_content for doc in docs_retrieved[:2]], # Ej: 2 docs más relevantes\n",
        "             \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "         }\n",
        "\n",
        "         with open(\"training_data_positive.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
        "             f.write(json.dumps(training_example, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "         logger.info(f\"Guardada interacción con feedback {feedback_score} para posible entrenamiento.\")\n",
        "\n",
        "     except Exception as e:\n",
        "         logger.error(f\"Error guardando interacción para entrenamiento: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "def normalize_text(text):\n",
        "    \"\"\"Normaliza el texto: elimina tildes, minúsculas, quita signos de puntuación.\"\"\"\n",
        "    text = unicodedata.normalize('NFD', text)\n",
        "    text = ''.join(c for c in text if unicodedata.category(c) != 'Mn')  # quitar tildes\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # elimina puntuación\n",
        "    return text\n",
        "\n",
        "def is_action_request(query):\n",
        "    \"\"\"Detecta si la pregunta sugiere solicitud de acciones, derechos o pasos a seguir.\"\"\"\n",
        "    normalized_query = normalize_text(query)\n",
        "\n",
        "    action_phrases = [\n",
        "        \"que pueden hacer\",\n",
        "        \"que medidas pueden tomar\",\n",
        "        \"que derechos tienen\",\n",
        "        \"como pueden reclamar\",\n",
        "        \"como pueden actuar\",\n",
        "        \"como reclamar\",\n",
        "        \"como actuar\",\n",
        "        \"que opciones tienen\",\n",
        "        \"que pasos pueden seguir\",\n",
        "        \"que pueden solicitar\",\n",
        "        \"que acciones pueden emprender\",\n",
        "        \"como defenderse\",\n",
        "        \"como denunciar\",\n",
        "        \"como protegerse\",\n",
        "        \"como impugnar\",\n",
        "        \"como negarse\",\n",
        "        \"que recurso tienen\",\n",
        "        \"que alternativas tienen\",\n",
        "        \"que pueden exigir\",\n",
        "        \"que sanciones puede haber\",\n",
        "        \"que consecuencias hay\",\n",
        "    ]\n",
        "\n",
        "    return any(phrase in normalized_query for phrase in action_phrases)\n",
        "\n",
        "\n",
        "# --- Bloque Principal de Interacción ---\n",
        "intent_classification_cache = {}\n",
        "last_user_query = None #Para preguntas continuistas\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\": # Poner el código de ejecución aquí para evitar que se ejecute al importar\n",
        "\n",
        "    # Verificar que las variables necesarias existen\n",
        "    if 'llm' not in locals() or 'llm_evaluador' not in locals() or 'base_retriever' not in locals() or 'persisted_vectorstore' not in locals():\n",
        "         print(\"ERROR: Asegúrate de haber ejecutado las celdas anteriores para inicializar llm, llm_evaluador, base_retriever y persisted_vectorstore.\")\n",
        "    else:\n",
        "         print(\"Modelo LLM y retriever listos.\")\n",
        "\n",
        "         # Inicializar memoria conversacional\n",
        "         conversation_memory = ImprovedMemory(max_turns=4) # Guardar 4 turnos\n",
        "         last_query=\"\"\n",
        "         print(\"\\nBienvenido al Asistente Legal de Protección de Datos.\")\n",
        "         print(\"Escribe 'salir' para terminar.\")\n",
        "\n",
        "         while True:\n",
        "             user_query = input(\"\\nTu consulta: \")\n",
        "             if user_query.lower() == 'salir':\n",
        "                 break\n",
        "             if not user_query:\n",
        "                 continue\n",
        "\n",
        "             start_time = time.time()\n",
        "\n",
        "             # --- Llamada principal al sistema RAG con reintentos ---\n",
        "             result = get_response_with_retry(\n",
        "                 query=user_query,\n",
        "                 llm=llm,\n",
        "                 base_retriever=base_retriever,\n",
        "                 memory=conversation_memory, # Pasar la instancia de memoria\n",
        "                 last_query=last_query\n",
        "             )\n",
        "             last_query = user_query\n",
        "             end_time = time.time()\n",
        "\n",
        "             # --- Mostrar Resultados ---\n",
        "             print(\"\\n--- Respuesta del Asistente ---\")\n",
        "             print(result.get(\"response\", \"No se pudo generar respuesta.\"))\n",
        "             print(\"-\" * 30)\n",
        "             logger.info(f\"Respuesta generada en {end_time - start_time:.2f} segundos.\")\n",
        "             logger.info(f\"Categoría: {result.get('category', 'N/A')}, Intentos: {result.get('attempt', 'N/A')}\")\n",
        "             if result.get(\"validation\"):\n",
        "                 logger.info(f\"Validación: {result['validation']}\")\n",
        "             if result.get(\"error\"):\n",
        "                 logger.error(f\"Error reportado: {result['error']}\")\n",
        "\n",
        "             # Mostrar documentos fuente (opcional, para depuración)\n",
        "             if result.get(\"docs\"):\n",
        "                 print(f\"\\nFuentes consultadas ({len(result['docs'])} documentos):\")\n",
        "                 for i, doc in enumerate(result[\"docs\"]):\n",
        "                     source = doc.metadata.get('source', 'Desconocido')\n",
        "                     page = doc.metadata.get('page', '?')\n",
        "                     tipo = doc.metadata.get('tipo', '')\n",
        "                     print(f\"  [{i+1}] {source} (Pág: {page}, Tipo: {tipo})\") # Preview más corto\n",
        "\n",
        "             # --- Feedback y Evaluación (Opcional) ---\n",
        "             user_feedback = collect_feedback(user_query, result.get(\"response\"))\n",
        "             save_interaction(user_query, result, user_feedback)\n",
        "\n",
        "             if user_feedback:\n",
        "                 # Si hubo feedback positivo, guardar para posible entrenamiento\n",
        "                 save_interaction_for_training(user_query, result.get(\"response\"), user_feedback, result.get(\"docs\", []))\n",
        "\n",
        "             # Evaluar calidad automáticamente (opcional, consume tokens)\n",
        "             # quality_metrics = evaluate_response_quality(user_query, result.get(\"response\"), llm)\n",
        "             # logger.info(f\"Métricas de calidad automáticas: {quality_metrics}\")\n",
        "\n",
        "         print(\"\\n¡Hasta luego!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoxVqhOEWj5b"
      },
      "source": [
        "# Comparaciones de modelos\n",
        "\n",
        "A continuación esta el codigo para comparar las respuestas entre modelos\n",
        "\n",
        "Hay que resetear ollama entre consultas para evitar que use pueda recordar datos de la conversación entre pruebas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ior64B0AfA6Y"
      },
      "outputs": [],
      "source": [
        "#Aqui pegamos nuestra pregunta\n",
        "pregunta=\"¿Cuáles son los principios fundamentales que deben cumplirse para garantizar el tratamiento adecuado de datos personales según la normativa de protección de datos en España?\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rWBx9qAlajxd",
        "outputId": "53bb402a-6074-4e5b-baf7-4f6347b4eb6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-py3-none-any.whl size=7392 sha256=f57f9944ddce0f3b97c0d3ebf03273d6ba420fdad3938f992506e60bfb9d403b\n",
            "  Stored in directory: /root/.cache/pip/wheels/2b/4d/8f/55fb4f7b9b591891e8d3f72977c4ec6c7763b39c19f0861595\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (5.9.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install gputil\n",
        "!pip install psutil"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRlKHsidWshH"
      },
      "source": [
        "## Modelo Base"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3z-7dmDgZEpe"
      },
      "source": [
        "Descargamos el modelo base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ygs3fkqTY_1Y",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "!ollama run llama3:8b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRA0PR0yWjIu",
        "outputId": "556423d4-a229-46bc-d82d-b8800818a3c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- RESPUESTA ---\n",
            "Según la normativa de protección de datos personales en España, específicamente la Ley Orgánica 3/2018, de 5 de diciembre, sobre Protección de Datos Personales y garantía de los derechos digitales (LOPDGDD) y el Reglamento (UE) 2016/679 del Parlamento Europeo y del Consejo, de 27 de abril de 2016, relativo a la protección de las personas físicas con respecto al tratamiento de datos personales y al libre movimiento de dichos datos, y por la que se sustituye el Reglamento (UE) nº 95/46/CE (RGPD), los principios fundamentales para garantizar el tratamiento adecuado de datos personales son:\n",
            "\n",
            "1. **Legítimo**: El tratamiento debe ser lícito y basado en una base legítima, como la consignación del consentimiento explícito, la ejecución de un contrato o la satisfacción de un interés legítimo.\n",
            "2. **Especial**: Los datos personales deben tratarse de manera estricta y limitada a lo necesario para el propósito especifico, evitando cualquier tratamiento no autorizado o excesivo.\n",
            "3. **Transparencia**: El responsable del tratamiento debe informar claramente sobre la recopilación y el uso de los datos personales, incluyendo la identificación del responsable, la finalidad del tratamiento y las garantías de seguridad implementadas.\n",
            "4. **Limitación del acceso**: Solo aquellos que necesiten acceder a los datos personales para fines legítimos deben tener acceso a ellos.\n",
            "5. **Accesibilidad**: Los titulares de los datos personales deben tener acceso a sus datos y poder ejercer sus derechos, como el derecho a la rectificación o eliminación.\n",
            "6. **Integridad y seguridad**: El responsable del tratamiento debe implementar medidas para garantizar la integridad y seguridad de los datos personales, evitando cualquier tipo de daño, pérdida, modificación o acceso no autorizado.\n",
            "7. **No discriminación**: El tratamiento de datos personales no puede ser discriminatorio y debe respetar las igualdades y diferencias entre las personas.\n",
            "8. **Responsabilidad**: Los responsables del tratamiento deben ser conscientes de su responsabilidad en el tratamiento de datos personales y tomar medidas para garantizar que se cumplan los principios fundamentales.\n",
            "\n",
            "Al cumplir con estos principios, se puede garantizar un tratamiento adecuado y seguro de los datos personales en España.\n",
            "\n",
            "--- MÉTRICAS ---\n",
            "Modelo: llama3:8b\n",
            "Tokens generados: 335\n",
            "Tiempo total (incluye carga): 19.90 s\n",
            "Tiempo de inferencia (solo generación): 19.77 s\n",
            "Velocidad de generación: 16.94 tokens/seg\n",
            "RAM usada: 0.44 GB\n",
            "Uso CPU promedio durante inferencia: 64.5%\n",
            "VRAM usada: 5.73 GB\n",
            "Carga GPU: 92.0%\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import psutil\n",
        "import GPUtil\n",
        "from statistics import mean\n",
        "import threading\n",
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# --- Reiniciar Ollama ---\n",
        "!kill -9 $(ps aux | grep '[o]llama' | awk '{print $2}')\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "time.sleep(3)  # Esperar a que Ollama esté listo\n",
        "\n",
        "# --- Configuración de prueba ---\n",
        "model_name = \"llama3:8b\"\n",
        "\n",
        "# --- Monitorear uso de RAM ---\n",
        "def get_system_metrics():\n",
        "    ram_used = psutil.virtual_memory().used / (1024 ** 3)  # GB\n",
        "    return ram_used\n",
        "\n",
        "# --- Monitorear uso de GPU (VRAM) ---\n",
        "def get_gpu_metrics():\n",
        "    gpus = GPUtil.getGPUs()\n",
        "    if gpus:\n",
        "        gpu = gpus[0]\n",
        "        return gpu.memoryUsed / 1024, gpu.load * 100  # GB, %\n",
        "    return 0, 0\n",
        "\n",
        "# --- Iniciar modelo ---\n",
        "llm = OllamaLLM(model=model_name, max_tokens=250)\n",
        "\n",
        "# --- Medición de rendimiento ---\n",
        "start_total = time.time()\n",
        "\n",
        "ram_before = get_system_metrics()\n",
        "gpu_mem_before, gpu_load_before = get_gpu_metrics()\n",
        "\n",
        "# --- Medición activa de CPU en paralelo ---\n",
        "cpu_usage_during_infer = []\n",
        "stop_cpu_monitor = False\n",
        "\n",
        "def monitor_cpu():\n",
        "    while not stop_cpu_monitor:\n",
        "        cpu = psutil.cpu_percent(interval=0.1)\n",
        "        cpu_usage_during_infer.append(cpu)\n",
        "\n",
        "cpu_thread = threading.Thread(target=monitor_cpu)\n",
        "cpu_thread.start()\n",
        "\n",
        "# --- Inferencia ---\n",
        "start_infer = time.time()\n",
        "response = llm.invoke(pregunta)\n",
        "end_infer = time.time()\n",
        "\n",
        "# --- Detener CPU monitor ---\n",
        "stop_cpu_monitor = True\n",
        "cpu_thread.join()\n",
        "\n",
        "ram_after = get_system_metrics()\n",
        "gpu_mem_after, gpu_load_after = get_gpu_metrics()\n",
        "end_total = time.time()\n",
        "\n",
        "# --- Métricas calculadas ---\n",
        "total_time = end_total - start_total\n",
        "infer_time = end_infer - start_infer\n",
        "token_count = len(response.split())\n",
        "avg_cpu = mean(cpu_usage_during_infer) if cpu_usage_during_infer else 0\n",
        "\n",
        "# --- Salida ---\n",
        "print(\"\\n--- RESPUESTA ---\")\n",
        "print(response)\n",
        "\n",
        "print(\"\\n--- MÉTRICAS ---\")\n",
        "print(f\"Modelo: {model_name}\")\n",
        "print(f\"Tokens generados: {token_count}\")\n",
        "print(f\"Tiempo total (incluye carga): {total_time:.2f} s\")\n",
        "print(f\"Tiempo de inferencia (solo generación): {infer_time:.2f} s\")\n",
        "print(f\"Velocidad de generación: {token_count / infer_time:.2f} tokens/seg\")\n",
        "print(f\"RAM usada: {ram_after - ram_before:.2f} GB\")\n",
        "print(f\"Uso CPU promedio durante inferencia: {avg_cpu:.1f}%\")\n",
        "print(f\"VRAM usada: {gpu_mem_after - gpu_mem_before:.2f} GB\")\n",
        "print(f\"Carga GPU: {gpu_load_after:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoxPozkRXX33"
      },
      "source": [
        "## Modelo Base + RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ix4heZm2XaKT",
        "outputId": "889b5bc7-5a56-4794-8f68-6f7073f1ef3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "Asistente legal de protección de datos listo (usando load_qa_chain). Escribe 'Exit' para salir.\n",
            "--------------------------------------------------\n",
            "\n",
            "Respuesta:\n",
            "En primer lugar, es importante destacar que la publicación de las notas personales incluyendo el nombre completo y DNI del compañero sin su consentimiento puede considerarse un delito contra la protección de datos personales.\n",
            "\n",
            "Según el Artículo 4 del Reglamento General de Protección de Datos (RGPD), los datos personales deben tratarse de manera lícita, transparente y justificada. La publicación de las notas personales sin el consentimiento del titular puede considerarse un tratamiento ilícito de sus datos.\n",
            "\n",
            "Además, la publicación de información personal como el DNI puede considerarse una infracción del Artículo 5 del RGPD, que establece que los datos personales deben ser procesados de manera tal que se garantice su integridad y confidencialidad.\n",
            "\n",
            "En cuanto a la identificación de Sinosuque, aunque no hay suficiente información para determinar si es un responsable del tratamiento de datos (artículo 4.7 del RGPD), es importante destacar que la publicación de las notas personales puede considerarse un delito contra la protección de datos personales.\n",
            "\n",
            "En España, la infracción de la protección de datos personales puede ser punible según el Artículo 20 de la Ley Orgánica 15/1999, de 13 de diciembre, sobre Protección de Datos de Carácter Personal. Entre los delitos que se pueden cometer en este sentido se encuentran:\n",
            "\n",
            "* La publicación no autorizada de datos personales (Artículo 21 de la LO 15/1999)\n",
            "* El tratamiento ilícito de datos personales (Artículo 22 de la LO 15/1999)\n",
            "\n",
            "En conclusión, Sinosuque podría enfrentarse a delitos como la publicación no autorizada de datos personales y el tratamiento ilícito de datos personales, según la Ley Orgánica 15/1999, de 13 de diciembre, sobre Protección de Datos de Carácter Personal.\n",
            "\n",
            "Saliendo del asistente.\n"
          ]
        }
      ],
      "source": [
        "#Para resetear ollama\n",
        "!kill -9 $(ps aux | grep '[o]llama' | awk '{print $2}')\n",
        "\n",
        "import subprocess\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3) # Esperar a que Ollama se cargue\n",
        "\n",
        "\n",
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# Initialize the LLaMA model\n",
        "llm = OllamaLLM(model=\"llama3:8b\") #aqui poinemos el modelo base\n",
        "\n",
        "\n",
        "\n",
        "# Importación correcta para versiones recientes de Langchain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "import torch\n",
        "\n",
        "\n",
        "# Define la plantilla del prompt DETALLADA\n",
        "prompt_template = \"\"\"\n",
        "Eres un asistente legal experto en protección de datos en España.\n",
        "Tu tarea es analizar la legalidad de la situación descrita en la 'Pregunta' basándote **principalmente** en los siguientes 'Textos Legales Recuperados'.\n",
        "Si los textos recuperados contienen suficiente información, responde únicamente basándote en ellos.\n",
        "Si los textos no contienen una respuesta clara pero la pregunta se refiere a conceptos básicos del RGPD, usa lo aprendido en el entrenamiento para dar una respuesta general, especificando que no proviene de los textos recuperados.\n",
        "Si la pregunta requiere información específica que no está en los textos recuperados ni en tu entrenamiento, indícalo claramente.\n",
        "\n",
        "Textos Legales Recuperados:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "Análisis Legal y Conclusión:\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Crea la cadena usando load_qa_chain con el prompt personalizado\n",
        "# chain_type=\"stuff\" es adecuado si los chunks recuperados + pregunta caben en la ventana de contexto del LLM.\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=PROMPT)\n",
        "\n",
        "print (\"-\"*50)\n",
        "print (\"Asistente legal de protección de datos listo (usando load_qa_chain). Escribe 'Exit' para salir.\")\n",
        "print (\"-\"*50)\n",
        "\n",
        "\n",
        "query=pregunta\n",
        "# 1. Recupera los documentos relevantes\n",
        "# Se puede ajustar 'k' aquí si quieres más/menos contexto: retriever.search_kwargs = {'k': 5}\n",
        "docs_retrieved = retriever.invoke(query)\n",
        "\n",
        "# Prepara el input para la cadena\n",
        "input_data = {\n",
        "    \"input_documents\": docs_retrieved,\n",
        "    \"question\": query\n",
        "}\n",
        "\n",
        "# 2. Ejecuta la cadena de generación con los documentos recuperados y la pregunta\n",
        "# Si usas una versión de Langchain > 0.1.0, invoke devuelve un diccionario.\n",
        "# Si usas una versión anterior, podría devolver directamente el string.\n",
        "# El parámetro return_only_outputs=True ya no es necesario/válido en invoke para versiones > 0.1.0\n",
        "result = chain.invoke(input_data)\n",
        "\n",
        "# Imprime la salida del LLM (la clave suele ser 'output_text' en versiones recientes)\n",
        "if isinstance(result, dict) and 'output_text' in result:\n",
        "    print(\"\\nRespuesta:\")\n",
        "    print(result['output_text'])\n",
        "elif isinstance(result, str): # Compatibilidad con versiones anteriores\n",
        "      print(\"\\nRespuesta:\")\n",
        "      print(result)\n",
        "else:\n",
        "      print(\"\\nRespuesta recibida (formato inesperado):\")\n",
        "      print(result)\n",
        "\n",
        "\n",
        "print(\"\\nSaliendo del asistente.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8VmL4S2W7Zf"
      },
      "source": [
        "## Finetunning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkurbP2nW_9W"
      },
      "source": [
        "### 8bit Q8_0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWCIODDTLDQ3",
        "outputId": "e6fd21ed-51ef-4a79-b250-fb72dd4ed9ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G⠹ \u001b[K\u001b[?25h\u001b[?2026l^C\n"
          ]
        }
      ],
      "source": [
        "!ollama run hf.co/serdom02/Leyeneitor_8bitQ8_0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sI2VEYV6XWmD",
        "outputId": "4d21e444-9ddd-470f-ce24-6a6628b220d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- RESPUESTA ---\n",
            "Para garantizar el tratamiento adecuado de datos personales en España, se deben cumple los siguientes principios fundamentales:\n",
            "\n",
            "1. **Finalidad**: Los datos solo pueden tratarse para las finalidades expresadas al recopilarlos.\n",
            "2. **Transparencia**: El tratamiento debe ser transparente, informando claramente a los afectados sobre quién es el responsable del tratamiento y con qué finalidad se usarán sus datos.\n",
            "3. **Principio de minimización**: Solo deben recogerse los datos personales estrictamente necesarios para la finalidad declarada.\n",
            "4. **Limitación del plazo de conservación**: Los datos solo pueden conservarse durante el tiempo necesario para cumplir con la finalidad de su tratamiento.\n",
            "5. **Confidencialidad**: Los datos deben tratarse en condiciones de confidencialidad, evitando su acceso a terceros no autorizados.\n",
            "6. **Principio de libertad**: Los derechos y libertades del afectado deben ser respetados, sin que se imponga un tratamiento desproporcionado.\n",
            "7. **Principio de proporcionalidad**: El tratamiento debe ser el mínimo necesario para la finalidad perseguida.\n",
            "\n",
            " Además, los datos deben ser:\n",
            "\n",
            "8. **Accesibilidad**: Los afectados deben poder acceder a sus datos personales y solicitar su rectificación o supresión.\n",
            "9. **Limitación del plazo de conservación**: Los datos solo pueden conservarse durante el tiempo necesario para cumplir con la finalidad de su tratamiento.\n",
            "10. **Sustitución de derechos**: No se puede condicionar el ejercicio de derecho fundamental a la cesión de datos a terceros.\n",
            "\n",
            "La normativa española exige que estos principios se apliquen de forma exhaustiva en el tratamiento de datos personales, protegiendo los derechos fundamentales de las personas cuyos datos se recogen.\n",
            "\n",
            "--- MÉTRICAS ---\n",
            "Modelo: hf.co/serdom02/Leyeneitor_8bitQ8_0\n",
            "Tokens generados: 242\n",
            "Tiempo total (incluye carga): 20.91 s\n",
            "Tiempo de inferencia (solo generación): 20.79 s\n",
            "Velocidad de generación: 11.64 tokens/seg\n",
            "RAM usada: 0.42 GB\n",
            "Uso CPU promedio durante inferencia: 62.5%\n",
            "VRAM usada: 9.10 GB\n",
            "Carga GPU: 90.0%\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import psutil\n",
        "import GPUtil\n",
        "from statistics import mean\n",
        "import threading\n",
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# --- Reiniciar Ollama ---\n",
        "!kill -9 $(ps aux | grep '[o]llama' | awk '{print $2}')\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "time.sleep(3)  # Esperar a que Ollama esté listo\n",
        "\n",
        "# --- Configuración de prueba ---\n",
        "model_name = \"hf.co/serdom02/Leyeneitor_8bitQ8_0\"\n",
        "\n",
        "# --- Monitorear uso de RAM ---\n",
        "def get_system_metrics():\n",
        "    ram_used = psutil.virtual_memory().used / (1024 ** 3)  # GB\n",
        "    return ram_used\n",
        "\n",
        "# --- Monitorear uso de GPU (VRAM) ---\n",
        "def get_gpu_metrics():\n",
        "    gpus = GPUtil.getGPUs()\n",
        "    if gpus:\n",
        "        gpu = gpus[0]\n",
        "        return gpu.memoryUsed / 1024, gpu.load * 100  # GB, %\n",
        "    return 0, 0\n",
        "\n",
        "# --- Iniciar modelo ---\n",
        "llm = OllamaLLM(model=model_name, max_tokens=250)\n",
        "\n",
        "# --- Medición de rendimiento ---\n",
        "start_total = time.time()\n",
        "\n",
        "ram_before = get_system_metrics()\n",
        "gpu_mem_before, gpu_load_before = get_gpu_metrics()\n",
        "\n",
        "# --- Medición activa de CPU en paralelo ---\n",
        "cpu_usage_during_infer = []\n",
        "stop_cpu_monitor = False\n",
        "\n",
        "def monitor_cpu():\n",
        "    while not stop_cpu_monitor:\n",
        "        cpu = psutil.cpu_percent(interval=0.1)\n",
        "        cpu_usage_during_infer.append(cpu)\n",
        "\n",
        "cpu_thread = threading.Thread(target=monitor_cpu)\n",
        "cpu_thread.start()\n",
        "\n",
        "# --- Inferencia ---\n",
        "start_infer = time.time()\n",
        "response = llm.invoke(pregunta)\n",
        "end_infer = time.time()\n",
        "\n",
        "# --- Detener CPU monitor ---\n",
        "stop_cpu_monitor = True\n",
        "cpu_thread.join()\n",
        "\n",
        "ram_after = get_system_metrics()\n",
        "gpu_mem_after, gpu_load_after = get_gpu_metrics()\n",
        "end_total = time.time()\n",
        "\n",
        "# --- Métricas calculadas ---\n",
        "total_time = end_total - start_total\n",
        "infer_time = end_infer - start_infer\n",
        "token_count = len(response.split())\n",
        "avg_cpu = mean(cpu_usage_during_infer) if cpu_usage_during_infer else 0\n",
        "\n",
        "# --- Salida ---\n",
        "print(\"\\n--- RESPUESTA ---\")\n",
        "print(response)\n",
        "\n",
        "print(\"\\n--- MÉTRICAS ---\")\n",
        "print(f\"Modelo: {model_name}\")\n",
        "print(f\"Tokens generados: {token_count}\")\n",
        "print(f\"Tiempo total (incluye carga): {total_time:.2f} s\")\n",
        "print(f\"Tiempo de inferencia (solo generación): {infer_time:.2f} s\")\n",
        "print(f\"Velocidad de generación: {token_count / infer_time:.2f} tokens/seg\")\n",
        "print(f\"RAM usada: {ram_after - ram_before:.2f} GB\")\n",
        "print(f\"Uso CPU promedio durante inferencia: {avg_cpu:.1f}%\")\n",
        "print(f\"VRAM usada: {gpu_mem_after - gpu_mem_before:.2f} GB\")\n",
        "print(f\"Carga GPU: {gpu_load_after:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBPrNkBgXSRg"
      },
      "source": [
        "### 16bit GGUF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMDE2J2MLN42"
      },
      "outputs": [],
      "source": [
        "!ollama run hf.co/serdom02/model_16bitGGUF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NTWJXFTKXXHU",
        "outputId": "d3f4da4c-040e-4285-872d-bcfe9d89dde4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- RESPUESTA ---\n",
            "Los principios fundamentales para garantizar el tratamiento adecuado de datos personales según la normativa española son: legalidad, equilibrio, minimización (mínimo necesario), transparencia, limitación del plazo de conservación, confidencialidad, finalidad específica, y posibilidad de ejercicio de los derechos del afectado.\n",
            "\n",
            "--- MÉTRICAS ---\n",
            "Modelo: hf.co/serdom02/model_16bitGGUF\n",
            "Tokens generados: 39\n",
            "Tiempo total (incluye carga): 102.15 s\n",
            "Tiempo de inferencia (solo generación): 102.09 s\n",
            "Velocidad de generación: 0.38 tokens/seg\n",
            "RAM usada: 0.52 GB\n",
            "Uso CPU promedio durante inferencia: 34.5%\n",
            "VRAM usada: 14.43 GB\n",
            "Carga GPU: 30.0%\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import psutil\n",
        "import GPUtil\n",
        "from statistics import mean\n",
        "import threading\n",
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# --- Reiniciar Ollama ---\n",
        "!kill -9 $(ps aux | grep '[o]llama' | awk '{print $2}')\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "time.sleep(3)  # Esperar a que Ollama esté listo\n",
        "\n",
        "# --- Configuración de prueba ---\n",
        "model_name = \"hf.co/serdom02/model_16bitGGUF\"\n",
        "\n",
        "# --- Monitorear uso de RAM ---\n",
        "def get_system_metrics():\n",
        "    ram_used = psutil.virtual_memory().used / (1024 ** 3)  # GB\n",
        "    return ram_used\n",
        "\n",
        "# --- Monitorear uso de GPU (VRAM) ---\n",
        "def get_gpu_metrics():\n",
        "    gpus = GPUtil.getGPUs()\n",
        "    if gpus:\n",
        "        gpu = gpus[0]\n",
        "        return gpu.memoryUsed / 1024, gpu.load * 100  # GB, %\n",
        "    return 0, 0\n",
        "\n",
        "# --- Iniciar modelo ---\n",
        "llm = OllamaLLM(model=model_name, max_tokens=250)\n",
        "\n",
        "# --- Medición de rendimiento ---\n",
        "start_total = time.time()\n",
        "\n",
        "ram_before = get_system_metrics()\n",
        "gpu_mem_before, gpu_load_before = get_gpu_metrics()\n",
        "\n",
        "# --- Medición activa de CPU en paralelo ---\n",
        "cpu_usage_during_infer = []\n",
        "stop_cpu_monitor = False\n",
        "\n",
        "def monitor_cpu():\n",
        "    while not stop_cpu_monitor:\n",
        "        cpu = psutil.cpu_percent(interval=0.1)\n",
        "        cpu_usage_during_infer.append(cpu)\n",
        "\n",
        "cpu_thread = threading.Thread(target=monitor_cpu)\n",
        "cpu_thread.start()\n",
        "\n",
        "# --- Inferencia ---\n",
        "start_infer = time.time()\n",
        "response = llm.invoke(pregunta)\n",
        "end_infer = time.time()\n",
        "\n",
        "# --- Detener CPU monitor ---\n",
        "stop_cpu_monitor = True\n",
        "cpu_thread.join()\n",
        "\n",
        "ram_after = get_system_metrics()\n",
        "gpu_mem_after, gpu_load_after = get_gpu_metrics()\n",
        "end_total = time.time()\n",
        "\n",
        "# --- Métricas calculadas ---\n",
        "total_time = end_total - start_total\n",
        "infer_time = end_infer - start_infer\n",
        "token_count = len(response.split())\n",
        "avg_cpu = mean(cpu_usage_during_infer) if cpu_usage_during_infer else 0\n",
        "\n",
        "# --- Salida ---\n",
        "print(\"\\n--- RESPUESTA ---\")\n",
        "print(response)\n",
        "\n",
        "print(\"\\n--- MÉTRICAS ---\")\n",
        "print(f\"Modelo: {model_name}\")\n",
        "print(f\"Tokens generados: {token_count}\")\n",
        "print(f\"Tiempo total (incluye carga): {total_time:.2f} s\")\n",
        "print(f\"Tiempo de inferencia (solo generación): {infer_time:.2f} s\")\n",
        "print(f\"Velocidad de generación: {token_count / infer_time:.2f} tokens/seg\")\n",
        "print(f\"RAM usada: {ram_after - ram_before:.2f} GB\")\n",
        "print(f\"Uso CPU promedio durante inferencia: {avg_cpu:.1f}%\")\n",
        "print(f\"VRAM usada: {gpu_mem_after - gpu_mem_before:.2f} GB\")\n",
        "print(f\"Carga GPU: {gpu_load_after:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8SWdO38XT5k"
      },
      "source": [
        "### q4_k_m GGUF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8d4j3eDmLRzH"
      },
      "outputs": [],
      "source": [
        "!ollama run hf.co/serdom02/model_q4_k_mGGUF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8VuZ5VWXXi1",
        "outputId": "6dc9c016-4c08-4ab7-b944-fee11d00d368"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- RESPUESTA ---\n",
            "Según la normativa española, los principios fundamentales para el tratamiento de datos personales son:\n",
            "\n",
            "1. **Legalidad**: El tratamiento debe ser lícito y debidamente autorizado por una ley o regulación.\n",
            "2. **Eficacia**: El tratamiento debe ser preciso y completo, evitando la recopilación de datos innecesarios.\n",
            "3. **Finalidad**: El tratamiento debe tener una finalidad clara y legítima, informada al titular de los datos.\n",
            "4. **Transparencia**: Debe existir una información clara y accesible sobre el tratamiento y sus finalidades.\n",
            "5. **Limitación del plazo de conservación**: Los datos solo deben conservarse durante el tiempo necesario para cumplir con la finalidad que justificó su recopilación.\n",
            "6. **Integridad y confidencialidad**: El tratamiento debe garantizar la integridad y confidencialidad de los datos, evitando accesos no autorizados.\n",
            "7. **Minimalización**: Solo se deben recopilar los datos necesarios para cumplir con la finalidad del tratamiento.\n",
            "\n",
            "Además, en algunos casos específicos, también es aplicable el principio de:\n",
            "\n",
            "8. **No discriminación**: No se pueden tratar datos personales de manera que afecte a derechos fundamentales como la igualdad o la no discriminación.\n",
            "\n",
            "--- MÉTRICAS ---\n",
            "Modelo: hf.co/serdom02/model_q4_k_mGGUF\n",
            "Tokens generados: 169\n",
            "Tiempo total (incluye carga): 14.33 s\n",
            "Tiempo de inferencia (solo generación): 14.23 s\n",
            "Velocidad de generación: 11.88 tokens/seg\n",
            "RAM usada: 0.45 GB\n",
            "Uso CPU promedio durante inferencia: 66.4%\n",
            "VRAM usada: 5.97 GB\n",
            "Carga GPU: 52.0%\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import time\n",
        "import psutil\n",
        "import GPUtil\n",
        "from statistics import mean\n",
        "import threading\n",
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# --- Reiniciar Ollama ---\n",
        "!kill -9 $(ps aux | grep '[o]llama' | awk '{print $2}')\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "time.sleep(3)  # Esperar a que Ollama esté listo\n",
        "\n",
        "# --- Configuración de prueba ---\n",
        "model_name = \"hf.co/serdom02/model_q4_k_mGGUF\"\n",
        "\n",
        "# --- Monitorear uso de RAM ---\n",
        "def get_system_metrics():\n",
        "    ram_used = psutil.virtual_memory().used / (1024 ** 3)  # GB\n",
        "    return ram_used\n",
        "\n",
        "# --- Monitorear uso de GPU (VRAM) ---\n",
        "def get_gpu_metrics():\n",
        "    gpus = GPUtil.getGPUs()\n",
        "    if gpus:\n",
        "        gpu = gpus[0]\n",
        "        return gpu.memoryUsed / 1024, gpu.load * 100  # GB, %\n",
        "    return 0, 0\n",
        "\n",
        "# --- Iniciar modelo ---\n",
        "llm = OllamaLLM(model=model_name, max_tokens=250)\n",
        "\n",
        "# --- Medición de rendimiento ---\n",
        "start_total = time.time()\n",
        "\n",
        "ram_before = get_system_metrics()\n",
        "gpu_mem_before, gpu_load_before = get_gpu_metrics()\n",
        "\n",
        "# --- Medición activa de CPU en paralelo ---\n",
        "cpu_usage_during_infer = []\n",
        "stop_cpu_monitor = False\n",
        "\n",
        "def monitor_cpu():\n",
        "    while not stop_cpu_monitor:\n",
        "        cpu = psutil.cpu_percent(interval=0.1)\n",
        "        cpu_usage_during_infer.append(cpu)\n",
        "\n",
        "cpu_thread = threading.Thread(target=monitor_cpu)\n",
        "cpu_thread.start()\n",
        "\n",
        "# --- Inferencia ---\n",
        "start_infer = time.time()\n",
        "response = llm.invoke(pregunta)\n",
        "end_infer = time.time()\n",
        "\n",
        "# --- Detener CPU monitor ---\n",
        "stop_cpu_monitor = True\n",
        "cpu_thread.join()\n",
        "\n",
        "ram_after = get_system_metrics()\n",
        "gpu_mem_after, gpu_load_after = get_gpu_metrics()\n",
        "end_total = time.time()\n",
        "\n",
        "# --- Métricas calculadas ---\n",
        "total_time = end_total - start_total\n",
        "infer_time = end_infer - start_infer\n",
        "token_count = len(response.split())\n",
        "avg_cpu = mean(cpu_usage_during_infer) if cpu_usage_during_infer else 0\n",
        "\n",
        "# --- Salida ---\n",
        "print(\"\\n--- RESPUESTA ---\")\n",
        "print(response)\n",
        "\n",
        "print(\"\\n--- MÉTRICAS ---\")\n",
        "print(f\"Modelo: {model_name}\")\n",
        "print(f\"Tokens generados: {token_count}\")\n",
        "print(f\"Tiempo total (incluye carga): {total_time:.2f} s\")\n",
        "print(f\"Tiempo de inferencia (solo generación): {infer_time:.2f} s\")\n",
        "print(f\"Velocidad de generación: {token_count / infer_time:.2f} tokens/seg\")\n",
        "print(f\"RAM usada: {ram_after - ram_before:.2f} GB\")\n",
        "print(f\"Uso CPU promedio durante inferencia: {avg_cpu:.1f}%\")\n",
        "print(f\"VRAM usada: {gpu_mem_after - gpu_mem_before:.2f} GB\")\n",
        "print(f\"Carga GPU: {gpu_load_after:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWMJxQ09X0f2"
      },
      "source": [
        "## Finetunning + RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFcUQV9HX2sX"
      },
      "source": [
        "### 8bit Q8_0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sA2brIQWYDVE",
        "outputId": "e08ec015-5955-454d-ba1a-b88bdb1ab1e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "Asistente legal de protección de datos listo (usando load_qa_chain). Escribe 'Exit' para salir.\n",
            "--------------------------------------------------\n",
            "\n",
            "Respuesta:\n",
            "Sinosuque podría enfrentarse a multas económicas por vulneración de la protección de datos, ya que publicar información personal como el DNI sin consentimiento del afectado constituye una infracción grave según el RGPD. Además, el insulto en línea puede ser considerado difamación si afecta a la reputación de las personas agraviadas.\n",
            "\n",
            "Saliendo del asistente.\n"
          ]
        }
      ],
      "source": [
        "#Para resetear ollama\n",
        "!kill -9 $(ps aux | grep '[o]llama' | awk '{print $2}')\n",
        "\n",
        "import subprocess\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3) # Esperar a que Ollama se cargue\n",
        "\n",
        "\n",
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# Initialize the LLaMA model\n",
        "llm = OllamaLLM(model=\"hf.co/serdom02/model_8bitQ8_0\") #aqui poinemos el modelo base\n",
        "\n",
        "# Importación correcta para versiones recientes de Langchain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "import torch\n",
        "\n",
        "\n",
        "# Define la plantilla del prompt DETALLADA\n",
        "prompt_template = \"\"\"\n",
        "Eres un asistente legal experto en protección de datos en España.\n",
        "Tu tarea es analizar la legalidad de la situación descrita en la 'Pregunta' basándote **principalmente** en los siguientes 'Textos Legales Recuperados'.\n",
        "Si los textos recuperados contienen suficiente información, responde únicamente basándote en ellos.\n",
        "Si los textos no contienen una respuesta clara pero la pregunta se refiere a conceptos básicos del RGPD, usa lo aprendido en el entrenamiento para dar una respuesta general, especificando que no proviene de los textos recuperados.\n",
        "Si la pregunta requiere información específica que no está en los textos recuperados ni en tu entrenamiento, indícalo claramente.\n",
        "\n",
        "Textos Legales Recuperados:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "Análisis Legal y Conclusión:\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Crea la cadena usando load_qa_chain con el prompt personalizado\n",
        "# chain_type=\"stuff\" es adecuado si los chunks recuperados + pregunta caben en la ventana de contexto del LLM.\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=PROMPT)\n",
        "\n",
        "print (\"-\"*50)\n",
        "print (\"Asistente legal de protección de datos listo (usando load_qa_chain). Escribe 'Exit' para salir.\")\n",
        "print (\"-\"*50)\n",
        "\n",
        "\n",
        "query=pregunta\n",
        "# 1. Recupera los documentos relevantes\n",
        "# Se puede ajustar 'k' aquí si quieres más/menos contexto: retriever.search_kwargs = {'k': 5}\n",
        "docs_retrieved = retriever.invoke(query)\n",
        "\n",
        "# Prepara el input para la cadena\n",
        "input_data = {\n",
        "    \"input_documents\": docs_retrieved,\n",
        "    \"question\": query\n",
        "}\n",
        "\n",
        "# 2. Ejecuta la cadena de generación con los documentos recuperados y la pregunta\n",
        "# Si usas una versión de Langchain > 0.1.0, invoke devuelve un diccionario.\n",
        "# Si usas una versión anterior, podría devolver directamente el string.\n",
        "# El parámetro return_only_outputs=True ya no es necesario/válido en invoke para versiones > 0.1.0\n",
        "result = chain.invoke(input_data)\n",
        "\n",
        "# Imprime la salida del LLM (la clave suele ser 'output_text' en versiones recientes)\n",
        "if isinstance(result, dict) and 'output_text' in result:\n",
        "    print(\"\\nRespuesta:\")\n",
        "    print(result['output_text'])\n",
        "elif isinstance(result, str): # Compatibilidad con versiones anteriores\n",
        "      print(\"\\nRespuesta:\")\n",
        "      print(result)\n",
        "else:\n",
        "      print(\"\\nRespuesta recibida (formato inesperado):\")\n",
        "      print(result)\n",
        "\n",
        "\n",
        "print(\"\\nSaliendo del asistente.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz5jjApKYDsc"
      },
      "source": [
        "### 16bit GGUF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLKZ3HsTYG4l",
        "outputId": "d3f4a2f0-d956-4aa4-d832-bbaeacbf7b15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "Asistente legal de protección de datos listo (usando load_qa_chain). Escribe 'Exit' para salir.\n",
            "--------------------------------------------------\n",
            "\n",
            "Respuesta:\n",
            "Sinosuque podría enfrentarse a varias sanciones por violación de la protección de datos. Al compartir el DNI y las notas académicas del compañero sin consentimiento, vulneró el principio de minimización de datos personales (RGPD, art. 5). Además, la difusión de información privada con fines vejatorios constituiría acoso online (Ley Orgánica 3/2018), mientras que la publicación de datos bancarios sin justificación podría ser considerada revelación indebida (RGPD, art. 4).\n",
            "\n",
            "Saliendo del asistente.\n"
          ]
        }
      ],
      "source": [
        "#Para resetear ollama\n",
        "!kill -9 $(ps aux | grep '[o]llama' | awk '{print $2}')\n",
        "\n",
        "import subprocess\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3) # Esperar a que Ollama se cargue\n",
        "\n",
        "\n",
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# Initialize the LLaMA model\n",
        "llm = OllamaLLM(model=\"hf.co/serdom02/model_16bitGGUF\") #aqui poinemos el modelo base\n",
        "\n",
        "# Importación correcta para versiones recientes de Langchain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "import torch\n",
        "\n",
        "\n",
        "# Define la plantilla del prompt DETALLADA\n",
        "prompt_template = \"\"\"\n",
        "Eres un asistente legal experto en protección de datos en España.\n",
        "Tu tarea es analizar la legalidad de la situación descrita en la 'Pregunta' basándote **principalmente** en los siguientes 'Textos Legales Recuperados'.\n",
        "Si los textos recuperados contienen suficiente información, responde únicamente basándote en ellos.\n",
        "Si los textos no contienen una respuesta clara pero la pregunta se refiere a conceptos básicos del RGPD, usa lo aprendido en el entrenamiento para dar una respuesta general, especificando que no proviene de los textos recuperados.\n",
        "Si la pregunta requiere información específica que no está en los textos recuperados ni en tu entrenamiento, indícalo claramente.\n",
        "\n",
        "Textos Legales Recuperados:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "Análisis Legal y Conclusión:\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Crea la cadena usando load_qa_chain con el prompt personalizado\n",
        "# chain_type=\"stuff\" es adecuado si los chunks recuperados + pregunta caben en la ventana de contexto del LLM.\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=PROMPT)\n",
        "\n",
        "print (\"-\"*50)\n",
        "print (\"Asistente legal de protección de datos listo (usando load_qa_chain). Escribe 'Exit' para salir.\")\n",
        "print (\"-\"*50)\n",
        "\n",
        "\n",
        "query=pregunta\n",
        "# 1. Recupera los documentos relevantes\n",
        "# Se puede ajustar 'k' aquí si quieres más/menos contexto: retriever.search_kwargs = {'k': 5}\n",
        "docs_retrieved = retriever.invoke(query)\n",
        "\n",
        "# Prepara el input para la cadena\n",
        "input_data = {\n",
        "    \"input_documents\": docs_retrieved,\n",
        "    \"question\": query\n",
        "}\n",
        "\n",
        "# 2. Ejecuta la cadena de generación con los documentos recuperados y la pregunta\n",
        "# Si usas una versión de Langchain > 0.1.0, invoke devuelve un diccionario.\n",
        "# Si usas una versión anterior, podría devolver directamente el string.\n",
        "# El parámetro return_only_outputs=True ya no es necesario/válido en invoke para versiones > 0.1.0\n",
        "result = chain.invoke(input_data)\n",
        "\n",
        "# Imprime la salida del LLM (la clave suele ser 'output_text' en versiones recientes)\n",
        "if isinstance(result, dict) and 'output_text' in result:\n",
        "    print(\"\\nRespuesta:\")\n",
        "    print(result['output_text'])\n",
        "elif isinstance(result, str): # Compatibilidad con versiones anteriores\n",
        "      print(\"\\nRespuesta:\")\n",
        "      print(result)\n",
        "else:\n",
        "      print(\"\\nRespuesta recibida (formato inesperado):\")\n",
        "      print(result)\n",
        "\n",
        "\n",
        "print(\"\\nSaliendo del asistente.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QYAWjBEYFDn"
      },
      "source": [
        "### q4_k_m GGUF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dL5mYhqYEzo",
        "outputId": "8bcb88b2-d6cd-46a1-e5a9-0cd6794e824d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--------------------------------------------------\n",
            "Asistente legal de protección de datos listo (usando load_qa_chain). Escribe 'Exit' para salir.\n",
            "--------------------------------------------------\n",
            "\n",
            "Respuesta:\n",
            "Sinosuque podría enfrentarse a varios delitos, como:\n",
            "\n",
            "* Vía de difusión no consentida (artículo 7.Ley Orgánica 3/1989): Al publicar las notas personales de su compañero sin autorización, Sinosuque estuvo vulnerando su derecho a la protección de datos.\n",
            "* Acoso o vejigio (artículos 489 y siguientes del Código Penal): Si las publicaciones incluían contenido ofensivo o humillante, Sinosuque podría ser sancionado por acoso o vejigio.\n",
            "* Uso indebido de datos personales (artículo 11.Ley Orgánica 3/1989): Al compartir información sensible como el DNI sin consentimiento, Sinosuque incumplió con la normativa de protección de datos.\n",
            "\n",
            "Es importante recordar que cualquier persona tiene derecho a la protección de sus datos personales y a solicitar su supresión o limitación en los casos previstos por la ley.\n",
            "\n",
            "Saliendo del asistente.\n"
          ]
        }
      ],
      "source": [
        "#Para resetear ollama\n",
        "!kill -9 $(ps aux | grep '[o]llama' | awk '{print $2}')\n",
        "\n",
        "import subprocess\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3) # Esperar a que Ollama se cargue\n",
        "\n",
        "\n",
        "from langchain_ollama import OllamaLLM\n",
        "\n",
        "# Initialize the LLaMA model\n",
        "llm = OllamaLLM(model=\"hf.co/serdom02/model_q4_k_mGGUF\") #aqui poinemos el modelo base\n",
        "\n",
        "# Importación correcta para versiones recientes de Langchain\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts import PromptTemplate\n",
        "import torch\n",
        "\n",
        "\n",
        "# Define la plantilla del prompt DETALLADA\n",
        "prompt_template = \"\"\"\n",
        "Eres un asistente legal experto en protección de datos en España.\n",
        "Tu tarea es analizar la legalidad de la situación descrita en la 'Pregunta' basándote **principalmente** en los siguientes 'Textos Legales Recuperados'.\n",
        "Si los textos recuperados contienen suficiente información, responde únicamente basándote en ellos.\n",
        "Si los textos no contienen una respuesta clara pero la pregunta se refiere a conceptos básicos del RGPD, usa lo aprendido en el entrenamiento para dar una respuesta general, especificando que no proviene de los textos recuperados.\n",
        "Si la pregunta requiere información específica que no está en los textos recuperados ni en tu entrenamiento, indícalo claramente.\n",
        "\n",
        "Textos Legales Recuperados:\n",
        "---------------------\n",
        "{context}\n",
        "---------------------\n",
        "\n",
        "Pregunta: {question}\n",
        "\n",
        "Análisis Legal y Conclusión:\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "\n",
        "# Crea la cadena usando load_qa_chain con el prompt personalizado\n",
        "# chain_type=\"stuff\" es adecuado si los chunks recuperados + pregunta caben en la ventana de contexto del LLM.\n",
        "chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=PROMPT)\n",
        "\n",
        "print (\"-\"*50)\n",
        "print (\"Asistente legal de protección de datos listo (usando load_qa_chain). Escribe 'Exit' para salir.\")\n",
        "print (\"-\"*50)\n",
        "\n",
        "\n",
        "query=pregunta\n",
        "# 1. Recupera los documentos relevantes\n",
        "# Se puede ajustar 'k' aquí si quieres más/menos contexto: retriever.search_kwargs = {'k': 5}\n",
        "docs_retrieved = retriever.invoke(query)\n",
        "\n",
        "# Prepara el input para la cadena\n",
        "input_data = {\n",
        "    \"input_documents\": docs_retrieved,\n",
        "    \"question\": query\n",
        "}\n",
        "\n",
        "# 2. Ejecuta la cadena de generación con los documentos recuperados y la pregunta\n",
        "# Si usas una versión de Langchain > 0.1.0, invoke devuelve un diccionario.\n",
        "# Si usas una versión anterior, podría devolver directamente el string.\n",
        "# El parámetro return_only_outputs=True ya no es necesario/válido en invoke para versiones > 0.1.0\n",
        "result = chain.invoke(input_data)\n",
        "\n",
        "# Imprime la salida del LLM (la clave suele ser 'output_text' en versiones recientes)\n",
        "if isinstance(result, dict) and 'output_text' in result:\n",
        "    print(\"\\nRespuesta:\")\n",
        "    print(result['output_text'])\n",
        "elif isinstance(result, str): # Compatibilidad con versiones anteriores\n",
        "      print(\"\\nRespuesta:\")\n",
        "      print(result)\n",
        "else:\n",
        "      print(\"\\nRespuesta recibida (formato inesperado):\")\n",
        "      print(result)\n",
        "\n",
        "\n",
        "print(\"\\nSaliendo del asistente.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVn5G4j93f2x"
      },
      "source": [
        "# Servidor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VzaBVm53lVJ"
      },
      "source": [
        "En este apartado se configura el backend que da servicio a la pagina web que usa el usuario para interactuar con el modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9r5qu7R3lwa"
      },
      "source": [
        "## Paso 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQ3wKgOb3oN_"
      },
      "source": [
        "Creamos la función que vamos a utilizar para interactuar con el modelo (Hay que ejecutar antes las celdas de la parte Aplicación Final Mejorada)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1In3ms-6fe-V"
      },
      "source": [
        "Se Asocia una instancia de ImprovedMemory a cada nombre de usuario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwsZfjXJk6x-"
      },
      "outputs": [],
      "source": [
        "def responder_web(pregunta: str, usuario: str, app) -> str:\n",
        "    try:\n",
        "        sesiones = app.state.user_sessions\n",
        "        usuario = usuario.strip().lower()\n",
        "\n",
        "        # Obtener o crear sesión del usuario\n",
        "        if usuario not in sesiones:\n",
        "            sesiones[usuario] = UsuarioSession()\n",
        "\n",
        "        sesion = sesiones[usuario]\n",
        "\n",
        "        # Inyectar el last_query en get_response_with_retry (requiere adaptación)\n",
        "        result = get_response_with_retry(\n",
        "            query=pregunta,\n",
        "            llm=llm,\n",
        "            base_retriever=base_retriever,\n",
        "            memory=sesion.memory,\n",
        "            last_query=sesion.last_query\n",
        "        )\n",
        "\n",
        "        # Actualizar last_query\n",
        "        sesion.last_query = pregunta\n",
        "\n",
        "        return result.get(\"response\", \"No se pudo generar una respuesta.\")\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error en responder_web: {e}\", exc_info=True)\n",
        "        return f\"Ocurrió un error al procesar tu consulta: {e}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rw4mRKzz36rJ"
      },
      "source": [
        "## Paso 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4E448E2T39bL"
      },
      "source": [
        "Instalamos la dependencias y creamos la API usando FastAPI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLqVQo1j37cT",
        "outputId": "5f85d1c5-50e3-47e9-a1e7-d99e69fdd2dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: fastapi in c:\\programdata\\anaconda3\\lib\\site-packages (0.115.12)\n",
            "Requirement already satisfied: uvicorn in c:\\programdata\\anaconda3\\lib\\site-packages (0.34.2)\n",
            "Requirement already satisfied: nest_asyncio in c:\\programdata\\anaconda3\\lib\\site-packages (1.6.0)\n",
            "Requirement already satisfied: pyngrok in c:\\programdata\\anaconda3\\lib\\site-packages (7.2.5)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from fastapi) (0.46.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from fastapi) (2.10.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from fastapi) (4.12.2)\n",
            "Requirement already satisfied: click>=7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from uvicorn) (8.1.7)\n",
            "Requirement already satisfied: h11>=0.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from uvicorn) (0.14.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pyngrok) (6.0.1)\n",
            "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click>=7.0->uvicorn) (0.4.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.27.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from starlette<0.47.0,>=0.40.0->fastapi) (4.2.0)\n",
            "Requirement already satisfied: idna>=2.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (3.7)\n",
            "Requirement already satisfied: sniffio>=1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install fastapi uvicorn nest_asyncio pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdBIkRHy4B_y",
        "outputId": "e4fc5395-801b-4416-a8b0-d6b26db3c710"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 15:01:05,891 - pyngrok.ngrok - INFO - Opening tunnel named: http-8000-5ce72bd7-578c-4bd1-9b78-8465225c6ca9\n",
            "2025-05-08 15:01:05,959 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:01:05+0200 lvl=info msg=\"no configuration paths supplied\"\n",
            "2025-05-08 15:01:05,961 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:01:05+0200 lvl=info msg=\"using configuration at default config path\" path=C:\\\\Users\\\\sdominguez\\\\AppData\\\\Local/ngrok/ngrok.yml\n",
            "2025-05-08 15:01:05,963 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:01:05+0200 lvl=info msg=\"open config file\" path=C:\\\\Users\\\\sdominguez\\\\AppData\\\\Local\\\\ngrok\\\\ngrok.yml err=nil\n",
            "2025-05-08 15:01:05,965 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:01:05+0200 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\n",
            "2025-05-08 15:01:06,336 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:01:06+0200 lvl=info msg=\"client session established\" obj=tunnels.session\n",
            "2025-05-08 15:01:06,337 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:01:06+0200 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "2025-05-08 15:01:06,642 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:01:06+0200 lvl=info msg=start pg=/api/tunnels id=fbe052db02c42d39\n",
            "2025-05-08 15:01:06,648 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:01:06+0200 lvl=info msg=end pg=/api/tunnels id=fbe052db02c42d39 status=200 dur=0s\n",
            "2025-05-08 15:01:06,651 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:01:06+0200 lvl=info msg=start pg=/api/tunnels id=628925aeebf17b2b\n",
            "2025-05-08 15:01:06,653 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:01:06+0200 lvl=info msg=end pg=/api/tunnels id=628925aeebf17b2b status=200 dur=0s\n",
            "2025-05-08 15:01:06,655 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:01:06+0200 lvl=info msg=start pg=/api/tunnels id=efd96a404d8805c8\n",
            "2025-05-08 15:01:06,919 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:01:06+0200 lvl=info msg=\"started tunnel\" obj=tunnels name=http-8000-5ce72bd7-578c-4bd1-9b78-8465225c6ca9 addr=http://localhost:8000 url=https://2c8f-138-100-68-196.ngrok-free.app\n",
            "2025-05-08 15:01:06,922 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:01:06+0200 lvl=info msg=end pg=/api/tunnels id=efd96a404d8805c8 status=201 dur=268.3244ms\n",
            "INFO:     Started server process [6096]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tu servidor está disponible en: NgrokTunnel: \"https://2c8f-138-100-68-196.ngrok-free.app\" -> \"http://localhost:8000\"\n",
            "INFO:     138.100.68.196:0 - \"OPTIONS /chat HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 15:02:31,080 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:02:31+0200 lvl=info msg=\"join connections\" obj=join id=66ec1d75393b l=127.0.0.1:8000 r=138.100.68.196:56615\n",
            "2025-05-08 15:02:31,342 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu\n",
            "2025-05-08 15:02:31,344 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-mpnet-base-v2\n",
            "2025-05-08 15:02:38,091 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:02:38,234 - asistente_legal - INFO - Clasificación: GENERAL_INFO, Confianza: 50\n",
            "2025-05-08 15:02:38,235 - asistente_legal - INFO - Intención clasificada como: GENERAL_INFO (Confianza: 50%) - Usar RAG: True\n",
            "2025-05-08 15:02:38,237 - asistente_legal - INFO - Usando el prompt: general_info_prompt\n",
            "2025-05-08 15:02:38,240 - asistente_legal - INFO - Intento RAG 1 para consulta: 'porque' (Categoría: GENERAL_INFO)\n",
            "C:\\Users\\sdominguez\\AppData\\Local\\Temp\\ipykernel_6096\\1207801112.py:564: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  retrieved_docs = retriever.get_relevant_documents(query)\n",
            "2025-05-08 15:02:38,301 - asistente_legal - INFO - Recuperados 5 documentos.\n",
            "C:\\Users\\sdominguez\\AppData\\Local\\Temp\\ipykernel_6096\\1207801112.py:449: LangChainDeprecationWarning: This class is deprecated. See the following migration guides for replacements based on `chain_type`:\n",
            "stuff: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain\n",
            "map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\n",
            "refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\n",
            "map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\n",
            "\n",
            "See also guides on retrieval and question-answering here: https://python.langchain.com/docs/how_to/#qa-with-rag\n",
            "  qa_chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=prompt, verbose=False) # verbose=True para debug\n",
            "2025-05-08 15:02:45,725 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:02:48,315 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:02:50,162 - asistente_legal - INFO - Validación: {'valid': False, 'precision': False, 'completitud': None, 'sin_errores': None, 'raw_validation_output': '[PRECISIÓN: No, la normativa es más amplia que solo el consentimiento o una necesidad legítima. Existen otras bases legales como interés público, obligación legal o intereses empresariales.]'}\n",
            "2025-05-08 15:02:50,164 - asistente_legal - WARNING - Respuesta no válida en intento 1: {'valid': False, 'precision': False, 'completitud': None, 'sin_errores': None, 'raw_validation_output': '[PRECISIÓN: No, la normativa es más amplia que solo el consentimiento o una necesidad legítima. Existen otras bases legales como interés público, obligación legal o intereses empresariales.]'}\n",
            "2025-05-08 15:02:50,166 - asistente_legal - INFO - Usando el prompt: general_info_prompt\n",
            "2025-05-08 15:02:50,168 - asistente_legal - INFO - Prompt adaptado con sugerencias del validador.\n",
            "2025-05-08 15:02:50,169 - asistente_legal - INFO - Intento RAG 2 para consulta: 'porque' (Categoría: GENERAL_INFO)\n",
            "2025-05-08 15:02:50,172 - asistente_legal - INFO - Reintento 1: aumentando k y activando reranking.\n",
            "2025-05-08 15:02:50,211 - asistente_legal - INFO - Recuperados 8 documentos.\n",
            "2025-05-08 15:02:50,212 - asistente_legal - INFO - Iniciando reranking para 8 documentos recuperados...\n",
            "2025-05-08 15:02:50,529 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:02:50,714 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:02:50,995 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:02:51,291 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:02:51,575 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:02:51,890 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:02:52,172 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:02:52,485 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:02:52,527 - asistente_legal - INFO - Reranking completado. 4 documentos seleccionados.\n",
            "2025-05-08 15:02:52,528 - asistente_legal - INFO - Documentos después de reranking: 4\n",
            "2025-05-08 15:02:54,606 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:02:56,007 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:02:59,985 - asistente_legal - INFO - Validación: {'valid': False, 'precision': True, 'completitud': False, 'sin_errores': False, 'raw_validation_output': '[PRECISIÓN: Sí, la respuesta refleja el principio de transparencia del RGPD y la LOPDGDD.], [COMPLETITUD: No, no explica qué se entiende por \"interés legítimo\" o cómo se garantiza la protección de datos en cada caso concreto.], [ERRORES: Sí, omite detalles clave sobre los derechos del usuario y no ofrece orientación específica para situaciones complejas.]'}\n",
            "2025-05-08 15:02:59,987 - asistente_legal - WARNING - Respuesta no válida en intento 2: {'valid': False, 'precision': True, 'completitud': False, 'sin_errores': False, 'raw_validation_output': '[PRECISIÓN: Sí, la respuesta refleja el principio de transparencia del RGPD y la LOPDGDD.], [COMPLETITUD: No, no explica qué se entiende por \"interés legítimo\" o cómo se garantiza la protección de datos en cada caso concreto.], [ERRORES: Sí, omite detalles clave sobre los derechos del usuario y no ofrece orientación específica para situaciones complejas.]'}\n",
            "2025-05-08 15:02:59,988 - asistente_legal - INFO - Usando el prompt: general_info_prompt\n",
            "2025-05-08 15:02:59,990 - asistente_legal - INFO - Prompt adaptado con sugerencias del validador.\n",
            "2025-05-08 15:02:59,993 - asistente_legal - INFO - Intento RAG 3 para consulta: 'porque' (Categoría: GENERAL_INFO)\n",
            "2025-05-08 15:02:59,995 - asistente_legal - INFO - Reintento 2: creando un retriever más flexible (MMR + reformulación posible).\n",
            "2025-05-08 15:03:00,037 - asistente_legal - INFO - Recuperados 10 documentos.\n",
            "2025-05-08 15:03:00,037 - asistente_legal - INFO - Iniciando reranking para 10 documentos recuperados...\n",
            "2025-05-08 15:03:00,352 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:03:00,672 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:03:00,986 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:03:01,264 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:03:01,550 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:03:01,759 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:03:02,037 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:03:02,336 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:03:02,378 - asistente_legal - INFO - Reranking completado. 4 documentos seleccionados.\n",
            "2025-05-08 15:03:02,379 - asistente_legal - INFO - Documentos después de reranking: 4\n",
            "2025-05-08 15:03:05,920 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:03:07,467 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:03:08,387 - asistente_legal - INFO - Validación: {'valid': False, 'precision': False, 'completitud': False, 'sin_errores': False, 'raw_validation_output': '[PRECISIÓN: No], [COMPLETITUD: No], [ERRORES: Sí]'}\n",
            "2025-05-08 15:03:08,388 - asistente_legal - WARNING - Respuesta no válida en intento 3: {'valid': False, 'precision': False, 'completitud': False, 'sin_errores': False, 'raw_validation_output': '[PRECISIÓN: No], [COMPLETITUD: No], [ERRORES: Sí]'}\n",
            "2025-05-08 15:03:08,390 - asistente_legal - INFO - Iniciando auto-refinamiento de respuesta con feedback del validador.\n",
            "C:\\Users\\sdominguez\\AppData\\Local\\Temp\\ipykernel_6096\\1207801112.py:636: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  refinement_chain = LLMChain(llm=llm, prompt=refinement_prompt)\n",
            "2025-05-08 15:03:08,749 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     138.100.68.196:0 - \"POST /chat HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 15:03:21,949 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:03:21+0200 lvl=info msg=\"join connections\" obj=join id=7f62c61d1a9b l=127.0.0.1:8000 r=138.100.68.196:56662\n",
            "2025-05-08 15:03:21,959 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu\n",
            "2025-05-08 15:03:21,960 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-mpnet-base-v2\n",
            "2025-05-08 15:03:24,355 - asistente_legal - INFO - Detectada pregunta de seguimiento. Incorporando última consulta como contexto.\n",
            "2025-05-08 15:03:28,762 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:03:28,930 - asistente_legal - INFO - Clasificación: CONVERSATION, Confianza: 99\n",
            "2025-05-08 15:03:28,931 - asistente_legal - INFO - Intención clasificada como: CONVERSATION (Confianza: 99%) - Usar RAG: False\n",
            "2025-05-08 15:03:29,167 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     138.100.68.196:0 - \"POST /chat HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 15:04:32,387 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:04:32+0200 lvl=info msg=\"join connections\" obj=join id=51d850ccb46b l=127.0.0.1:8000 r=138.100.68.196:56662\n",
            "2025-05-08 15:04:32,399 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu\n",
            "2025-05-08 15:04:32,401 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-mpnet-base-v2\n",
            "2025-05-08 15:04:36,319 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:04:36,479 - asistente_legal - INFO - Clasificación: CONVERSATION, Confianza: 98\n",
            "2025-05-08 15:04:36,481 - asistente_legal - INFO - Intención clasificada como: CONVERSATION (Confianza: 98%) - Usar RAG: False\n",
            "2025-05-08 15:04:36,745 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     138.100.68.196:0 - \"POST /chat HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 15:04:52,680 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:04:52+0200 lvl=info msg=\"join connections\" obj=join id=9de6fce4ebb1 l=127.0.0.1:8000 r=138.100.68.196:56662\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     138.100.68.196:0 - \"OPTIONS /reset HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 15:04:52,934 - asistente_legal - INFO - Solicitud de reseteo para: sergio\n",
            "2025-05-08 15:04:52,936 - asistente_legal - INFO - Memoria borrada para sergio\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     138.100.68.196:0 - \"POST /reset HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 15:04:58,768 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:04:58+0200 lvl=info msg=\"join connections\" obj=join id=0b467bb5f2be l=127.0.0.1:8000 r=138.100.68.196:56662\n",
            "2025-05-08 15:04:58,778 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu\n",
            "2025-05-08 15:04:58,779 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-mpnet-base-v2\n",
            "2025-05-08 15:05:01,120 - asistente_legal - INFO - Resultado de clasificación obtenido de caché para: di mi nombre, quiero saber si lo has podido recordar\n",
            "2025-05-08 15:05:01,121 - asistente_legal - INFO - Intención clasificada como: CONVERSATION (Confianza: 98%) - Usar RAG: False\n",
            "2025-05-08 15:05:01,660 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     138.100.68.196:0 - \"POST /chat HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 15:05:18,721 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:05:18+0200 lvl=info msg=\"join connections\" obj=join id=1e4e84cd385f l=127.0.0.1:8000 r=138.100.68.196:56662\n",
            "2025-05-08 15:05:18,731 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu\n",
            "2025-05-08 15:05:18,733 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-mpnet-base-v2\n",
            "2025-05-08 15:05:22,445 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:05:22,614 - asistente_legal - INFO - Clasificación: LEGAL_ANALYSIS, Confianza: 92\n",
            "2025-05-08 15:05:22,615 - asistente_legal - INFO - Intención clasificada como: LEGAL_ANALYSIS (Confianza: 92%) - Usar RAG: True\n",
            "2025-05-08 15:05:22,617 - asistente_legal - INFO - Usando el prompt: legal_analysis_prompt\n",
            "2025-05-08 15:05:22,619 - asistente_legal - INFO - Intento RAG 1 para consulta: 'Un gimnasio quiere implementar un sistema de acceso mediante huella dactilar para sus socios. Planean incluir una cláusula en el contrato de inscripción donde el socio 'acepta el uso de sus datos biométricos para el control de acceso'. ¿Es esta forma de obtener el consentimiento válida según el RGPD y la LOPDGDD, considerando que los datos biométricos son una categoría especial de datos? Justifica la respuesta con los artículos pertinentes que encuentres.' (Categoría: LEGAL_ANALYSIS)\n",
            "2025-05-08 15:05:22,696 - asistente_legal - INFO - Recuperados 5 documentos.\n",
            "2025-05-08 15:05:32,124 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:05:43,761 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:05:48,295 - asistente_legal - INFO - Validación: {'valid': False, 'precision': False, 'completitud': False, 'sin_errores': False, 'raw_validation_output': '[PRECISIÓN: No, la normativa exige una información detallada y proporcional sobre el tratamiento de datos biométricos, no solo su inclusión en un contrato.] [COMPLETITUD: No, la respuesta no explica específicamente cómo informar a los socios sobre sus derechos con estos datos sensibles.] [ERRORES: Sí, se omite que el consentimiento debe ser libremente otorgado, y no se mencionan las garantías de seguridad adicionales para proteger estos datos.]'}\n",
            "2025-05-08 15:05:48,297 - asistente_legal - WARNING - Respuesta no válida en intento 1: {'valid': False, 'precision': False, 'completitud': False, 'sin_errores': False, 'raw_validation_output': '[PRECISIÓN: No, la normativa exige una información detallada y proporcional sobre el tratamiento de datos biométricos, no solo su inclusión en un contrato.] [COMPLETITUD: No, la respuesta no explica específicamente cómo informar a los socios sobre sus derechos con estos datos sensibles.] [ERRORES: Sí, se omite que el consentimiento debe ser libremente otorgado, y no se mencionan las garantías de seguridad adicionales para proteger estos datos.]'}\n",
            "2025-05-08 15:05:48,299 - asistente_legal - INFO - Usando el prompt: legal_analysis_prompt\n",
            "2025-05-08 15:05:48,302 - asistente_legal - INFO - Prompt adaptado con sugerencias del validador.\n",
            "2025-05-08 15:05:48,304 - asistente_legal - INFO - Intento RAG 2 para consulta: 'Un gimnasio quiere implementar un sistema de acceso mediante huella dactilar para sus socios. Planean incluir una cláusula en el contrato de inscripción donde el socio 'acepta el uso de sus datos biométricos para el control de acceso'. ¿Es esta forma de obtener el consentimiento válida según el RGPD y la LOPDGDD, considerando que los datos biométricos son una categoría especial de datos? Justifica la respuesta con los artículos pertinentes que encuentres.' (Categoría: LEGAL_ANALYSIS)\n",
            "2025-05-08 15:05:48,306 - asistente_legal - INFO - Reintento 1: aumentando k y activando reranking.\n",
            "2025-05-08 15:05:48,371 - asistente_legal - INFO - Recuperados 8 documentos.\n",
            "2025-05-08 15:05:48,372 - asistente_legal - INFO - Iniciando reranking para 8 documentos recuperados...\n",
            "2025-05-08 15:05:48,740 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:05:49,048 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:05:49,342 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:05:49,627 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:05:49,922 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:05:50,216 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:05:50,527 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:05:50,817 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:05:50,863 - asistente_legal - INFO - Reranking completado. 4 documentos seleccionados.\n",
            "2025-05-08 15:05:50,865 - asistente_legal - INFO - Documentos después de reranking: 4\n",
            "2025-05-08 15:05:54,427 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:06:08,734 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:06:12,520 - asistente_legal - INFO - Validación: {'valid': False, 'precision': False, 'completitud': False, 'sin_errores': False, 'raw_validation_output': '[PRECISIÓN: No, la normativa no se aplica de forma general sin consideraciones adicionales.]\\n[COMPLETITUD: No, omite detalles sobre el consentimiento informado y la alternativa para los socios que no quieran usar su huella dactilar.]\\n[ERRORES: Sí, contiene una afirmación incorrecta al considerar que el consentimiento es suficiente solo por ser incluido en un contrato.]'}\n",
            "2025-05-08 15:06:12,522 - asistente_legal - WARNING - Respuesta no válida en intento 2: {'valid': False, 'precision': False, 'completitud': False, 'sin_errores': False, 'raw_validation_output': '[PRECISIÓN: No, la normativa no se aplica de forma general sin consideraciones adicionales.]\\n[COMPLETITUD: No, omite detalles sobre el consentimiento informado y la alternativa para los socios que no quieran usar su huella dactilar.]\\n[ERRORES: Sí, contiene una afirmación incorrecta al considerar que el consentimiento es suficiente solo por ser incluido en un contrato.]'}\n",
            "2025-05-08 15:06:12,525 - asistente_legal - INFO - Usando el prompt: legal_analysis_prompt\n",
            "2025-05-08 15:06:12,527 - asistente_legal - INFO - Prompt adaptado con sugerencias del validador.\n",
            "2025-05-08 15:06:12,528 - asistente_legal - INFO - Intento RAG 3 para consulta: 'Un gimnasio quiere implementar un sistema de acceso mediante huella dactilar para sus socios. Planean incluir una cláusula en el contrato de inscripción donde el socio 'acepta el uso de sus datos biométricos para el control de acceso'. ¿Es esta forma de obtener el consentimiento válida según el RGPD y la LOPDGDD, considerando que los datos biométricos son una categoría especial de datos? Justifica la respuesta con los artículos pertinentes que encuentres.' (Categoría: LEGAL_ANALYSIS)\n",
            "2025-05-08 15:06:12,529 - asistente_legal - INFO - Reintento 2: creando un retriever más flexible (MMR + reformulación posible).\n",
            "2025-05-08 15:06:12,596 - asistente_legal - INFO - Recuperados 10 documentos.\n",
            "2025-05-08 15:06:12,597 - asistente_legal - INFO - Iniciando reranking para 10 documentos recuperados...\n",
            "2025-05-08 15:06:12,967 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:06:13,265 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:06:13,567 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:06:13,862 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:06:14,160 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:06:14,458 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:06:14,765 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:06:15,065 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:06:15,109 - asistente_legal - INFO - Reranking completado. 4 documentos seleccionados.\n",
            "2025-05-08 15:06:15,111 - asistente_legal - INFO - Documentos después de reranking: 4\n",
            "2025-05-08 15:06:19,195 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:06:33,282 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:06:37,730 - asistente_legal - INFO - Validación: {'valid': None, 'precision': None, 'completitud': None, 'sin_errores': None, 'raw_validation_output': '1. **No**, aunque el consentimiento no sea válido por defecto, la respuesta debería ofrecer más información sobre los requisitos específicos para tratar datos biométricos.\\n2. **No**, la respuesta podría ser más completa si explicara mejor las garantías adicionales necesarias para proteger estos datos sensibles.\\n3. **Sí**, se omite la importancia de informar detalladamente sobre el tratamiento especial de datos biométricos y las medidas de seguridad adicionales requeridas por la normativa.'}\n",
            "2025-05-08 15:06:37,732 - asistente_legal - WARNING - Respuesta no válida en intento 3: {'valid': None, 'precision': None, 'completitud': None, 'sin_errores': None, 'raw_validation_output': '1. **No**, aunque el consentimiento no sea válido por defecto, la respuesta debería ofrecer más información sobre los requisitos específicos para tratar datos biométricos.\\n2. **No**, la respuesta podría ser más completa si explicara mejor las garantías adicionales necesarias para proteger estos datos sensibles.\\n3. **Sí**, se omite la importancia de informar detalladamente sobre el tratamiento especial de datos biométricos y las medidas de seguridad adicionales requeridas por la normativa.'}\n",
            "2025-05-08 15:06:37,734 - asistente_legal - INFO - Iniciando auto-refinamiento de respuesta con feedback del validador.\n",
            "2025-05-08 15:06:38,402 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:06:54,107 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:06:54+0200 lvl=info msg=\"join connections\" obj=join id=6f89c1ec2513 l=127.0.0.1:8000 r=138.100.68.196:56662\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     138.100.68.196:0 - \"POST /chat HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 15:06:55,326 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu\n",
            "2025-05-08 15:06:55,327 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-mpnet-base-v2\n",
            "2025-05-08 15:07:02,051 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:07:02,220 - asistente_legal - INFO - Clasificación: CONVERSATION, Confianza: 99\n",
            "2025-05-08 15:07:02,222 - asistente_legal - INFO - Intención clasificada como: CONVERSATION (Confianza: 99%) - Usar RAG: False\n",
            "2025-05-08 15:07:02,443 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     138.100.68.196:0 - \"POST /chat HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 15:07:36,863 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:07:36+0200 lvl=info msg=\"join connections\" obj=join id=e83afe023116 l=127.0.0.1:8000 r=138.100.68.196:56662\n",
            "2025-05-08 15:07:36,872 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu\n",
            "2025-05-08 15:07:36,874 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-mpnet-base-v2\n",
            "2025-05-08 15:07:39,339 - asistente_legal - INFO - Resultado de clasificación obtenido de caché para: di mi nombre, quiero saber si lo has podido recordar\n",
            "2025-05-08 15:07:39,341 - asistente_legal - INFO - Intención clasificada como: CONVERSATION (Confianza: 98%) - Usar RAG: False\n",
            "2025-05-08 15:07:40,330 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     138.100.68.196:0 - \"POST /chat HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 15:07:58,976 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:07:58+0200 lvl=info msg=\"join connections\" obj=join id=f31b8fae2836 l=127.0.0.1:8000 r=138.100.68.196:56662\n",
            "2025-05-08 15:07:58,986 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu\n",
            "2025-05-08 15:07:58,988 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-mpnet-base-v2\n",
            "2025-05-08 15:08:01,521 - asistente_legal - INFO - Detectada pregunta de seguimiento. Incorporando última consulta como contexto.\n",
            "2025-05-08 15:08:02,597 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n",
            "2025-05-08 15:08:02,762 - asistente_legal - INFO - Clasificación: CONVERSATION, Confianza: 98\n",
            "2025-05-08 15:08:02,764 - asistente_legal - INFO - Intención clasificada como: CONVERSATION (Confianza: 98%) - Usar RAG: False\n",
            "2025-05-08 15:08:03,011 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     138.100.68.196:0 - \"POST /chat HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 15:09:41,699 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:09:41+0200 lvl=info msg=\"join connections\" obj=join id=f467c384cb89 l=127.0.0.1:8000 r=138.100.68.196:56662\n",
            "2025-05-08 15:09:41,702 - asistente_legal - INFO - Solicitud de reseteo para: pepe\n",
            "2025-05-08 15:09:41,707 - asistente_legal - INFO - Memoria borrada para pepe\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     138.100.68.196:0 - \"POST /reset HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-08 15:09:49,326 - pyngrok.process.ngrok - INFO - t=2025-05-08T15:09:49+0200 lvl=info msg=\"join connections\" obj=join id=bab9af4b3e0d l=127.0.0.1:8000 r=138.100.68.196:56662\n",
            "2025-05-08 15:09:49,337 - sentence_transformers.SentenceTransformer - INFO - Use pytorch device_name: cpu\n",
            "2025-05-08 15:09:49,339 - sentence_transformers.SentenceTransformer - INFO - Load pretrained SentenceTransformer: paraphrase-multilingual-mpnet-base-v2\n",
            "2025-05-08 15:09:51,721 - asistente_legal - INFO - Resultado de clasificación obtenido de caché para: di mi nombre, quiero saber si lo has podido recordar\n",
            "2025-05-08 15:09:51,723 - asistente_legal - INFO - Intención clasificada como: CONVERSATION (Confianza: 98%) - Usar RAG: False\n",
            "2025-05-08 15:09:52,122 - httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/generate \"HTTP/1.1 200 OK\"\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:     138.100.68.196:0 - \"POST /chat HTTP/1.1\" 200 OK\n"
          ]
        }
      ],
      "source": [
        "import nest_asyncio\n",
        "import uvicorn\n",
        "from fastapi import FastAPI, Request\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Necesario para que FastAPI funcione dentro del notebook\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Crea la app\n",
        "app = FastAPI()\n",
        "app.state.user_sessions = {}\n",
        "\n",
        "# CORS para permitir acceso desde GitHub Pages u otro frontend\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],  # O restringe a tu dominio exacto\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "@app.post(\"/reset\") #peticiones HTTP POST que lleguen a la ruta /reset por ej POST https://ip.ngrok.io/reset\n",
        "async def reset_memoria(request: Request):\n",
        "    datos = await request.json()\n",
        "    usuario = datos.get(\"usuario\", \"\").strip().lower()\n",
        "    sesiones = request.app.state.user_sessions\n",
        "\n",
        "    logger.info(f\"Solicitud de reseteo para: {usuario}\")\n",
        "    if usuario in sesiones:\n",
        "        del sesiones[usuario]\n",
        "        logger.info(f\"Memoria borrada para {usuario}\")\n",
        "        return {\"status\": \"ok\", \"mensaje\": f\"Memoria reiniciada para {usuario}\"}\n",
        "    else:\n",
        "        logger.warning(f\"No se encontró memoria activa para {usuario}\")\n",
        "        return {\"status\": \"no-op\", \"mensaje\": \"Usuario no tenía memoria activa\"}\n",
        "\n",
        "\n",
        "\n",
        "# Ruta del chat\n",
        "@app.post(\"/chat\")\n",
        "async def chat(request: Request):\n",
        "    datos = await request.json()\n",
        "    pregunta = datos.get(\"mensaje\", \"\")\n",
        "    usuario = datos.get(\"usuario\", \"invitado\").strip().lower()\n",
        "\n",
        "    respuesta = responder_web(pregunta, usuario, request.app)\n",
        "    return {\"respuesta\": respuesta}\n",
        "\n",
        "# Crear túnel ngrok (puedes copiar la URL que imprime)\n",
        "public_url = ngrok.connect(8000)\n",
        "print(f\"Tu servidor está disponible en: {public_url}\")\n",
        "\n",
        "# Lanzar el servidor\n",
        "uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VFohj9Ife-W"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "NgUHfiVbwTfQ"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0015e48e30504dcfa20e224ddfcf823d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "005b82a3932a4ecb9770921e61eee168": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0067bf50ac57403d95bed599f67e05a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0aed6b8fffc9441dbe28059150085e39",
              "IPY_MODEL_5650051de25b49b3a3cd2ec15dd6ef35",
              "IPY_MODEL_e0b9c95bd1cb4c34ad24e7983d5fb019"
            ],
            "layout": "IPY_MODEL_b2d07be45d1f4b1ea9845b302c32ef9e"
          }
        },
        "00a9009b6cb34da0b8d11233bafe42f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0104974262494a418b716c5fcbd36207": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "014e363aae62423787d393d949df2c9b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0176b9971228499fa742bfce6f754663": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3dcd01f64e374bc6ae6c1d587b81bac5",
            "max": 345,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_939fd41a1efd49f6a0e5e880c8a6aa7e",
            "value": 345
          }
        },
        "03ffb1d6834b444383139a123f373dd1": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0487c8e4a08f4e20b33aae61a069b7d7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04df9580b1ec4ca79d282383eafabaa2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0561192c90dd4ea6a120a2b9940e5ab8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05af9e6bb60047a3ad7e3142a177ca14": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "066a8a3400384015a2ab91be7925e856": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "07e44da4ada941eb8fc5b45251049802": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08746819f1ba4895bcbc13fd0ecbd0ac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08974acddcf14d3684295695bbe6fbac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08ac4c49dee94f17a6ef197a985fd780": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90dbffdf96bf4d609c59d0c631d6fdfd",
            "max": 122,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7d238b16dac94154bd3e24df5636bc65",
            "value": 122
          }
        },
        "08e35bdac488451b82a2f9b785aa6138": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08f864ad78c8485bb45fa50dda7f188a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6cbfef56108b4992b385d82ff36cdbc4",
              "IPY_MODEL_1c7015ec55f545b8a1a26cbf8e32cc39",
              "IPY_MODEL_a4e479e28ce94b0f9b63dd4c435e6e7a"
            ],
            "layout": "IPY_MODEL_81acfacf75ef46d88c4b530896c1d6bb"
          }
        },
        "0a248e4eeb884e16966688be3c5f815b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b0226de01884c16bd3824355fd2daa2",
            "max": 5702746403,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6c0fad39a89342be9404a09ec7c4fcb9",
            "value": 5702745860
          }
        },
        "0ae11327bf7f40fa9a452c313fb4be49": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ae3f54842c04d6ea1b5fa1be3bfbc5e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0aed6b8fffc9441dbe28059150085e39": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0104974262494a418b716c5fcbd36207",
            "placeholder": "​",
            "style": "IPY_MODEL_dc68039b2b7840dabb1d39d90e90afd1",
            "value": "Flattening the indices: 100%"
          }
        },
        "0b98782b5d1d4862af17ae67cc5777ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ed1a808877b54b14a01c0d4ab757dc81",
              "IPY_MODEL_5462e6730819423292ff22ba556578d2",
              "IPY_MODEL_148d32d501274b418bb8d63f2831da34"
            ],
            "layout": "IPY_MODEL_9dff1428059d4bc1b396338372a0ac66"
          }
        },
        "0d29128b3b8d42178f3ae6d6b4772d2f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0dbbc41cb5da49c09557d77318e1d89a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e7b0815f4424615866b745392e1348c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0fcfe5f97c8147a3a375cec97fecd359": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "121e5097c4da4938af588656ee8a14dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9013e44d4b234355bf5cb5ab2cf282a1",
              "IPY_MODEL_54421f39df1a41efacb0b907c57922f6",
              "IPY_MODEL_23ca931599514777b22a9c78d175ed21"
            ],
            "layout": "IPY_MODEL_5ca0d161ddec4450bec2a0cb8d0ad683"
          }
        },
        "131b5d0fad894cdf9ac88fff9c123efe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "138f00d720ac475c9c752dd46e878db2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13f56339d8ff4e548f3389b99b6cd09f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "148d32d501274b418bb8d63f2831da34": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_adc72504dd424f948cc7117553212d57",
            "placeholder": "​",
            "style": "IPY_MODEL_0e7b0815f4424615866b745392e1348c",
            "value": " 723/723 [00:00&lt;00:00, 29.3kB/s]"
          }
        },
        "14d56ccfd0904350abe91735f301aab7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_da6f107176a642e3bf5d4620ae011d03",
              "IPY_MODEL_ed9e04c3c7f84a649a4c36b8f517ca90",
              "IPY_MODEL_e093a79cbce24644ac2f510187798adf"
            ],
            "layout": "IPY_MODEL_4adaeabf499941c1b5c3ca13bccda9f8"
          }
        },
        "15614a90630f48f28e345ee0e71d4306": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "157dcb1f0bf242139f3546cc93b7b936": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15a9b66c449a43ee9e759b29457e1df3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1681c2a889474147a281aa57ea49fa01": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16d2bef217f04594bbc94acd7971ddac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "174183bcb63a4efa8f1420bfc4d0b5c8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1843dac8987a4441819a184141373de2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bd00d3d8b214109973049badc3cb4be",
            "placeholder": "​",
            "style": "IPY_MODEL_39bc31df5c4947ae90a0bf862e739ae8",
            "value": "Flattening the indices: 100%"
          }
        },
        "193259b79e294176983888af51b51103": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a2674c45ca046cba90f0040eef6fe84": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1e88ba228d844f697ac66ab1f11a0f5",
            "placeholder": "​",
            "style": "IPY_MODEL_2d9361af973f426aaf8b6e7816c18f80",
            "value": " 53.0/53.0 [00:00&lt;00:00, 1.59kB/s]"
          }
        },
        "1a9d2ffe04bf4ba2a2f9c372307ab239": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d4c6e36a1b8485d9f8371bd4e88e859",
            "max": 796,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7926b22c81d24b3faa488d7e8cd45e25",
            "value": 796
          }
        },
        "1b9f60aac66949cba9a3f20759d1f5c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a816656658de4212968acb301df96937",
            "max": 51052,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_989344ac67d24b4098b718695ece3f9b",
            "value": 51052
          }
        },
        "1bed7cd946514427849fee72d8762fb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ff68046ed0bd417e92e303c15c43d7f2",
              "IPY_MODEL_b7c8a149304948f7850e24b3b6e2f3b4",
              "IPY_MODEL_9402beee1ca44853b81ca1425b38bdad"
            ],
            "layout": "IPY_MODEL_709541f6add042f9942ece0d18d0ed8b"
          }
        },
        "1c7015ec55f545b8a1a26cbf8e32cc39": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_741258f9827546319a1dc4d2f892210a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e10e80363b754204887fffc4af7cd3d8",
            "value": 1
          }
        },
        "1cd4963370fd42d8beae2e798ffee9a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ec0f34d02eee4fcf9d7f51d86b768b46",
            "placeholder": "​",
            "style": "IPY_MODEL_30ed64dac9cc4a218734f8678d6b35b4",
            "value": "sentencepiece.bpe.model: 100%"
          }
        },
        "1eabbd3c84064a2a86020c1c28b0426c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1fbc81be877b4712933eb6e5c0a28cd4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20804d2b3c7e4ef1ba3d67916d9768d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "20e48edeeb834c16a0e592889bfb33a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2227fdd27eaa4ce49be2a0f852496be2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4d842b8447c4d2db29a2a5646be53db",
            "placeholder": "​",
            "style": "IPY_MODEL_16d2bef217f04594bbc94acd7971ddac",
            "value": " 229/229 [00:00&lt;00:00, 5.67kB/s]"
          }
        },
        "22d6e496881c4d6d89d060a4c785c22c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22ffe8f6ecc941b492179e53a8ad6928": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1cd4963370fd42d8beae2e798ffee9a3",
              "IPY_MODEL_b5d505d5789f43cca86cb1a621745015",
              "IPY_MODEL_2676c6f283104ded9ae4c7f9420f1473"
            ],
            "layout": "IPY_MODEL_7ae263e1a157415b989d86b39dc86ab9"
          }
        },
        "2337590ca184415e9c27920a91f2bddb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73d06604b4f540b58e9cf87c7f56152b",
            "placeholder": "​",
            "style": "IPY_MODEL_3e53b04b3f754fc282692b31ea10baae",
            "value": "Flattening the indices: 100%"
          }
        },
        "23ca931599514777b22a9c78d175ed21": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_24e1e0ebc9f04fadbae02b192b57bfa4",
            "placeholder": "​",
            "style": "IPY_MODEL_07e44da4ada941eb8fc5b45251049802",
            "value": " 1.11G/1.11G [00:06&lt;00:00, 143MB/s]"
          }
        },
        "24e1e0ebc9f04fadbae02b192b57bfa4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "256205400ed448e0a51efdd73e593eb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2676c6f283104ded9ae4c7f9420f1473": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5dd53b147d08431e8ecc73aa798ad889",
            "placeholder": "​",
            "style": "IPY_MODEL_615c3c243a214a27a95a1b3cc807bc21",
            "value": " 5.07M/5.07M [00:00&lt;00:00, 144MB/s]"
          }
        },
        "26d9440af08a4404abe52a76f096bb6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_138f00d720ac475c9c752dd46e878db2",
            "placeholder": "​",
            "style": "IPY_MODEL_3c65196ca805431ebb766846980c6d11",
            "value": "unsloth.Q8_0.gguf: "
          }
        },
        "278d6110c5d946748d5dd90256c326c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_174183bcb63a4efa8f1420bfc4d0b5c8",
            "placeholder": "​",
            "style": "IPY_MODEL_66106b47a3ba43ceb3b943dee3dbe3a0",
            "value": " 796/796 [00:00&lt;00:00, 27148.04 examples/s]"
          }
        },
        "27ae2827671c4143b0b2b2075fc1f972": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_592e543174ac4c149b978a9b076e6947",
              "IPY_MODEL_0a248e4eeb884e16966688be3c5f815b",
              "IPY_MODEL_9c0b992d847a4edc8bae02bdb0a47a52"
            ],
            "layout": "IPY_MODEL_c9b8ab31024948dbab3b9df93a5cc238"
          }
        },
        "283f2a5610894deb9a7b10c6403cd4bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_289b9bc44ac44e6b9876b95b2f0a3133",
            "placeholder": "​",
            "style": "IPY_MODEL_20e48edeeb834c16a0e592889bfb33a6",
            "value": " 402/402 [00:00&lt;00:00, 45.5kB/s]"
          }
        },
        "285d4440076a4c68a6f8317266e880c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5808bf7c644e4228ae669343d44c59bd",
            "placeholder": "​",
            "style": "IPY_MODEL_4a2150ec026648bcba1c75a31618131d",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "288b72f069454a308bbcdd817d8e0af2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_774c667bb01445a79cf8381a7dc73314",
              "IPY_MODEL_4f2a96b0b9654e56a30bfe89a5386a23",
              "IPY_MODEL_92c00c5cb9c24de1af7e40aa0ed15b47"
            ],
            "layout": "IPY_MODEL_fc8181c289ff44c5a83a88684203906e"
          }
        },
        "289b9bc44ac44e6b9876b95b2f0a3133": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b0226de01884c16bd3824355fd2daa2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b400f884db74feb8be26093bdd792a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2b8e98da62ad465793e858bbdc7db9ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40217752dc6245a1b2c67ec4e79e379c",
            "max": 796,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d9476b1342c74f95b72318a5b809402b",
            "value": 796
          }
        },
        "2ce45738eaed494288408212e78724f2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d9361af973f426aaf8b6e7816c18f80": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e39a170df4e413f960ec363bbea2095": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f913635f3df441b82a5374c8a4f7422": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fd1ce058941425da07b9eb0b22fd09f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30ed64dac9cc4a218734f8678d6b35b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32840c7d1b414841bc28c7a9fd81be33": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "332acfff1e5e459c8d229d3a0d2169d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_559cf60479a44452900421c985949a3a",
              "IPY_MODEL_bdf00aa9b8da43b0b229c5a4566ff4bd",
              "IPY_MODEL_fd633cad3d994284a8084f93c2668669"
            ],
            "layout": "IPY_MODEL_6a329639b41149069cd87e5df3b7b5a9"
          }
        },
        "33893db8b278422ba6f508f5a9c78177": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33f28884e71c4a3bba92ef0615ee3a48": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34b786a9cd8d42b7b1c69ca91d3c9359": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ccdb1a7be02b4f77a4635f1612be9360",
              "IPY_MODEL_6daa63e3012b435a887107e392aa6651",
              "IPY_MODEL_a3ee21e71ae44f1ebbe0f6e2e06bcbcb"
            ],
            "layout": "IPY_MODEL_36c4e3436ae64ee9bbf784aa8c39c06b"
          }
        },
        "34d9097b358f4066b81610a643a91367": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "355096deee0842068a23019ab8d5ee1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0ae3f54842c04d6ea1b5fa1be3bfbc5e",
            "max": 8540771296,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f7a60f172ea441df9fde0e5852305f57",
            "value": 8540771296
          }
        },
        "365a847d2fb949b08ad2799ddb9bdfe8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36c4e3436ae64ee9bbf784aa8c39c06b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "375e18cf4fae4b38935c18b1b026d194": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_95dd39f5325241719f5838f21828ca20",
            "placeholder": "​",
            "style": "IPY_MODEL_ff2bab5e8aad485a86195e2f69834c5e",
            "value": "generation_config.json: 100%"
          }
        },
        "38053e33cafd4207bc3d5ed0537b92ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "382bbae83bbc416395edb127b6bec9ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "38afaf81d45f45fca24a96b6ff25a4b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "39ae9634aef84b949cef2f9107773320": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39bc31df5c4947ae90a0bf862e739ae8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3c65196ca805431ebb766846980c6d11": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3cbca6985d8340a9b0e073a24622004f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3cdf4dc180b64a54844353e64725cf3e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d274df88eac4e16b4c220b1a6372a57": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3dcd01f64e374bc6ae6c1d587b81bac5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e4bc9f043854488b878656483ad6688": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e53b04b3f754fc282692b31ea10baae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40217752dc6245a1b2c67ec4e79e379c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41c05166642345b0a00f7d186218be49": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43ec4ef991ca471fb7b409eee3553ed0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49c87fcf94e74d7da5dc14803b102cb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b950535e3e7347a0af7628d5bf7b80ef",
            "placeholder": "​",
            "style": "IPY_MODEL_b82314f359324390b94eb921847d5e08",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "4a2150ec026648bcba1c75a31618131d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4a8bbd1eb7a54f94a0933ea719e27a2e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4adaeabf499941c1b5c3ca13bccda9f8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bebb5119aab4cd196509bd84f5e6015": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d0f2246d36943cfab3dd60260c92912": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d5d028e5e304226b654fca8859514a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e3861b0138546da835a21cb88267d2e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e81edbb12584ac9b2bcc3d34aff7e8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f2a96b0b9654e56a30bfe89a5386a23": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1fd1ac2fbb6434a97aedd66419f3a1f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_38afaf81d45f45fca24a96b6ff25a4b4",
            "value": 1
          }
        },
        "503b8be124f24ea49d8061863ad68f0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5095763539d846d39b9348e51bd7a627": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "50efde67ef904d4693da167df766d993": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "51ff0c36700e4c92866ea7239e40e7f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "536953943b1a44ad9c8953bd0e5e5260": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "53bd285ba55e428bbdcad85c808dca54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "54421f39df1a41efacb0b907c57922f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92e8fb8fd761435b8873b6ea8bf8fe91",
            "max": 1112201288,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_83bd0985ce934f8e90925148a2fb29c2",
            "value": 1112201288
          }
        },
        "5462e6730819423292ff22ba556578d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a47b8d1c755f486686afbd01f85dff34",
            "max": 723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_32840c7d1b414841bc28c7a9fd81be33",
            "value": 723
          }
        },
        "559cf60479a44452900421c985949a3a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_711c3e4a2fc94774931c340fcdd94039",
            "placeholder": "​",
            "style": "IPY_MODEL_3cbca6985d8340a9b0e073a24622004f",
            "value": "Flattening the indices: 100%"
          }
        },
        "5650051de25b49b3a3cd2ec15dd6ef35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b61c3734c88480e84c4ca8d2ede49bf",
            "max": 796,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f6971aba51584d6e879ae4a026f71110",
            "value": 796
          }
        },
        "5808bf7c644e4228ae669343d44c59bd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58e4b815ec4a4747a0b56e33a540c9bb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "592e543174ac4c149b978a9b076e6947": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_62abec66de98471dbac0a2993bea23ad",
            "placeholder": "​",
            "style": "IPY_MODEL_c1d1a9e630ae45a896a48c40d2ef9849",
            "value": "model.safetensors: 100%"
          }
        },
        "5a5e5d22b7ae4273bbb8664a877397e2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bd00d3d8b214109973049badc3cb4be": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c1ac9846bfb48d19298a110c29bf2e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4a8bbd1eb7a54f94a0933ea719e27a2e",
            "max": 796,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b5a109286ea842b0b2ba9534713c1ffe",
            "value": 796
          }
        },
        "5ca0d161ddec4450bec2a0cb8d0ad683": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d9c82cc676d40c9a92d9ccca8a4e3e7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dcf287617bb4db5ac7b65bcf81d1887": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5dd53b147d08431e8ecc73aa798ad889": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e5c95ab789c470090fbee9c41bc596a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "615c3c243a214a27a95a1b3cc807bc21": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "61950496fcb54235a6a0e1fff0991037": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "629ec748ee184a8eb69a3cb65ec65e0f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62abec66de98471dbac0a2993bea23ad": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64d9434b4d4c43bcab5188ef0cd22ccc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66106b47a3ba43ceb3b943dee3dbe3a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "67f6191f7d934ba995d4deda864aa147": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5dcf287617bb4db5ac7b65bcf81d1887",
            "placeholder": "​",
            "style": "IPY_MODEL_c9bd4c42e09a459abd7574111b60812b",
            "value": "Converting to ShareGPT: 100%"
          }
        },
        "6895588ff3ae4dfdb44f545b9f2c9a0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_77256958d12141eb809dd0c585983dee",
              "IPY_MODEL_0176b9971228499fa742bfce6f754663",
              "IPY_MODEL_fdf7e9de3889464e940f1637fc8ec65e"
            ],
            "layout": "IPY_MODEL_9e6500d06fad492783db0849ceb7f26b"
          }
        },
        "69de33eb6326457298070f7fe84cfff1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_743d80e6489542f7b9493fef1b9998a3",
            "max": 796,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2b400f884db74feb8be26093bdd792a2",
            "value": 796
          }
        },
        "6a329639b41149069cd87e5df3b7b5a9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a3b9333aeae4a00a21d6b857c5c54c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6a4f221663c444b3a82a71f850162467": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b61c3734c88480e84c4ca8d2ede49bf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c0fad39a89342be9404a09ec7c4fcb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6cbfef56108b4992b385d82ff36cdbc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58e4b815ec4a4747a0b56e33a540c9bb",
            "placeholder": "​",
            "style": "IPY_MODEL_51ff0c36700e4c92866ea7239e40e7f3",
            "value": "100%"
          }
        },
        "6ce15f1736b343948196bb520cae9a84": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a42fb19dbd8f4557bf3bc62f79c729bd",
              "IPY_MODEL_2b8e98da62ad465793e858bbdc7db9ab",
              "IPY_MODEL_8bb5db854b6743218980a1909031f9d1"
            ],
            "layout": "IPY_MODEL_6a4f221663c444b3a82a71f850162467"
          }
        },
        "6cea37a6ac1244faa10da603cf53c985": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6daa63e3012b435a887107e392aa6651": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c893c659fe674f73a391219bcc9ce9f2",
            "max": 796,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fe2cb4a4a87e425bb741dc2e96cb3c19",
            "value": 796
          }
        },
        "6e4fb3251724456fae6b6f13a6c12097": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6f342ebd95af4f33b51649261a89bea2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "709541f6add042f9942ece0d18d0ed8b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70c1b4c89dc04af4b88cec3d9fd76143": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "711c3e4a2fc94774931c340fcdd94039": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7232fabaecbe44f0bfb85efabc852adf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_285d4440076a4c68a6f8317266e880c7",
              "IPY_MODEL_ddf79a29dab14cc9916571d5ef10b224",
              "IPY_MODEL_283f2a5610894deb9a7b10c6403cd4bb"
            ],
            "layout": "IPY_MODEL_2e39a170df4e413f960ec363bbea2095"
          }
        },
        "731686f94c174dc0b65556bca9eca0b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c01604f930e94bc79dc10369f36819b0",
              "IPY_MODEL_931336892af0406cb159b2ebef825df3",
              "IPY_MODEL_fa438c641a1e41c08b2ebbb4e87a2018"
            ],
            "layout": "IPY_MODEL_8106f79da6974840a0c228354066bdac"
          }
        },
        "73d06604b4f540b58e9cf87c7f56152b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "741258f9827546319a1dc4d2f892210a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "743d80e6489542f7b9493fef1b9998a3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75e83248694a401283c01c1f1052519d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ebca870c675e4ade83fd5712b97a96f8",
              "IPY_MODEL_d223583186b748afa0af48cf021b1ac6",
              "IPY_MODEL_f802cc878c98461598a166e9b59fcd3f"
            ],
            "layout": "IPY_MODEL_db15ee6f267a4620978d4ef3b822d711"
          }
        },
        "7661221145d04590a76c181231d8eb56": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_df917f2ba37a41039d8305bce24a3c6d",
              "IPY_MODEL_1b9f60aac66949cba9a3f20759d1f5c4",
              "IPY_MODEL_c672b2b5cb574c33955eb644fbe35b98"
            ],
            "layout": "IPY_MODEL_c47b1a19346a4a568abf1a3281233887"
          }
        },
        "76b661b674b845738960e48caba5ae30": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f913635f3df441b82a5374c8a4f7422",
            "placeholder": "​",
            "style": "IPY_MODEL_536953943b1a44ad9c8953bd0e5e5260",
            "value": " 10/10 [01:49&lt;00:00,  9.60s/it]"
          }
        },
        "77256958d12141eb809dd0c585983dee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61950496fcb54235a6a0e1fff0991037",
            "placeholder": "​",
            "style": "IPY_MODEL_b587a76b966544dba862b0eff56543b4",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "774c667bb01445a79cf8381a7dc73314": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08e35bdac488451b82a2f9b785aa6138",
            "placeholder": "​",
            "style": "IPY_MODEL_f93543b90d1f4db491d07f743561389f",
            "value": "Generating train split: "
          }
        },
        "777674f274264d22ade6581c5b9de8e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7e598dfeef2432eb628d35ff2a1628e",
            "max": 796,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9f97bbebf0e140fab4945e412b6ed2fa",
            "value": 796
          }
        },
        "782afad7b7474bc09650a78759a916e9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7926b22c81d24b3faa488d7e8cd45e25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7ae263e1a157415b989d86b39dc86ab9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d238b16dac94154bd3e24df5636bc65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7df0ca10a6b64cf49549a592019744af": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fdf5fdff9f64e749803ad4c4f4a5db7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "808f22940b3e4d56a044858af7717a82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_375e18cf4fae4b38935c18b1b026d194",
              "IPY_MODEL_f7039d5c73024c2bb49232de870c0753",
              "IPY_MODEL_b013a83d84e340ca9bdb032e04a4fe82"
            ],
            "layout": "IPY_MODEL_629ec748ee184a8eb69a3cb65ec65e0f"
          }
        },
        "80ca55c4cb834e07a222a05eed953108": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8106f79da6974840a0c228354066bdac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "81acfacf75ef46d88c4b530896c1d6bb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83bd0985ce934f8e90925148a2fb29c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "848cfb3b291b46b2b5ad89ae0b370abc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1681c2a889474147a281aa57ea49fa01",
            "placeholder": "​",
            "style": "IPY_MODEL_8d3865be2e614d97a600f65d8fcd2f53",
            "value": "Generando dataset: 100%"
          }
        },
        "84ded545991a482ea540a08ec0efac93": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d0f2246d36943cfab3dd60260c92912",
            "placeholder": "​",
            "style": "IPY_MODEL_86f61fb933494d148da6d6b0d64cf56c",
            "value": "tokenizer.json: 100%"
          }
        },
        "8521f1b081c8426ab052535bccb87a51": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d274df88eac4e16b4c220b1a6372a57",
            "placeholder": "​",
            "style": "IPY_MODEL_e5caecafe5ba42fbbdeabcc688980df9",
            "value": "Map: 100%"
          }
        },
        "866d6d319be342cc9c9c1584d79e276f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f03102b868c54d0aa2e47d4971d01cd0",
              "IPY_MODEL_8a74df7879114abf83c02f59b86179e3",
              "IPY_MODEL_e1897a57fea34737ac7848789f951bae"
            ],
            "layout": "IPY_MODEL_34d9097b358f4066b81610a643a91367"
          }
        },
        "86f61fb933494d148da6d6b0d64cf56c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "887830af532541a88b552b499e751cf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8879a8c8c73442f1913797a796271b23": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_67f6191f7d934ba995d4deda864aa147",
              "IPY_MODEL_f84e15027cc044b18041b2c14a6ca5b9",
              "IPY_MODEL_c31c0805bee54f6e9d2ee25efb5455b3"
            ],
            "layout": "IPY_MODEL_33f28884e71c4a3bba92ef0615ee3a48"
          }
        },
        "898936f8509843b089c36807a31e9f73": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8a74df7879114abf83c02f59b86179e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04df9580b1ec4ca79d282383eafabaa2",
            "max": 239,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b75d3f266ade490b8b49393883fad4c5",
            "value": 239
          }
        },
        "8aa62bcdb50444f68cbcb6d8da3c8a5b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bb5db854b6743218980a1909031f9d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f604e6f0f77c4d63bf49f4728d411f73",
            "placeholder": "​",
            "style": "IPY_MODEL_4bebb5119aab4cd196509bd84f5e6015",
            "value": " 796/796 [00:03&lt;00:00, 286.31 examples/s]"
          }
        },
        "8d3865be2e614d97a600f65d8fcd2f53": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9013e44d4b234355bf5cb5ab2cf282a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0015e48e30504dcfa20e224ddfcf823d",
            "placeholder": "​",
            "style": "IPY_MODEL_4e81edbb12584ac9b2bcc3d34aff7e8f",
            "value": "model.safetensors: 100%"
          }
        },
        "90dbffdf96bf4d609c59d0c631d6fdfd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92c00c5cb9c24de1af7e40aa0ed15b47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5a5e5d22b7ae4273bbb8664a877397e2",
            "placeholder": "​",
            "style": "IPY_MODEL_131b5d0fad894cdf9ac88fff9c123efe",
            "value": " 796/0 [00:00&lt;00:00, 7426.77 examples/s]"
          }
        },
        "92e8fb8fd761435b8873b6ea8bf8fe91": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "931336892af0406cb159b2ebef825df3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d29128b3b8d42178f3ae6d6b4772d2f",
            "max": 796,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5095763539d846d39b9348e51bd7a627",
            "value": 796
          }
        },
        "9392415e91b049ac9b173eaecb6e8c5f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "939fd41a1efd49f6a0e5e880c8a6aa7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9402beee1ca44853b81ca1425b38bdad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cdf4dc180b64a54844353e64725cf3e",
            "placeholder": "​",
            "style": "IPY_MODEL_382bbae83bbc416395edb127b6bec9ec",
            "value": " 3.90k/3.90k [00:00&lt;00:00, 134kB/s]"
          }
        },
        "946f98b1fd3544509b709249cb96374b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1843dac8987a4441819a184141373de2",
              "IPY_MODEL_5c1ac9846bfb48d19298a110c29bf2e7",
              "IPY_MODEL_278d6110c5d946748d5dd90256c326c9"
            ],
            "layout": "IPY_MODEL_03ffb1d6834b444383139a123f373dd1"
          }
        },
        "9585c39899db42858cecf93aff9d5c15": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95dd39f5325241719f5838f21828ca20": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "968ab1d17382458daefc7e2724c6912e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_26d9440af08a4404abe52a76f096bb6e",
              "IPY_MODEL_355096deee0842068a23019ab8d5ee1c",
              "IPY_MODEL_c5f6ba1f51a1425b90b3946ca66cc685"
            ],
            "layout": "IPY_MODEL_43ec4ef991ca471fb7b409eee3553ed0"
          }
        },
        "97a26d010d1340b2811b8cdaf4c8bce9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "982efc40a70245b7ad8468c8bede757b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "989344ac67d24b4098b718695ece3f9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9932f74e242f4119bf2ef6c3c9306087": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "99f72a35e9b84bb391be822e751193ab": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a4aa0c0a5844740abe4d275fe038c43": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f52a496379bf4f91bba56679c7495c67",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_898936f8509843b089c36807a31e9f73",
            "value": 10
          }
        },
        "9a665075297c4dd5b36cb32d5a4e76f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9bb6e499ffb0400e9abfb4c4df69ad10": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c0b992d847a4edc8bae02bdb0a47a52": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db50691252d44ad2ad8059a948b2a05f",
            "placeholder": "​",
            "style": "IPY_MODEL_6a3b9333aeae4a00a21d6b857c5c54c1",
            "value": " 5.70G/5.70G [00:53&lt;00:00, 371MB/s]"
          }
        },
        "9d4c6e36a1b8485d9f8371bd4e88e859": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9dd150275e074a99813d6d33f497a678": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_49c87fcf94e74d7da5dc14803b102cb3",
              "IPY_MODEL_08ac4c49dee94f17a6ef197a985fd780",
              "IPY_MODEL_f3e1221f87d3405f9e0a2626f2202dfc"
            ],
            "layout": "IPY_MODEL_d35613bcb73e4faeb93e74fd71441c48"
          }
        },
        "9dff1428059d4bc1b396338372a0ac66": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e6500d06fad492783db0849ceb7f26b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f97bbebf0e140fab4945e412b6ed2fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a3b7c42a2be84209a595cc209ba12755": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5250b86c3a544929037a12dac2d898d",
            "placeholder": "​",
            "style": "IPY_MODEL_887830af532541a88b552b499e751cf2",
            "value": "Extending conversations: 100%"
          }
        },
        "a3ee21e71ae44f1ebbe0f6e2e06bcbcb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9585c39899db42858cecf93aff9d5c15",
            "placeholder": "​",
            "style": "IPY_MODEL_982efc40a70245b7ad8468c8bede757b",
            "value": " 796/796 [00:00&lt;00:00, 2044.37 examples/s]"
          }
        },
        "a42fb19dbd8f4557bf3bc62f79c729bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de7427201d2d46b0a507f4e43dec20ed",
            "placeholder": "​",
            "style": "IPY_MODEL_15a9b66c449a43ee9e759b29457e1df3",
            "value": "Unsloth: Tokenizing [&quot;text&quot;] (num_proc=2): 100%"
          }
        },
        "a47b8d1c755f486686afbd01f85dff34": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4b5c9120f6d484285cad69f75a4c4e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70c1b4c89dc04af4b88cec3d9fd76143",
            "max": 796,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4d5d028e5e304226b654fca8859514a4",
            "value": 796
          }
        },
        "a4e479e28ce94b0f9b63dd4c435e6e7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08746819f1ba4895bcbc13fd0ecbd0ac",
            "placeholder": "​",
            "style": "IPY_MODEL_fec29fe05b334fb488537ab92066ddb5",
            "value": " 1/1 [01:14&lt;00:00, 74.24s/it]"
          }
        },
        "a581b54fb35b483dbe8db53c11ab2e07": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5fc373283e74ad38980e2a1c588619e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a3b7c42a2be84209a595cc209ba12755",
              "IPY_MODEL_69de33eb6326457298070f7fe84cfff1",
              "IPY_MODEL_a8fb0593650f4007be70d0fbb0fdfced"
            ],
            "layout": "IPY_MODEL_bdb97cdc500d4ef6be5175bdfd970914"
          }
        },
        "a816656658de4212968acb301df96937": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a86a7074d2aa4c0b9cc07304df896a09": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8fb0593650f4007be70d0fbb0fdfced": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab828a2583c44d1183da61e4663e1bba",
            "placeholder": "​",
            "style": "IPY_MODEL_bb1368132ca94034bbae49c2150ad2e1",
            "value": " 796/796 [00:00&lt;00:00, 6913.35 examples/s]"
          }
        },
        "ab828a2583c44d1183da61e4663e1bba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac6bd87456c84942b74b461d8a8126b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c1b94c562bc04a9197388bbe47290df8",
              "IPY_MODEL_c543d5b573f746ac8cbf54af455a242c",
              "IPY_MODEL_1a2674c45ca046cba90f0040eef6fe84"
            ],
            "layout": "IPY_MODEL_0fcfe5f97c8147a3a375cec97fecd359"
          }
        },
        "ac984ea7b70c4cda959655d375c978b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "adb37cae3e0c4ab986c5c0704becf8f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "adc72504dd424f948cc7117553212d57": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b013a83d84e340ca9bdb032e04a4fe82": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f50acf3c440e46da9e2445e7a010f81d",
            "placeholder": "​",
            "style": "IPY_MODEL_20804d2b3c7e4ef1ba3d67916d9768d1",
            "value": " 220/220 [00:00&lt;00:00, 22.7kB/s]"
          }
        },
        "b252adcd86424e549429e91c42e4231c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b264b66edb224b469dde034cec7af3a9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2d07be45d1f4b1ea9845b302c32ef9e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b3b7b90e3607427f94cffed566a51bcb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5250b86c3a544929037a12dac2d898d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b587a76b966544dba862b0eff56543b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5a109286ea842b0b2ba9534713c1ffe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b5d505d5789f43cca86cb1a621745015": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_365a847d2fb949b08ad2799ddb9bdfe8",
            "max": 5069051,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9932f74e242f4119bf2ef6c3c9306087",
            "value": 5069051
          }
        },
        "b75d3f266ade490b8b49393883fad4c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b7c8a149304948f7850e24b3b6e2f3b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_782afad7b7474bc09650a78759a916e9",
            "max": 3896,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e4cf5cf7bdda4c4b8785ed121d800531",
            "value": 3896
          }
        },
        "b82314f359324390b94eb921847d5e08": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b85cbf9a65f54e699e1c539938fe5a6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b950535e3e7347a0af7628d5bf7b80ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba9223c462864718968d1d2399fba000": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bae2e5076ac64f6f9b89e73197d2de1f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb1368132ca94034bbae49c2150ad2e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bdb97cdc500d4ef6be5175bdfd970914": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bdf00aa9b8da43b0b229c5a4566ff4bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7a644cb693544c9b42f2764cbd4abf8",
            "max": 796,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6e4fb3251724456fae6b6f13a6c12097",
            "value": 796
          }
        },
        "bf4f47b2da0344fe8f03eb26d982b96a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf90afd20c1f4717910955a9d481ae9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_84ded545991a482ea540a08ec0efac93",
              "IPY_MODEL_e218685f2f2a403daeaf63c34204d6e5",
              "IPY_MODEL_dee2d0ee41014773b887fb8365d9506d"
            ],
            "layout": "IPY_MODEL_5d9c82cc676d40c9a92d9ccca8a4e3e7"
          }
        },
        "bfbaf1e01bc34bb9a3d7a11d47fdc683": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c01604f930e94bc79dc10369f36819b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf4f47b2da0344fe8f03eb26d982b96a",
            "placeholder": "​",
            "style": "IPY_MODEL_9a665075297c4dd5b36cb32d5a4e76f8",
            "value": "Flattening the indices: 100%"
          }
        },
        "c1b94c562bc04a9197388bbe47290df8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b252adcd86424e549429e91c42e4231c",
            "placeholder": "​",
            "style": "IPY_MODEL_c58bab0f2bd546bcb962fb45a3d6de17",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "c1d1a9e630ae45a896a48c40d2ef9849": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2e2e5027bff43b5b47f07cd7b80ff66": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c315c9e0934e4965ade0eddda9035103": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fdf5fdff9f64e749803ad4c4f4a5db7",
            "placeholder": "​",
            "style": "IPY_MODEL_ac984ea7b70c4cda959655d375c978b3",
            "value": " 796/796 [00:00&lt;00:00, 55683.41 examples/s]"
          }
        },
        "c31c0805bee54f6e9d2ee25efb5455b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d19d330b75eb4951ba2f4a65825d7eba",
            "placeholder": "​",
            "style": "IPY_MODEL_08974acddcf14d3684295695bbe6fbac",
            "value": " 796/796 [00:00&lt;00:00, 30649.08 examples/s]"
          }
        },
        "c418be902e11469cbb66deb117f9362d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c47b1a19346a4a568abf1a3281233887": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c543d5b573f746ac8cbf54af455a242c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3b7b90e3607427f94cffed566a51bcb",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c2e2e5027bff43b5b47f07cd7b80ff66",
            "value": 53
          }
        },
        "c58bab0f2bd546bcb962fb45a3d6de17": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5f6ba1f51a1425b90b3946ca66cc685": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9392415e91b049ac9b173eaecb6e8c5f",
            "placeholder": "​",
            "style": "IPY_MODEL_33893db8b278422ba6f508f5a9c78177",
            "value": " 8.54G/? [01:13&lt;00:00, 649MB/s]"
          }
        },
        "c672b2b5cb574c33955eb644fbe35b98": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e3861b0138546da835a21cb88267d2e",
            "placeholder": "​",
            "style": "IPY_MODEL_41c05166642345b0a00f7d186218be49",
            "value": " 51.1k/51.1k [00:00&lt;00:00, 4.79MB/s]"
          }
        },
        "c7a644cb693544c9b42f2764cbd4abf8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c893c659fe674f73a391219bcc9ce9f2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9b8ab31024948dbab3b9df93a5cc238": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9bd4c42e09a459abd7574111b60812b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cac764d083e34e8798681ea14fcf1a75": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dde8564ed950447490393f433add95a6",
              "IPY_MODEL_d5aa82b997ab4be189285e681efc564d",
              "IPY_MODEL_2227fdd27eaa4ce49be2a0f852496be2"
            ],
            "layout": "IPY_MODEL_99f72a35e9b84bb391be822e751193ab"
          }
        },
        "ccdb1a7be02b4f77a4635f1612be9360": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_014e363aae62423787d393d949df2c9b",
            "placeholder": "​",
            "style": "IPY_MODEL_157dcb1f0bf242139f3546cc93b7b936",
            "value": "Unsloth: Standardizing formats (num_proc=2): 100%"
          }
        },
        "cd4a034727aa4a5eabe5d0160d2ca331": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7c0d3b5f98d4943aae99cb39a1a8974",
            "placeholder": "​",
            "style": "IPY_MODEL_0ae11327bf7f40fa9a452c313fb4be49",
            "value": " 796/796 [00:00&lt;00:00, 31142.82 examples/s]"
          }
        },
        "cdc832f653e340999903760c0fe93d51": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cecb244f1d094f188023e10ea1863471": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e4bc9f043854488b878656483ad6688",
            "placeholder": "​",
            "style": "IPY_MODEL_0561192c90dd4ea6a120a2b9940e5ab8",
            "value": " 796/796 [00:00&lt;00:00, 4886.58 examples/s]"
          }
        },
        "cf1622cec12146bdac9caa903985e340": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d19d330b75eb4951ba2f4a65825d7eba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1e7a5f5fb554318a2dcf99b68077cf4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d223583186b748afa0af48cf021b1ac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13f56339d8ff4e548f3389b99b6cd09f",
            "max": 9085698,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_256205400ed448e0a51efdd73e593eb8",
            "value": 9085698
          }
        },
        "d34e92f473bf4d60924f5d9018132646": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d35613bcb73e4faeb93e74fd71441c48": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d3d61388da654567bcf61577a65f60a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5aa82b997ab4be189285e681efc564d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f342ebd95af4f33b51649261a89bea2",
            "max": 229,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d1e7a5f5fb554318a2dcf99b68077cf4",
            "value": 229
          }
        },
        "d7c0d3b5f98d4943aae99cb39a1a8974": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7e598dfeef2432eb628d35ff2a1628e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9476b1342c74f95b72318a5b809402b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "da6f107176a642e3bf5d4620ae011d03": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_005b82a3932a4ecb9770921e61eee168",
            "placeholder": "​",
            "style": "IPY_MODEL_d3d61388da654567bcf61577a65f60a5",
            "value": "config.json: 100%"
          }
        },
        "dac4ebc417c14936af453c15258c605e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dacc18e6fa914351b71468698aea27ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fd33365745b64ffe80031385cb1ae772",
              "IPY_MODEL_a4b5c9120f6d484285cad69f75a4c4e7",
              "IPY_MODEL_cd4a034727aa4a5eabe5d0160d2ca331"
            ],
            "layout": "IPY_MODEL_22d6e496881c4d6d89d060a4c785c22c"
          }
        },
        "dacd4821451644cebf3cdc739762698f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db15ee6f267a4620978d4ef3b822d711": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db50691252d44ad2ad8059a948b2a05f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db9b43bd16f541f5bfca796e23391575": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc68039b2b7840dabb1d39d90e90afd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dde8564ed950447490393f433add95a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0487c8e4a08f4e20b33aae61a069b7d7",
            "placeholder": "​",
            "style": "IPY_MODEL_6cea37a6ac1244faa10da603cf53c985",
            "value": "modules.json: 100%"
          }
        },
        "ddf79a29dab14cc9916571d5ef10b224": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dacd4821451644cebf3cdc739762698f",
            "max": 402,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c418be902e11469cbb66deb117f9362d",
            "value": 402
          }
        },
        "de7427201d2d46b0a507f4e43dec20ed": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dee2d0ee41014773b887fb8365d9506d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7df0ca10a6b64cf49549a592019744af",
            "placeholder": "​",
            "style": "IPY_MODEL_38053e33cafd4207bc3d5ed0537b92ea",
            "value": " 9.08M/9.08M [00:00&lt;00:00, 24.6MB/s]"
          }
        },
        "df917f2ba37a41039d8305bce24a3c6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1fbc81be877b4712933eb6e5c0a28cd4",
            "placeholder": "​",
            "style": "IPY_MODEL_00a9009b6cb34da0b8d11233bafe42f9",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "e0058619ab004c3891f1f211e7138bf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8521f1b081c8426ab052535bccb87a51",
              "IPY_MODEL_777674f274264d22ade6581c5b9de8e6",
              "IPY_MODEL_cecb244f1d094f188023e10ea1863471"
            ],
            "layout": "IPY_MODEL_fda1be9042914fec8a145e0cac60ca01"
          }
        },
        "e06d7dd1a809445db2ae8996af6dabf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e093a79cbce24644ac2f510187798adf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf1622cec12146bdac9caa903985e340",
            "placeholder": "​",
            "style": "IPY_MODEL_15614a90630f48f28e345ee0e71d4306",
            "value": " 190/190 [00:00&lt;00:00, 11.5kB/s]"
          }
        },
        "e0b9c95bd1cb4c34ad24e7983d5fb019": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6a99c6a2a1d4de7b8ae59675718052f",
            "placeholder": "​",
            "style": "IPY_MODEL_0dbbc41cb5da49c09557d77318e1d89a",
            "value": " 796/796 [00:00&lt;00:00, 21355.30 examples/s]"
          }
        },
        "e10e80363b754204887fffc4af7cd3d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e1897a57fea34737ac7848789f951bae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_193259b79e294176983888af51b51103",
            "placeholder": "​",
            "style": "IPY_MODEL_dac4ebc417c14936af453c15258c605e",
            "value": " 239/239 [00:00&lt;00:00, 20.5kB/s]"
          }
        },
        "e1fd1ac2fbb6434a97aedd66419f3a1f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "e218685f2f2a403daeaf63c34204d6e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfbaf1e01bc34bb9a3d7a11d47fdc683",
            "max": 9081518,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_adb37cae3e0c4ab986c5c0704becf8f3",
            "value": 9081518
          }
        },
        "e4cf5cf7bdda4c4b8785ed121d800531": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e5caecafe5ba42fbbdeabcc688980df9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e80f507e19904e4fa56d9da77d4f535e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eaa5c7ea39344301bab459bc04d5716b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2337590ca184415e9c27920a91f2bddb",
              "IPY_MODEL_1a9d2ffe04bf4ba2a2f9c372307ab239",
              "IPY_MODEL_c315c9e0934e4965ade0eddda9035103"
            ],
            "layout": "IPY_MODEL_cdc832f653e340999903760c0fe93d51"
          }
        },
        "ebca870c675e4ade83fd5712b97a96f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b264b66edb224b469dde034cec7af3a9",
            "placeholder": "​",
            "style": "IPY_MODEL_a581b54fb35b483dbe8db53c11ab2e07",
            "value": "tokenizer.json: 100%"
          }
        },
        "ec0f34d02eee4fcf9d7f51d86b768b46": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed1a808877b54b14a01c0d4ab757dc81": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97a26d010d1340b2811b8cdaf4c8bce9",
            "placeholder": "​",
            "style": "IPY_MODEL_e06d7dd1a809445db2ae8996af6dabf9",
            "value": "config.json: 100%"
          }
        },
        "ed9e04c3c7f84a649a4c36b8f517ca90": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bae2e5076ac64f6f9b89e73197d2de1f",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_50efde67ef904d4693da167df766d993",
            "value": 190
          }
        },
        "edd2ae155b8f45658f0fed29db260588": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_848cfb3b291b46b2b5ad89ae0b370abc",
              "IPY_MODEL_9a4aa0c0a5844740abe4d275fe038c43",
              "IPY_MODEL_76b661b674b845738960e48caba5ae30"
            ],
            "layout": "IPY_MODEL_8aa62bcdb50444f68cbcb6d8da3c8a5b"
          }
        },
        "f03102b868c54d0aa2e47d4971d01cd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ce45738eaed494288408212e78724f2",
            "placeholder": "​",
            "style": "IPY_MODEL_9bb6e499ffb0400e9abfb4c4df69ad10",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "f1e88ba228d844f697ac66ab1f11a0f5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3e1221f87d3405f9e0a2626f2202dfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db9b43bd16f541f5bfca796e23391575",
            "placeholder": "​",
            "style": "IPY_MODEL_b85cbf9a65f54e699e1c539938fe5a6f",
            "value": " 122/122 [00:00&lt;00:00, 3.41kB/s]"
          }
        },
        "f4d842b8447c4d2db29a2a5646be53db": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f50acf3c440e46da9e2445e7a010f81d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f52a496379bf4f91bba56679c7495c67": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f5943977ac2e47f79c80772f7aed2bd8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f604e6f0f77c4d63bf49f4728d411f73": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6971aba51584d6e879ae4a026f71110": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f6a99c6a2a1d4de7b8ae59675718052f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7039d5c73024c2bb49232de870c0753": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a86a7074d2aa4c0b9cc07304df896a09",
            "max": 220,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_80ca55c4cb834e07a222a05eed953108",
            "value": 220
          }
        },
        "f7a60f172ea441df9fde0e5852305f57": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f802cc878c98461598a166e9b59fcd3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2fd1ce058941425da07b9eb0b22fd09f",
            "placeholder": "​",
            "style": "IPY_MODEL_e80f507e19904e4fa56d9da77d4f535e",
            "value": " 9.09M/9.09M [00:01&lt;00:00, 8.54MB/s]"
          }
        },
        "f84e15027cc044b18041b2c14a6ca5b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5943977ac2e47f79c80772f7aed2bd8",
            "max": 796,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_53bd285ba55e428bbdcad85c808dca54",
            "value": 796
          }
        },
        "f93543b90d1f4db491d07f743561389f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa438c641a1e41c08b2ebbb4e87a2018": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d34e92f473bf4d60924f5d9018132646",
            "placeholder": "​",
            "style": "IPY_MODEL_fce4bd29180641d29cc7e6310706d109",
            "value": " 796/796 [00:00&lt;00:00, 28681.22 examples/s]"
          }
        },
        "fc8181c289ff44c5a83a88684203906e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fce4bd29180641d29cc7e6310706d109": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd33365745b64ffe80031385cb1ae772": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_39ae9634aef84b949cef2f9107773320",
            "placeholder": "​",
            "style": "IPY_MODEL_066a8a3400384015a2ab91be7925e856",
            "value": "Merging columns: 100%"
          }
        },
        "fd633cad3d994284a8084f93c2668669": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05af9e6bb60047a3ad7e3142a177ca14",
            "placeholder": "​",
            "style": "IPY_MODEL_1eabbd3c84064a2a86020c1c28b0426c",
            "value": " 796/796 [00:00&lt;00:00, 27088.13 examples/s]"
          }
        },
        "fda1be9042914fec8a145e0cac60ca01": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdf7e9de3889464e940f1637fc8ec65e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e5c95ab789c470090fbee9c41bc596a",
            "placeholder": "​",
            "style": "IPY_MODEL_503b8be124f24ea49d8061863ad68f0b",
            "value": " 345/345 [00:00&lt;00:00, 38.9kB/s]"
          }
        },
        "fe2cb4a4a87e425bb741dc2e96cb3c19": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fec29fe05b334fb488537ab92066ddb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff2bab5e8aad485a86195e2f69834c5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff68046ed0bd417e92e303c15c43d7f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba9223c462864718968d1d2399fba000",
            "placeholder": "​",
            "style": "IPY_MODEL_64d9434b4d4c43bcab5188ef0cd22ccc",
            "value": "README.md: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}